{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1x1cMVd9qHeW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuL0xmxbq25O",
        "outputId": "3fa17369-8ccc-4238-dde8-0e17a69c6539"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train =  datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor(),download=True)\n",
        "X_test =  datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor(),Download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt9zHyvfqvwG",
        "outputId": "d5f201fa-e04d-4cb0-9040-3fb459b8e167"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 13.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 207kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.93MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.39MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvAQ0EF2rKlj",
        "outputId": "c4907500-9589-4e3e-fca2-8be78500888b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset FashionMNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: ./data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: ToTensor(),\n",
              " Dataset FashionMNIST\n",
              "     Number of datapoints: 10000\n",
              "     Root location: ./data\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: ToTensor())"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert data into batches\n",
        "train_loader = DataLoader(X_train, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(X_test, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "56ytCxKyrTsK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader) ,len(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq4oJJC6rfr6",
        "outputId": "d3d4d957-538e-47c0-9319-dc9c95183604"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(938, 157)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build the class with 3 input_dim,hidden_layer1,hidden_layer2,hidden_layer3,ouptu_dim\n",
        "\n",
        "class Ann(nn.Module):\n",
        "  def __init__(self,input_layer, hidden_layer1, hidden_layer2,hidden_layer3,output_layer):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Linear(input_layer,hidden_layer1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.fc2 = nn.Sequential(\n",
        "        nn.Linear(hidden_layer1,hidden_layer2),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.fc3 = nn.Sequential(\n",
        "        nn.Linear(hidden_layer2,hidden_layer3),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.fc4 = nn.Sequential(\n",
        "        nn.Linear(hidden_layer3,output_layer),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = x.view(x.shape[0],-1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.fc4(x)\n",
        "    return F.log_softmax(x,dim=1)"
      ],
      "metadata": {
        "id": "70ZtC7ENrnh7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer = 28*28\n",
        "input_layer1 = 256\n",
        "input_layer2 = 128\n",
        "input_layer3 = 64\n",
        "output_layer = 10\n",
        "\n",
        "model = Ann(hidden_layer,input_layer1,input_layer2,input_layer3,output_layer)"
      ],
      "metadata": {
        "id": "jBdtentrtci-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7SWilAPt-d5",
        "outputId": "320d8e00-1bfe-440d-8f9a-e3c444e296a4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ann(\n",
              "  (fc1): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (fc2): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (fc3): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (fc4): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySQICWBauv-2",
        "outputId": "d1395598-1096-4a24-9234-7aa739c52bb1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('fc1.0.weight',\n",
              "              tensor([[-0.0182,  0.0252, -0.0190,  ...,  0.0036, -0.0293,  0.0308],\n",
              "                      [ 0.0305,  0.0166,  0.0239,  ..., -0.0211,  0.0140,  0.0224],\n",
              "                      [-0.0046, -0.0091,  0.0016,  ..., -0.0082, -0.0194,  0.0195],\n",
              "                      ...,\n",
              "                      [ 0.0203, -0.0311,  0.0047,  ..., -0.0263, -0.0308,  0.0121],\n",
              "                      [ 0.0151,  0.0140, -0.0173,  ...,  0.0193,  0.0231, -0.0315],\n",
              "                      [-0.0165, -0.0014, -0.0284,  ...,  0.0009,  0.0156, -0.0143]])),\n",
              "             ('fc1.0.bias',\n",
              "              tensor([-0.0317, -0.0170, -0.0130,  0.0230,  0.0204, -0.0025,  0.0270, -0.0103,\n",
              "                      -0.0054, -0.0283, -0.0230, -0.0091,  0.0180,  0.0079, -0.0289, -0.0072,\n",
              "                      -0.0104,  0.0143, -0.0129, -0.0203, -0.0255,  0.0043, -0.0123, -0.0085,\n",
              "                       0.0179,  0.0041, -0.0324,  0.0035,  0.0031, -0.0056, -0.0197,  0.0218,\n",
              "                      -0.0205, -0.0296,  0.0109, -0.0156,  0.0138,  0.0033, -0.0052,  0.0101,\n",
              "                      -0.0194,  0.0199, -0.0014,  0.0349, -0.0352,  0.0082,  0.0160, -0.0120,\n",
              "                       0.0347,  0.0200,  0.0200,  0.0231,  0.0109, -0.0022, -0.0088,  0.0216,\n",
              "                      -0.0074,  0.0134, -0.0032,  0.0241,  0.0061, -0.0193,  0.0197,  0.0283,\n",
              "                      -0.0276,  0.0113, -0.0228,  0.0327, -0.0115, -0.0094, -0.0100, -0.0157,\n",
              "                       0.0166, -0.0291,  0.0281,  0.0248, -0.0281, -0.0018,  0.0225, -0.0252,\n",
              "                      -0.0262, -0.0120,  0.0087, -0.0103, -0.0098, -0.0271,  0.0054,  0.0352,\n",
              "                       0.0314, -0.0301, -0.0259,  0.0098, -0.0176, -0.0186,  0.0111,  0.0240,\n",
              "                      -0.0135,  0.0030, -0.0027,  0.0008,  0.0070,  0.0310,  0.0141,  0.0055,\n",
              "                      -0.0302,  0.0347, -0.0266,  0.0190,  0.0219, -0.0154,  0.0107,  0.0131,\n",
              "                      -0.0127, -0.0237,  0.0021, -0.0294,  0.0205,  0.0237,  0.0040, -0.0024,\n",
              "                      -0.0013, -0.0088, -0.0095, -0.0108, -0.0014,  0.0270,  0.0287, -0.0034,\n",
              "                      -0.0232, -0.0178,  0.0011, -0.0119,  0.0285, -0.0033,  0.0083,  0.0046,\n",
              "                      -0.0232, -0.0042,  0.0225,  0.0053,  0.0253,  0.0064,  0.0251,  0.0153,\n",
              "                      -0.0181,  0.0196,  0.0127,  0.0172,  0.0304,  0.0218, -0.0198, -0.0180,\n",
              "                      -0.0053,  0.0094, -0.0209,  0.0163, -0.0177,  0.0356, -0.0347,  0.0001,\n",
              "                      -0.0166, -0.0298,  0.0292, -0.0125, -0.0062, -0.0011,  0.0039, -0.0298,\n",
              "                       0.0179,  0.0141,  0.0004,  0.0341, -0.0220,  0.0325,  0.0192,  0.0181,\n",
              "                      -0.0248, -0.0236,  0.0200,  0.0063,  0.0228, -0.0264,  0.0151, -0.0180,\n",
              "                       0.0304, -0.0137, -0.0305,  0.0228, -0.0241,  0.0072,  0.0152, -0.0154,\n",
              "                      -0.0159, -0.0196, -0.0108,  0.0329,  0.0174,  0.0079, -0.0108, -0.0144,\n",
              "                       0.0146,  0.0281, -0.0060,  0.0066,  0.0070, -0.0139, -0.0351,  0.0248,\n",
              "                       0.0260,  0.0164, -0.0014, -0.0255,  0.0050,  0.0259,  0.0307, -0.0141,\n",
              "                      -0.0007, -0.0248, -0.0006,  0.0223, -0.0284,  0.0316, -0.0005,  0.0003,\n",
              "                       0.0207, -0.0057,  0.0160,  0.0058, -0.0165,  0.0033, -0.0045,  0.0183,\n",
              "                      -0.0143, -0.0170, -0.0134, -0.0219, -0.0214,  0.0155, -0.0288,  0.0178,\n",
              "                       0.0173,  0.0037,  0.0217, -0.0143, -0.0229,  0.0330,  0.0016, -0.0333,\n",
              "                      -0.0079,  0.0117, -0.0066,  0.0159,  0.0158, -0.0138, -0.0172,  0.0030])),\n",
              "             ('fc2.0.weight',\n",
              "              tensor([[-0.0463,  0.0166, -0.0318,  ..., -0.0379,  0.0245, -0.0027],\n",
              "                      [-0.0308, -0.0359,  0.0334,  ...,  0.0417, -0.0289,  0.0616],\n",
              "                      [ 0.0206, -0.0011, -0.0388,  ...,  0.0353, -0.0587, -0.0264],\n",
              "                      ...,\n",
              "                      [-0.0479,  0.0535, -0.0355,  ...,  0.0261, -0.0056,  0.0004],\n",
              "                      [ 0.0155, -0.0172, -0.0222,  ...,  0.0382,  0.0395, -0.0221],\n",
              "                      [ 0.0332, -0.0174,  0.0296,  ..., -0.0190,  0.0143,  0.0162]])),\n",
              "             ('fc2.0.bias',\n",
              "              tensor([ 5.5866e-02, -3.1748e-02, -1.7784e-02, -5.5748e-02,  4.3569e-02,\n",
              "                      -5.1859e-02,  5.4700e-02, -1.1433e-02,  6.2092e-02, -2.2939e-02,\n",
              "                      -4.5661e-02, -5.0239e-03,  4.8143e-02,  5.6029e-02, -1.3227e-02,\n",
              "                      -1.0082e-02, -5.5406e-02,  5.3844e-02,  3.4576e-02,  4.5796e-02,\n",
              "                       5.0888e-02, -4.1004e-02, -2.7442e-03, -4.1404e-03, -3.9109e-02,\n",
              "                      -1.3930e-02, -3.3338e-02, -5.5186e-02, -3.7954e-02,  1.5315e-02,\n",
              "                       4.9619e-02,  5.4013e-02,  9.3639e-03,  2.7064e-02, -1.7671e-02,\n",
              "                      -3.1507e-02,  4.9818e-02, -6.1186e-02, -4.1608e-02, -6.0742e-03,\n",
              "                       5.0941e-02,  3.8136e-02,  1.7781e-03, -5.2452e-02,  1.7999e-03,\n",
              "                      -1.4755e-02,  4.5522e-02,  5.6521e-02, -3.1850e-02, -3.6534e-02,\n",
              "                      -2.7754e-02, -3.2449e-02, -1.7405e-02,  7.1071e-03,  2.6771e-02,\n",
              "                       4.6284e-02, -4.4429e-02, -3.2181e-03,  6.2069e-02, -5.2701e-02,\n",
              "                       3.9180e-02,  8.6511e-03, -9.4113e-03, -5.6323e-02,  5.8212e-02,\n",
              "                      -5.7878e-02,  5.7077e-02, -1.9404e-02, -4.1507e-02,  3.2255e-02,\n",
              "                       1.0162e-02, -2.8494e-02, -2.3132e-02,  4.6058e-02,  1.5251e-02,\n",
              "                       5.3086e-02,  3.7244e-03, -6.0468e-02,  3.5276e-03, -3.9786e-02,\n",
              "                      -6.1769e-02, -4.0770e-02, -1.4194e-02, -8.7783e-03, -5.0044e-02,\n",
              "                      -3.2803e-02, -4.9182e-02,  4.6193e-02, -3.7700e-02,  4.2894e-02,\n",
              "                       3.8074e-02, -5.9135e-02,  4.2970e-02, -1.4736e-02, -4.4841e-02,\n",
              "                      -4.5414e-02, -2.5033e-02, -4.3244e-02,  3.2918e-02, -6.1516e-02,\n",
              "                      -1.3044e-02,  5.1498e-02, -1.6284e-02, -3.6939e-02, -5.2677e-02,\n",
              "                       2.9121e-02,  4.0585e-02, -4.1651e-02, -1.1191e-05, -2.1900e-02,\n",
              "                       5.1717e-02, -4.6946e-02,  4.3976e-02, -5.7733e-02, -5.8662e-02,\n",
              "                       5.8554e-02,  2.2589e-02, -8.9151e-03, -2.7466e-02,  9.6050e-03,\n",
              "                       2.8435e-02, -2.6999e-02,  4.8300e-02, -4.8298e-02,  9.2330e-03,\n",
              "                       5.4571e-02, -1.8692e-02, -3.2992e-02])),\n",
              "             ('fc3.0.weight',\n",
              "              tensor([[-0.0085,  0.0838, -0.0680,  ..., -0.0353, -0.0109, -0.0717],\n",
              "                      [-0.0538,  0.0348, -0.0223,  ...,  0.0614, -0.0238,  0.0343],\n",
              "                      [ 0.0275,  0.0217,  0.0777,  ..., -0.0101,  0.0411,  0.0413],\n",
              "                      ...,\n",
              "                      [ 0.0165, -0.0041,  0.0724,  ...,  0.0133, -0.0378,  0.0037],\n",
              "                      [-0.0319,  0.0374, -0.0646,  ...,  0.0651, -0.0837,  0.0814],\n",
              "                      [ 0.0547,  0.0389,  0.0807,  ...,  0.0284,  0.0096, -0.0841]])),\n",
              "             ('fc3.0.bias',\n",
              "              tensor([ 0.0549, -0.0702, -0.0770,  0.0259, -0.0437, -0.0004,  0.0358, -0.0881,\n",
              "                      -0.0502, -0.0073, -0.0880, -0.0430, -0.0526, -0.0033,  0.0415, -0.0853,\n",
              "                       0.0354,  0.0484,  0.0221,  0.0788,  0.0342,  0.0123, -0.0636,  0.0594,\n",
              "                       0.0579, -0.0015,  0.0813, -0.0451, -0.0003, -0.0674, -0.0404, -0.0443,\n",
              "                       0.0617,  0.0371, -0.0882, -0.0678, -0.0502, -0.0307, -0.0170, -0.0670,\n",
              "                      -0.0421, -0.0293,  0.0096,  0.0053,  0.0733, -0.0144,  0.0449, -0.0486,\n",
              "                      -0.0507, -0.0472,  0.0166,  0.0633,  0.0747, -0.0625, -0.0765, -0.0746,\n",
              "                      -0.0214,  0.0541,  0.0363,  0.0218,  0.0046, -0.0776, -0.0834, -0.0163])),\n",
              "             ('fc4.0.weight',\n",
              "              tensor([[-0.1030,  0.0737, -0.0400,  0.1191, -0.0455, -0.0922, -0.0410,  0.0667,\n",
              "                        0.0292,  0.0539,  0.0662, -0.0650,  0.0743,  0.1226, -0.0864,  0.0365,\n",
              "                       -0.0568, -0.1166, -0.1224,  0.0556, -0.0394, -0.0646, -0.0452,  0.0417,\n",
              "                        0.0593, -0.0577,  0.0673,  0.0088, -0.0252, -0.0145, -0.1149,  0.1168,\n",
              "                        0.0635,  0.0712,  0.0150, -0.0652, -0.0599, -0.0760,  0.0614,  0.1195,\n",
              "                       -0.0113,  0.0631,  0.0666, -0.0718,  0.0014,  0.0670, -0.0569, -0.0217,\n",
              "                       -0.0154, -0.0821,  0.0454, -0.0370,  0.0516, -0.1127,  0.0733, -0.0090,\n",
              "                       -0.0749, -0.0499, -0.0853,  0.0714, -0.0992,  0.1230,  0.0546,  0.0932],\n",
              "                      [ 0.0396,  0.1014,  0.1198, -0.0339, -0.0327, -0.1164, -0.0475,  0.0304,\n",
              "                       -0.0137,  0.0749,  0.0326, -0.0296, -0.1141, -0.0282,  0.0275, -0.1054,\n",
              "                       -0.0890, -0.0474,  0.0944, -0.0760, -0.0325, -0.0468, -0.0991,  0.0304,\n",
              "                       -0.0593,  0.0085,  0.0616,  0.0405, -0.0818,  0.0625, -0.0537, -0.1185,\n",
              "                       -0.0459, -0.0103,  0.1233, -0.0105,  0.0584,  0.0732,  0.0734,  0.0195,\n",
              "                        0.0543, -0.0870, -0.0803, -0.0306,  0.0929,  0.0718, -0.1146,  0.0096,\n",
              "                       -0.0536, -0.1197,  0.1040,  0.0892, -0.0404, -0.0948, -0.0626, -0.1200,\n",
              "                        0.0835,  0.0486, -0.1140, -0.0722, -0.0858,  0.1091, -0.0983,  0.0329],\n",
              "                      [-0.0800, -0.0487, -0.0073,  0.0285,  0.0098, -0.1107, -0.0554, -0.0295,\n",
              "                        0.1182, -0.0698,  0.0990, -0.0692, -0.0045, -0.1182, -0.0205, -0.1225,\n",
              "                       -0.0517, -0.0264, -0.0132,  0.0145, -0.0655, -0.1010,  0.1197, -0.0706,\n",
              "                       -0.0402, -0.1221,  0.0790, -0.0132,  0.0124, -0.0600, -0.0613,  0.0046,\n",
              "                        0.0984, -0.0373, -0.0698, -0.0973,  0.0519,  0.0642, -0.0561,  0.0381,\n",
              "                       -0.0712,  0.1213, -0.0937, -0.0817, -0.1074,  0.0311,  0.1082,  0.0156,\n",
              "                        0.1216, -0.0199, -0.1136,  0.1167,  0.0842,  0.0593, -0.0586, -0.0887,\n",
              "                       -0.0824,  0.1097, -0.0676,  0.0635,  0.0816, -0.0986,  0.0975,  0.0984],\n",
              "                      [-0.0016, -0.0618, -0.1039,  0.0170, -0.1235, -0.0309, -0.0105, -0.1232,\n",
              "                        0.0461,  0.0850, -0.0323, -0.0230, -0.0199, -0.1015,  0.0882, -0.0111,\n",
              "                        0.0046, -0.0006,  0.0225, -0.0370, -0.0935, -0.1233, -0.0661,  0.1150,\n",
              "                        0.0538, -0.0909,  0.0545,  0.0829, -0.1065, -0.0534,  0.0905, -0.1053,\n",
              "                        0.0331,  0.1070, -0.1141, -0.0182, -0.0936, -0.0511,  0.0696,  0.0022,\n",
              "                       -0.1143,  0.0035,  0.1138,  0.0679, -0.0444,  0.0024, -0.0458,  0.0031,\n",
              "                       -0.1131,  0.1014,  0.0561, -0.1193, -0.0351,  0.0609, -0.0089,  0.0933,\n",
              "                        0.0475,  0.0244, -0.0122,  0.0035, -0.0545,  0.0192, -0.0853, -0.0811],\n",
              "                      [ 0.0285, -0.0863, -0.0937,  0.0574, -0.1058,  0.0632,  0.1028, -0.0672,\n",
              "                        0.0202, -0.0174, -0.0801, -0.0105, -0.0297,  0.0237, -0.0312,  0.0289,\n",
              "                        0.1185,  0.0844, -0.1174, -0.0104,  0.0832, -0.1153,  0.0726,  0.0075,\n",
              "                       -0.0404, -0.0315,  0.1187, -0.0547, -0.0932,  0.0368,  0.0261,  0.0692,\n",
              "                       -0.0024,  0.1135, -0.1004,  0.0890, -0.0516, -0.0845,  0.0309,  0.0446,\n",
              "                        0.0810,  0.1161,  0.0507, -0.1132,  0.0961,  0.0670, -0.0449,  0.0349,\n",
              "                        0.0696,  0.0045,  0.1075,  0.0810, -0.0277, -0.0669, -0.0211, -0.0631,\n",
              "                        0.1175, -0.0609,  0.1229,  0.0391,  0.0940,  0.0576,  0.1192, -0.0504],\n",
              "                      [-0.0981, -0.0579, -0.0202, -0.0687,  0.0593,  0.0620, -0.0739, -0.0897,\n",
              "                       -0.0253,  0.1146, -0.1081, -0.1222, -0.0059,  0.0209,  0.0879,  0.0583,\n",
              "                       -0.0217,  0.1121,  0.0258, -0.0707,  0.1229,  0.0722,  0.0617, -0.0433,\n",
              "                       -0.0711, -0.0792, -0.0592,  0.0789,  0.0331, -0.0931,  0.1136,  0.0113,\n",
              "                       -0.1048, -0.0048, -0.0878,  0.0216, -0.0082,  0.1010, -0.0489,  0.0330,\n",
              "                       -0.0716, -0.1144,  0.0218, -0.0334, -0.1038,  0.0444,  0.0313,  0.0269,\n",
              "                       -0.0566,  0.0855,  0.0403,  0.0674,  0.0498, -0.0963, -0.0891,  0.0025,\n",
              "                        0.0025, -0.0698,  0.0943, -0.1010,  0.0655,  0.1146,  0.0161,  0.0370],\n",
              "                      [ 0.1098,  0.0202, -0.1002,  0.0323, -0.0054, -0.1005, -0.0273, -0.0254,\n",
              "                       -0.0606, -0.0713, -0.0621, -0.0935,  0.0879,  0.0629,  0.0987,  0.0256,\n",
              "                        0.0321, -0.0228, -0.0544, -0.0752,  0.0267,  0.0685,  0.0745, -0.1072,\n",
              "                       -0.0386, -0.0887,  0.0334, -0.0349, -0.0547, -0.0052, -0.0207,  0.1210,\n",
              "                       -0.0848, -0.0490, -0.0822, -0.0719, -0.0070, -0.1068,  0.0656, -0.0524,\n",
              "                        0.0341, -0.0129,  0.0776,  0.0088,  0.0724,  0.0560,  0.0333,  0.0187,\n",
              "                        0.0509, -0.0238, -0.0036, -0.1106,  0.1043, -0.0278,  0.0805, -0.0564,\n",
              "                       -0.1003,  0.0964,  0.1004, -0.1020,  0.0244, -0.0968,  0.0595,  0.0996],\n",
              "                      [-0.1095, -0.0226, -0.0580, -0.0680,  0.0406, -0.0180, -0.0068,  0.1138,\n",
              "                        0.0089,  0.0924, -0.0429,  0.0201, -0.0458,  0.0500,  0.0548, -0.0979,\n",
              "                        0.0674,  0.0901,  0.0104,  0.0157, -0.0650,  0.1233, -0.0993,  0.0166,\n",
              "                       -0.1096,  0.0970,  0.0052,  0.0770,  0.0738,  0.1103,  0.0920,  0.0660,\n",
              "                       -0.0656,  0.0455,  0.0188, -0.1179, -0.0915, -0.0744,  0.1095, -0.0839,\n",
              "                       -0.0525, -0.1074, -0.0613, -0.0138,  0.0126, -0.0538, -0.0427,  0.0766,\n",
              "                        0.0138, -0.1070,  0.0327,  0.0473,  0.1239,  0.0189, -0.1102, -0.0066,\n",
              "                       -0.0373, -0.0337,  0.0722,  0.0159, -0.0576,  0.1077, -0.0821, -0.0656],\n",
              "                      [-0.0171,  0.0912, -0.0773,  0.0671, -0.0323,  0.0989,  0.0879, -0.1240,\n",
              "                        0.0641, -0.1094, -0.0449,  0.0700, -0.0386,  0.1080,  0.0957,  0.0994,\n",
              "                        0.0347,  0.0004, -0.0016, -0.1206,  0.1165, -0.0919,  0.0295,  0.0148,\n",
              "                       -0.0872,  0.0329, -0.0255, -0.1038,  0.1157, -0.1067,  0.0492,  0.1184,\n",
              "                       -0.0100,  0.0232,  0.0968,  0.0624, -0.1099, -0.0698, -0.1089, -0.1067,\n",
              "                       -0.0660,  0.0978,  0.0511, -0.0425,  0.0322,  0.0129,  0.0509,  0.0243,\n",
              "                       -0.0017, -0.0714, -0.1065, -0.1202, -0.0894,  0.0656,  0.1243, -0.0648,\n",
              "                       -0.0572,  0.0077, -0.1190,  0.0682,  0.0074,  0.0124, -0.1072,  0.0752],\n",
              "                      [ 0.1182,  0.1016, -0.0820, -0.0900,  0.0403, -0.0236,  0.1103, -0.0170,\n",
              "                        0.0029, -0.0851,  0.1172,  0.0750,  0.1164, -0.0045, -0.0318,  0.0625,\n",
              "                       -0.0453,  0.1221,  0.1002,  0.0957, -0.1071, -0.0717, -0.0003,  0.0341,\n",
              "                        0.0920,  0.0427, -0.0124,  0.0303,  0.0954, -0.0664,  0.0939, -0.0178,\n",
              "                        0.0083, -0.0699, -0.1111,  0.0880, -0.0618, -0.0509,  0.0284,  0.0437,\n",
              "                        0.0662,  0.0080, -0.0067,  0.1189,  0.0845, -0.0428,  0.0544, -0.1015,\n",
              "                        0.0469,  0.0493,  0.0251, -0.0697,  0.0222,  0.0526,  0.0778,  0.0623,\n",
              "                       -0.0223,  0.1025, -0.0005, -0.0379, -0.0856,  0.0987, -0.0609,  0.1004]])),\n",
              "             ('fc4.0.bias',\n",
              "              tensor([ 0.0545, -0.0633,  0.0311,  0.0096, -0.1052,  0.0399,  0.0848,  0.0492,\n",
              "                       0.0486,  0.0965]))])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "fHOeWMk6vFpy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the traing loop\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f\"epochs {epoch+1}\")\n",
        "  train_loss = 0.0\n",
        "  train_acc = 0.0\n",
        "  for i,data in enumerate(train_loader,1):\n",
        "    images,labels = data\n",
        "    images = images.view(images.shape[0],-1)\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    res = model(images)\n",
        "    loss = loss_fn(res,labels)\n",
        "    train_loss += loss.item()\n",
        "    _,prediction = torch.max(res,1)\n",
        "    train_acc +=(prediction==labels).float().mean()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i%300==0:\n",
        "      print(f\"[{i+1} / {i}]  Loss:{train_loss/i:6f}, Accuracy:{train_acc/i:6f}\")\n",
        "    print(f'[{i+1}/ {i}] loss: {train_loss/i:6f}, Accuracy: {train_acc/i:6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxYhUTezwFul",
        "outputId": "a03762be-c5fe-40a5-fac1-f0e3e8c28d36"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[651/ 650] loss: 0.728911, Accuracy: 0.741683\n",
            "[652/ 651] loss: 0.728836, Accuracy: 0.741695\n",
            "[653/ 652] loss: 0.729135, Accuracy: 0.741564\n",
            "[654/ 653] loss: 0.728955, Accuracy: 0.741553\n",
            "[655/ 654] loss: 0.728888, Accuracy: 0.741590\n",
            "[656/ 655] loss: 0.728879, Accuracy: 0.741627\n",
            "[657/ 656] loss: 0.728627, Accuracy: 0.741735\n",
            "[658/ 657] loss: 0.728317, Accuracy: 0.741866\n",
            "[659/ 658] loss: 0.728552, Accuracy: 0.741784\n",
            "[660/ 659] loss: 0.728366, Accuracy: 0.741915\n",
            "[661/ 660] loss: 0.728680, Accuracy: 0.741856\n",
            "[662/ 661] loss: 0.728699, Accuracy: 0.741845\n",
            "[663/ 662] loss: 0.728532, Accuracy: 0.741857\n",
            "[664/ 663] loss: 0.728495, Accuracy: 0.741940\n",
            "[665/ 664] loss: 0.728890, Accuracy: 0.741835\n",
            "[666/ 665] loss: 0.728882, Accuracy: 0.741823\n",
            "[667/ 666] loss: 0.729015, Accuracy: 0.741765\n",
            "[668/ 667] loss: 0.729234, Accuracy: 0.741707\n",
            "[669/ 668] loss: 0.729050, Accuracy: 0.741790\n",
            "[670/ 669] loss: 0.728818, Accuracy: 0.741872\n",
            "[671/ 670] loss: 0.728587, Accuracy: 0.741954\n",
            "[672/ 671] loss: 0.728427, Accuracy: 0.742013\n",
            "[673/ 672] loss: 0.728662, Accuracy: 0.741908\n",
            "[674/ 673] loss: 0.728871, Accuracy: 0.741828\n",
            "[675/ 674] loss: 0.728772, Accuracy: 0.741863\n",
            "[676/ 675] loss: 0.728736, Accuracy: 0.741852\n",
            "[677/ 676] loss: 0.728762, Accuracy: 0.741864\n",
            "[678/ 677] loss: 0.728616, Accuracy: 0.741876\n",
            "[679/ 678] loss: 0.728363, Accuracy: 0.741934\n",
            "[680/ 679] loss: 0.728491, Accuracy: 0.741831\n",
            "[681/ 680] loss: 0.728421, Accuracy: 0.741912\n",
            "[682/ 681] loss: 0.728852, Accuracy: 0.741740\n",
            "[683/ 682] loss: 0.728880, Accuracy: 0.741729\n",
            "[684/ 683] loss: 0.728670, Accuracy: 0.741764\n",
            "[685/ 684] loss: 0.728778, Accuracy: 0.741708\n",
            "[686/ 685] loss: 0.729015, Accuracy: 0.741629\n",
            "[687/ 686] loss: 0.728828, Accuracy: 0.741755\n",
            "[688/ 687] loss: 0.728681, Accuracy: 0.741789\n",
            "[689/ 688] loss: 0.728420, Accuracy: 0.741892\n",
            "[690/ 689] loss: 0.728329, Accuracy: 0.741859\n",
            "[691/ 690] loss: 0.728289, Accuracy: 0.741848\n",
            "[692/ 691] loss: 0.727873, Accuracy: 0.742018\n",
            "[693/ 692] loss: 0.727822, Accuracy: 0.742007\n",
            "[694/ 693] loss: 0.727995, Accuracy: 0.741951\n",
            "[695/ 694] loss: 0.727776, Accuracy: 0.742030\n",
            "[696/ 695] loss: 0.727986, Accuracy: 0.741974\n",
            "[697/ 696] loss: 0.727930, Accuracy: 0.741963\n",
            "[698/ 697] loss: 0.728042, Accuracy: 0.741952\n",
            "[699/ 698] loss: 0.728038, Accuracy: 0.741964\n",
            "[700/ 699] loss: 0.727772, Accuracy: 0.742087\n",
            "[701/ 700] loss: 0.727618, Accuracy: 0.742188\n",
            "[702/ 701] loss: 0.727481, Accuracy: 0.742176\n",
            "[703/ 702] loss: 0.727388, Accuracy: 0.742165\n",
            "[704/ 703] loss: 0.727386, Accuracy: 0.742132\n",
            "[705/ 704] loss: 0.727442, Accuracy: 0.742054\n",
            "[706/ 705] loss: 0.727520, Accuracy: 0.742021\n",
            "[707/ 706] loss: 0.728092, Accuracy: 0.741811\n",
            "[708/ 707] loss: 0.728275, Accuracy: 0.741734\n",
            "[709/ 708] loss: 0.728327, Accuracy: 0.741724\n",
            "[710/ 709] loss: 0.727979, Accuracy: 0.741824\n",
            "[711/ 710] loss: 0.727880, Accuracy: 0.741835\n",
            "[712/ 711] loss: 0.727773, Accuracy: 0.741869\n",
            "[713/ 712] loss: 0.727583, Accuracy: 0.741902\n",
            "[714/ 713] loss: 0.727724, Accuracy: 0.741848\n",
            "[715/ 714] loss: 0.727557, Accuracy: 0.741925\n",
            "[716/ 715] loss: 0.727478, Accuracy: 0.741914\n",
            "[717/ 716] loss: 0.727749, Accuracy: 0.741795\n",
            "[718/ 717] loss: 0.727697, Accuracy: 0.741850\n",
            "[719/ 718] loss: 0.727612, Accuracy: 0.741883\n",
            "[720/ 719] loss: 0.727432, Accuracy: 0.741916\n",
            "[721/ 720] loss: 0.727675, Accuracy: 0.741884\n",
            "[722/ 721] loss: 0.727526, Accuracy: 0.741938\n",
            "[723/ 722] loss: 0.727568, Accuracy: 0.741885\n",
            "[724/ 723] loss: 0.727589, Accuracy: 0.741874\n",
            "[725/ 724] loss: 0.727597, Accuracy: 0.741821\n",
            "[726/ 725] loss: 0.727405, Accuracy: 0.741875\n",
            "[727/ 726] loss: 0.727095, Accuracy: 0.741972\n",
            "[728/ 727] loss: 0.727004, Accuracy: 0.741962\n",
            "[729/ 728] loss: 0.727271, Accuracy: 0.741823\n",
            "[730/ 729] loss: 0.726984, Accuracy: 0.741941\n",
            "[731/ 730] loss: 0.727006, Accuracy: 0.741931\n",
            "[732/ 731] loss: 0.727080, Accuracy: 0.741984\n",
            "[733/ 732] loss: 0.727387, Accuracy: 0.741889\n",
            "[734/ 733] loss: 0.727626, Accuracy: 0.741814\n",
            "[735/ 734] loss: 0.727506, Accuracy: 0.741847\n",
            "[736/ 735] loss: 0.727403, Accuracy: 0.741986\n",
            "[737/ 736] loss: 0.727349, Accuracy: 0.742039\n",
            "[738/ 737] loss: 0.727878, Accuracy: 0.741838\n",
            "[739/ 738] loss: 0.727853, Accuracy: 0.741870\n",
            "[740/ 739] loss: 0.727937, Accuracy: 0.741839\n",
            "[741/ 740] loss: 0.727968, Accuracy: 0.741807\n",
            "[742/ 741] loss: 0.727934, Accuracy: 0.741840\n",
            "[743/ 742] loss: 0.728054, Accuracy: 0.741787\n",
            "[744/ 743] loss: 0.728282, Accuracy: 0.741693\n",
            "[745/ 744] loss: 0.728180, Accuracy: 0.741746\n",
            "[746/ 745] loss: 0.728427, Accuracy: 0.741674\n",
            "[747/ 746] loss: 0.728564, Accuracy: 0.741664\n",
            "[748/ 747] loss: 0.728335, Accuracy: 0.741738\n",
            "[749/ 748] loss: 0.728362, Accuracy: 0.741770\n",
            "[750/ 749] loss: 0.728438, Accuracy: 0.741760\n",
            "[751/ 750] loss: 0.728226, Accuracy: 0.741854\n",
            "[752/ 751] loss: 0.728469, Accuracy: 0.741761\n",
            "[753/ 752] loss: 0.728462, Accuracy: 0.741751\n",
            "[754/ 753] loss: 0.728376, Accuracy: 0.741783\n",
            "[755/ 754] loss: 0.728142, Accuracy: 0.741856\n",
            "[756/ 755] loss: 0.728051, Accuracy: 0.741887\n",
            "[757/ 756] loss: 0.728131, Accuracy: 0.741898\n",
            "[758/ 757] loss: 0.728115, Accuracy: 0.741847\n",
            "[759/ 758] loss: 0.728273, Accuracy: 0.741713\n",
            "[760/ 759] loss: 0.728447, Accuracy: 0.741642\n",
            "[761/ 760] loss: 0.728370, Accuracy: 0.741632\n",
            "[762/ 761] loss: 0.728323, Accuracy: 0.741684\n",
            "[763/ 762] loss: 0.728399, Accuracy: 0.741654\n",
            "[764/ 763] loss: 0.728229, Accuracy: 0.741747\n",
            "[765/ 764] loss: 0.728217, Accuracy: 0.741738\n",
            "[766/ 765] loss: 0.728263, Accuracy: 0.741708\n",
            "[767/ 766] loss: 0.728295, Accuracy: 0.741718\n",
            "[768/ 767] loss: 0.728146, Accuracy: 0.741770\n",
            "[769/ 768] loss: 0.728115, Accuracy: 0.741740\n",
            "[770/ 769] loss: 0.728287, Accuracy: 0.741690\n",
            "[771/ 770] loss: 0.727950, Accuracy: 0.741782\n",
            "[772/ 771] loss: 0.728304, Accuracy: 0.741650\n",
            "[773/ 772] loss: 0.728484, Accuracy: 0.741540\n",
            "[774/ 773] loss: 0.728555, Accuracy: 0.741510\n",
            "[775/ 774] loss: 0.728397, Accuracy: 0.741562\n",
            "[776/ 775] loss: 0.728165, Accuracy: 0.741653\n",
            "[777/ 776] loss: 0.728613, Accuracy: 0.741463\n",
            "[778/ 777] loss: 0.728827, Accuracy: 0.741393\n",
            "[779/ 778] loss: 0.728758, Accuracy: 0.741424\n",
            "[780/ 779] loss: 0.729109, Accuracy: 0.741335\n",
            "[781/ 780] loss: 0.728926, Accuracy: 0.741366\n",
            "[782/ 781] loss: 0.728574, Accuracy: 0.741537\n",
            "[783/ 782] loss: 0.728898, Accuracy: 0.741488\n",
            "[784/ 783] loss: 0.728800, Accuracy: 0.741519\n",
            "[785/ 784] loss: 0.728544, Accuracy: 0.741610\n",
            "[786/ 785] loss: 0.728985, Accuracy: 0.741461\n",
            "[787/ 786] loss: 0.728928, Accuracy: 0.741452\n",
            "[788/ 787] loss: 0.728720, Accuracy: 0.741522\n",
            "[789/ 788] loss: 0.728668, Accuracy: 0.741553\n",
            "[790/ 789] loss: 0.728527, Accuracy: 0.741603\n",
            "[791/ 790] loss: 0.728634, Accuracy: 0.741574\n",
            "[792/ 791] loss: 0.728615, Accuracy: 0.741644\n",
            "[793/ 792] loss: 0.728961, Accuracy: 0.741517\n",
            "[794/ 793] loss: 0.728682, Accuracy: 0.741587\n",
            "[795/ 794] loss: 0.728897, Accuracy: 0.741479\n",
            "[796/ 795] loss: 0.729287, Accuracy: 0.741333\n",
            "[797/ 796] loss: 0.729343, Accuracy: 0.741324\n",
            "[798/ 797] loss: 0.729206, Accuracy: 0.741394\n",
            "[799/ 798] loss: 0.729386, Accuracy: 0.741306\n",
            "[800/ 799] loss: 0.729182, Accuracy: 0.741337\n",
            "[801/ 800] loss: 0.729440, Accuracy: 0.741230\n",
            "[802/ 801] loss: 0.729720, Accuracy: 0.741144\n",
            "[803/ 802] loss: 0.729671, Accuracy: 0.741155\n",
            "[804/ 803] loss: 0.730053, Accuracy: 0.740991\n",
            "[805/ 804] loss: 0.729857, Accuracy: 0.741080\n",
            "[806/ 805] loss: 0.729703, Accuracy: 0.741110\n",
            "[807/ 806] loss: 0.729616, Accuracy: 0.741121\n",
            "[808/ 807] loss: 0.729693, Accuracy: 0.741094\n",
            "[809/ 808] loss: 0.729668, Accuracy: 0.741143\n",
            "[810/ 809] loss: 0.729539, Accuracy: 0.741193\n",
            "[811/ 810] loss: 0.729429, Accuracy: 0.741262\n",
            "[812/ 811] loss: 0.729671, Accuracy: 0.741234\n",
            "[813/ 812] loss: 0.729468, Accuracy: 0.741302\n",
            "[814/ 813] loss: 0.729429, Accuracy: 0.741275\n",
            "[815/ 814] loss: 0.729571, Accuracy: 0.741285\n",
            "[816/ 815] loss: 0.729311, Accuracy: 0.741411\n",
            "[817/ 816] loss: 0.729312, Accuracy: 0.741402\n",
            "[818/ 817] loss: 0.729137, Accuracy: 0.741489\n",
            "[819/ 818] loss: 0.729028, Accuracy: 0.741557\n",
            "[820/ 819] loss: 0.728951, Accuracy: 0.741567\n",
            "[821/ 820] loss: 0.729146, Accuracy: 0.741482\n",
            "[822/ 821] loss: 0.729068, Accuracy: 0.741455\n",
            "[823/ 822] loss: 0.729099, Accuracy: 0.741446\n",
            "[824/ 823] loss: 0.729604, Accuracy: 0.741267\n",
            "[825/ 824] loss: 0.729320, Accuracy: 0.741429\n",
            "[826/ 825] loss: 0.729222, Accuracy: 0.741458\n",
            "[827/ 826] loss: 0.729176, Accuracy: 0.741450\n",
            "[828/ 827] loss: 0.729263, Accuracy: 0.741441\n",
            "[829/ 828] loss: 0.729126, Accuracy: 0.741527\n",
            "[830/ 829] loss: 0.729328, Accuracy: 0.741462\n",
            "[831/ 830] loss: 0.729191, Accuracy: 0.741510\n",
            "[832/ 831] loss: 0.729174, Accuracy: 0.741520\n",
            "[833/ 832] loss: 0.729009, Accuracy: 0.741530\n",
            "[834/ 833] loss: 0.728850, Accuracy: 0.741615\n",
            "[835/ 834] loss: 0.728967, Accuracy: 0.741569\n",
            "[836/ 835] loss: 0.728911, Accuracy: 0.741654\n",
            "[837/ 836] loss: 0.728852, Accuracy: 0.741645\n",
            "[838/ 837] loss: 0.728702, Accuracy: 0.741711\n",
            "[839/ 838] loss: 0.728737, Accuracy: 0.741684\n",
            "[840/ 839] loss: 0.729042, Accuracy: 0.741601\n",
            "[841/ 840] loss: 0.729139, Accuracy: 0.741555\n",
            "[842/ 841] loss: 0.728917, Accuracy: 0.741621\n",
            "[843/ 842] loss: 0.728965, Accuracy: 0.741649\n",
            "[844/ 843] loss: 0.728742, Accuracy: 0.741715\n",
            "[845/ 844] loss: 0.728663, Accuracy: 0.741725\n",
            "[846/ 845] loss: 0.728524, Accuracy: 0.741790\n",
            "[847/ 846] loss: 0.728749, Accuracy: 0.741707\n",
            "[848/ 847] loss: 0.728923, Accuracy: 0.741643\n",
            "[849/ 848] loss: 0.728761, Accuracy: 0.741690\n",
            "[850/ 849] loss: 0.728563, Accuracy: 0.741755\n",
            "[851/ 850] loss: 0.728570, Accuracy: 0.741746\n",
            "[852/ 851] loss: 0.728457, Accuracy: 0.741774\n",
            "[853/ 852] loss: 0.728395, Accuracy: 0.741766\n",
            "[854/ 853] loss: 0.728383, Accuracy: 0.741830\n",
            "[855/ 854] loss: 0.728699, Accuracy: 0.741748\n",
            "[856/ 855] loss: 0.728598, Accuracy: 0.741831\n",
            "[857/ 856] loss: 0.728474, Accuracy: 0.741877\n",
            "[858/ 857] loss: 0.728312, Accuracy: 0.741960\n",
            "[859/ 858] loss: 0.728564, Accuracy: 0.741860\n",
            "[860/ 859] loss: 0.728704, Accuracy: 0.741724\n",
            "[861/ 860] loss: 0.728716, Accuracy: 0.741715\n",
            "[862/ 861] loss: 0.728525, Accuracy: 0.741815\n",
            "[863/ 862] loss: 0.728650, Accuracy: 0.741734\n",
            "[864/ 863] loss: 0.728780, Accuracy: 0.741690\n",
            "[865/ 864] loss: 0.728882, Accuracy: 0.741627\n",
            "[866/ 865] loss: 0.729021, Accuracy: 0.741564\n",
            "[867/ 866] loss: 0.728943, Accuracy: 0.741592\n",
            "[868/ 867] loss: 0.729001, Accuracy: 0.741638\n",
            "[869/ 868] loss: 0.728908, Accuracy: 0.741665\n",
            "[870/ 869] loss: 0.728896, Accuracy: 0.741657\n",
            "[871/ 870] loss: 0.728949, Accuracy: 0.741667\n",
            "[872/ 871] loss: 0.729182, Accuracy: 0.741605\n",
            "[873/ 872] loss: 0.729143, Accuracy: 0.741650\n",
            "[874/ 873] loss: 0.729068, Accuracy: 0.741695\n",
            "[875/ 874] loss: 0.729099, Accuracy: 0.741687\n",
            "[876/ 875] loss: 0.728968, Accuracy: 0.741768\n",
            "[877/ 876] loss: 0.728835, Accuracy: 0.741813\n",
            "[878/ 877] loss: 0.728703, Accuracy: 0.741894\n",
            "[879/ 878] loss: 0.728926, Accuracy: 0.741760\n",
            "[880/ 879] loss: 0.728926, Accuracy: 0.741770\n",
            "[881/ 880] loss: 0.729127, Accuracy: 0.741673\n",
            "[882/ 881] loss: 0.729232, Accuracy: 0.741629\n",
            "[883/ 882] loss: 0.729502, Accuracy: 0.741532\n",
            "[884/ 883] loss: 0.729555, Accuracy: 0.741489\n",
            "[885/ 884] loss: 0.729429, Accuracy: 0.741516\n",
            "[886/ 885] loss: 0.729661, Accuracy: 0.741402\n",
            "[887/ 886] loss: 0.729338, Accuracy: 0.741553\n",
            "[888/ 887] loss: 0.729408, Accuracy: 0.741545\n",
            "[889/ 888] loss: 0.729426, Accuracy: 0.741536\n",
            "[890/ 889] loss: 0.729412, Accuracy: 0.741528\n",
            "[891/ 890] loss: 0.729308, Accuracy: 0.741573\n",
            "[892/ 891] loss: 0.729668, Accuracy: 0.741512\n",
            "[893/ 892] loss: 0.729856, Accuracy: 0.741469\n",
            "[894/ 893] loss: 0.729711, Accuracy: 0.741514\n",
            "[895/ 894] loss: 0.729625, Accuracy: 0.741523\n",
            "[896/ 895] loss: 0.729620, Accuracy: 0.741515\n",
            "[897/ 896] loss: 0.729510, Accuracy: 0.741560\n",
            "[898/ 897] loss: 0.729402, Accuracy: 0.741621\n",
            "[899/ 898] loss: 0.729442, Accuracy: 0.741613\n",
            "[900/ 899] loss: 0.729559, Accuracy: 0.741536\n",
            "[901 / 900]  Loss:0.729930, Accuracy:0.741458\n",
            "[901/ 900] loss: 0.729930, Accuracy: 0.741458\n",
            "[902/ 901] loss: 0.729907, Accuracy: 0.741450\n",
            "[903/ 902] loss: 0.729848, Accuracy: 0.741495\n",
            "[904/ 903] loss: 0.729817, Accuracy: 0.741487\n",
            "[905/ 904] loss: 0.729698, Accuracy: 0.741513\n",
            "[906/ 905] loss: 0.729814, Accuracy: 0.741454\n",
            "[907/ 906] loss: 0.729487, Accuracy: 0.741549\n",
            "[908/ 907] loss: 0.729433, Accuracy: 0.741576\n",
            "[909/ 908] loss: 0.729084, Accuracy: 0.741688\n",
            "[910/ 909] loss: 0.729407, Accuracy: 0.741560\n",
            "[911/ 910] loss: 0.729443, Accuracy: 0.741535\n",
            "[912/ 911] loss: 0.729431, Accuracy: 0.741579\n",
            "[913/ 912] loss: 0.729411, Accuracy: 0.741605\n",
            "[914/ 913] loss: 0.729190, Accuracy: 0.741700\n",
            "[915/ 914] loss: 0.729355, Accuracy: 0.741658\n",
            "[916/ 915] loss: 0.729491, Accuracy: 0.741615\n",
            "[917/ 916] loss: 0.729451, Accuracy: 0.741642\n",
            "[918/ 917] loss: 0.730025, Accuracy: 0.741446\n",
            "[919/ 918] loss: 0.730244, Accuracy: 0.741319\n",
            "[920/ 919] loss: 0.730318, Accuracy: 0.741295\n",
            "[921/ 920] loss: 0.730315, Accuracy: 0.741304\n",
            "[922/ 921] loss: 0.730149, Accuracy: 0.741365\n",
            "[923/ 922] loss: 0.730129, Accuracy: 0.741357\n",
            "[924/ 923] loss: 0.730202, Accuracy: 0.741366\n",
            "[925/ 924] loss: 0.730079, Accuracy: 0.741393\n",
            "[926/ 925] loss: 0.729988, Accuracy: 0.741436\n",
            "[927/ 926] loss: 0.730080, Accuracy: 0.741394\n",
            "[928/ 927] loss: 0.730114, Accuracy: 0.741387\n",
            "[929/ 928] loss: 0.730060, Accuracy: 0.741413\n",
            "[930/ 929] loss: 0.729904, Accuracy: 0.741473\n",
            "[931/ 930] loss: 0.729971, Accuracy: 0.741448\n",
            "[932/ 931] loss: 0.729888, Accuracy: 0.741441\n",
            "[933/ 932] loss: 0.730124, Accuracy: 0.741366\n",
            "[934/ 933] loss: 0.730229, Accuracy: 0.741325\n",
            "[935/ 934] loss: 0.730045, Accuracy: 0.741401\n",
            "[936/ 935] loss: 0.729659, Accuracy: 0.741561\n",
            "[937/ 936] loss: 0.729607, Accuracy: 0.741587\n",
            "[938/ 937] loss: 0.729783, Accuracy: 0.741545\n",
            "[939/ 938] loss: 0.729751, Accuracy: 0.741554\n",
            "epochs 6\n",
            "[2/ 1] loss: 1.014314, Accuracy: 0.656250\n",
            "[3/ 2] loss: 0.734503, Accuracy: 0.750000\n",
            "[4/ 3] loss: 0.722445, Accuracy: 0.755208\n",
            "[5/ 4] loss: 0.751496, Accuracy: 0.738281\n",
            "[6/ 5] loss: 0.738815, Accuracy: 0.737500\n",
            "[7/ 6] loss: 0.769940, Accuracy: 0.734375\n",
            "[8/ 7] loss: 0.743401, Accuracy: 0.743304\n",
            "[9/ 8] loss: 0.717357, Accuracy: 0.751953\n",
            "[10/ 9] loss: 0.708342, Accuracy: 0.755208\n",
            "[11/ 10] loss: 0.677792, Accuracy: 0.765625\n",
            "[12/ 11] loss: 0.673652, Accuracy: 0.765625\n",
            "[13/ 12] loss: 0.674845, Accuracy: 0.764323\n",
            "[14/ 13] loss: 0.672806, Accuracy: 0.763221\n",
            "[15/ 14] loss: 0.661570, Accuracy: 0.765625\n",
            "[16/ 15] loss: 0.678663, Accuracy: 0.758333\n",
            "[17/ 16] loss: 0.663672, Accuracy: 0.762695\n",
            "[18/ 17] loss: 0.673290, Accuracy: 0.759191\n",
            "[19/ 18] loss: 0.673495, Accuracy: 0.758681\n",
            "[20/ 19] loss: 0.676215, Accuracy: 0.755757\n",
            "[21/ 20] loss: 0.680950, Accuracy: 0.755469\n",
            "[22/ 21] loss: 0.685929, Accuracy: 0.752232\n",
            "[23/ 22] loss: 0.678514, Accuracy: 0.754972\n",
            "[24/ 23] loss: 0.692425, Accuracy: 0.747962\n",
            "[25/ 24] loss: 0.683364, Accuracy: 0.752604\n",
            "[26/ 25] loss: 0.688519, Accuracy: 0.751250\n",
            "[27/ 26] loss: 0.688738, Accuracy: 0.752404\n",
            "[28/ 27] loss: 0.688634, Accuracy: 0.751736\n",
            "[29/ 28] loss: 0.688792, Accuracy: 0.750558\n",
            "[30/ 29] loss: 0.691988, Accuracy: 0.750539\n",
            "[31/ 30] loss: 0.691400, Accuracy: 0.751562\n",
            "[32/ 31] loss: 0.694037, Accuracy: 0.751008\n",
            "[33/ 32] loss: 0.696370, Accuracy: 0.749512\n",
            "[34/ 33] loss: 0.696302, Accuracy: 0.750473\n",
            "[35/ 34] loss: 0.699174, Accuracy: 0.750000\n",
            "[36/ 35] loss: 0.698990, Accuracy: 0.750000\n",
            "[37/ 36] loss: 0.697830, Accuracy: 0.750868\n",
            "[38/ 37] loss: 0.703286, Accuracy: 0.748733\n",
            "[39/ 38] loss: 0.702847, Accuracy: 0.750000\n",
            "[40/ 39] loss: 0.704870, Accuracy: 0.748798\n",
            "[41/ 40] loss: 0.707255, Accuracy: 0.748047\n",
            "[42/ 41] loss: 0.704658, Accuracy: 0.750000\n",
            "[43/ 42] loss: 0.703504, Accuracy: 0.750000\n",
            "[44/ 43] loss: 0.699482, Accuracy: 0.751090\n",
            "[45/ 44] loss: 0.700378, Accuracy: 0.750710\n",
            "[46/ 45] loss: 0.700713, Accuracy: 0.750347\n",
            "[47/ 46] loss: 0.702916, Accuracy: 0.749321\n",
            "[48/ 47] loss: 0.707004, Accuracy: 0.747673\n",
            "[49/ 48] loss: 0.704334, Accuracy: 0.748372\n",
            "[50/ 49] loss: 0.705053, Accuracy: 0.748406\n",
            "[51/ 50] loss: 0.711701, Accuracy: 0.746562\n",
            "[52/ 51] loss: 0.714565, Accuracy: 0.745711\n",
            "[53/ 52] loss: 0.712581, Accuracy: 0.746394\n",
            "[54/ 53] loss: 0.714473, Accuracy: 0.744988\n",
            "[55/ 54] loss: 0.714452, Accuracy: 0.745660\n",
            "[56/ 55] loss: 0.716851, Accuracy: 0.744318\n",
            "[57/ 56] loss: 0.720854, Accuracy: 0.743304\n",
            "[58/ 57] loss: 0.720997, Accuracy: 0.742599\n",
            "[59/ 58] loss: 0.718901, Accuracy: 0.743535\n",
            "[60/ 59] loss: 0.718244, Accuracy: 0.743644\n",
            "[61/ 60] loss: 0.718283, Accuracy: 0.744010\n",
            "[62/ 61] loss: 0.723523, Accuracy: 0.741803\n",
            "[63/ 62] loss: 0.720088, Accuracy: 0.743196\n",
            "[64/ 63] loss: 0.719103, Accuracy: 0.743552\n",
            "[65/ 64] loss: 0.715896, Accuracy: 0.744629\n",
            "[66/ 65] loss: 0.715524, Accuracy: 0.744952\n",
            "[67/ 66] loss: 0.716218, Accuracy: 0.744792\n",
            "[68/ 67] loss: 0.715191, Accuracy: 0.745336\n",
            "[69/ 68] loss: 0.714036, Accuracy: 0.746324\n",
            "[70/ 69] loss: 0.714251, Accuracy: 0.746603\n",
            "[71/ 70] loss: 0.712185, Accuracy: 0.747098\n",
            "[72/ 71] loss: 0.713041, Accuracy: 0.746699\n",
            "[73/ 72] loss: 0.715105, Accuracy: 0.746311\n",
            "[74/ 73] loss: 0.715578, Accuracy: 0.745933\n",
            "[75/ 74] loss: 0.714864, Accuracy: 0.746199\n",
            "[76/ 75] loss: 0.715566, Accuracy: 0.746042\n",
            "[77/ 76] loss: 0.713544, Accuracy: 0.746711\n",
            "[78/ 77] loss: 0.717060, Accuracy: 0.745942\n",
            "[79/ 78] loss: 0.719395, Accuracy: 0.744591\n",
            "[80/ 79] loss: 0.718977, Accuracy: 0.744264\n",
            "[81/ 80] loss: 0.719686, Accuracy: 0.743750\n",
            "[82/ 81] loss: 0.720540, Accuracy: 0.743441\n",
            "[83/ 82] loss: 0.722230, Accuracy: 0.742759\n",
            "[84/ 83] loss: 0.720273, Accuracy: 0.743411\n",
            "[85/ 84] loss: 0.719728, Accuracy: 0.743676\n",
            "[86/ 85] loss: 0.720204, Accuracy: 0.743382\n",
            "[87/ 86] loss: 0.717117, Accuracy: 0.744368\n",
            "[88/ 87] loss: 0.715406, Accuracy: 0.744971\n",
            "[89/ 88] loss: 0.715900, Accuracy: 0.745561\n",
            "[90/ 89] loss: 0.716357, Accuracy: 0.745611\n",
            "[91/ 90] loss: 0.715177, Accuracy: 0.745660\n",
            "[92/ 91] loss: 0.714800, Accuracy: 0.745707\n",
            "[93/ 92] loss: 0.712773, Accuracy: 0.746773\n",
            "[94/ 93] loss: 0.714786, Accuracy: 0.746136\n",
            "[95/ 94] loss: 0.715628, Accuracy: 0.746011\n",
            "[96/ 95] loss: 0.715491, Accuracy: 0.746382\n",
            "[97/ 96] loss: 0.713971, Accuracy: 0.747070\n",
            "[98/ 97] loss: 0.716668, Accuracy: 0.746295\n",
            "[99/ 98] loss: 0.717120, Accuracy: 0.746014\n",
            "[100/ 99] loss: 0.720737, Accuracy: 0.744950\n",
            "[101/ 100] loss: 0.718647, Accuracy: 0.745469\n",
            "[102/ 101] loss: 0.719217, Accuracy: 0.745204\n",
            "[103/ 102] loss: 0.716831, Accuracy: 0.746170\n",
            "[104/ 103] loss: 0.716876, Accuracy: 0.746208\n",
            "[105/ 104] loss: 0.718251, Accuracy: 0.745643\n",
            "[106/ 105] loss: 0.718487, Accuracy: 0.745536\n",
            "[107/ 106] loss: 0.718005, Accuracy: 0.745430\n",
            "[108/ 107] loss: 0.716848, Accuracy: 0.745911\n",
            "[109/ 108] loss: 0.716390, Accuracy: 0.745949\n",
            "[110/ 109] loss: 0.716919, Accuracy: 0.745843\n",
            "[111/ 110] loss: 0.716681, Accuracy: 0.746023\n",
            "[112/ 111] loss: 0.717213, Accuracy: 0.746199\n",
            "[113/ 112] loss: 0.718169, Accuracy: 0.745954\n",
            "[114/ 113] loss: 0.719474, Accuracy: 0.745437\n",
            "[115/ 114] loss: 0.719919, Accuracy: 0.745066\n",
            "[116/ 115] loss: 0.720093, Accuracy: 0.744973\n",
            "[117/ 116] loss: 0.720818, Accuracy: 0.744477\n",
            "[118/ 117] loss: 0.720099, Accuracy: 0.744792\n",
            "[119/ 118] loss: 0.722134, Accuracy: 0.744174\n",
            "[120/ 119] loss: 0.720890, Accuracy: 0.744879\n",
            "[121/ 120] loss: 0.719661, Accuracy: 0.745052\n",
            "[122/ 121] loss: 0.718828, Accuracy: 0.745351\n",
            "[123/ 122] loss: 0.718300, Accuracy: 0.745645\n",
            "[124/ 123] loss: 0.719130, Accuracy: 0.745300\n",
            "[125/ 124] loss: 0.717734, Accuracy: 0.745716\n",
            "[126/ 125] loss: 0.716869, Accuracy: 0.745875\n",
            "[127/ 126] loss: 0.717794, Accuracy: 0.745412\n",
            "[128/ 127] loss: 0.718320, Accuracy: 0.745079\n",
            "[129/ 128] loss: 0.717306, Accuracy: 0.745605\n",
            "[130/ 129] loss: 0.718842, Accuracy: 0.744913\n",
            "[131/ 130] loss: 0.719677, Accuracy: 0.744591\n",
            "[132/ 131] loss: 0.719958, Accuracy: 0.744394\n",
            "[133/ 132] loss: 0.719055, Accuracy: 0.744555\n",
            "[134/ 133] loss: 0.718868, Accuracy: 0.744478\n",
            "[135/ 134] loss: 0.718200, Accuracy: 0.744520\n",
            "[136/ 135] loss: 0.717673, Accuracy: 0.744676\n",
            "[137/ 136] loss: 0.718763, Accuracy: 0.744485\n",
            "[138/ 137] loss: 0.721697, Accuracy: 0.743499\n",
            "[139/ 138] loss: 0.720274, Accuracy: 0.743886\n",
            "[140/ 139] loss: 0.719436, Accuracy: 0.744492\n",
            "[141/ 140] loss: 0.718681, Accuracy: 0.744754\n",
            "[142/ 141] loss: 0.717766, Accuracy: 0.745013\n",
            "[143/ 142] loss: 0.716416, Accuracy: 0.745378\n",
            "[144/ 143] loss: 0.716892, Accuracy: 0.745520\n",
            "[145/ 144] loss: 0.716839, Accuracy: 0.745334\n",
            "[146/ 145] loss: 0.716563, Accuracy: 0.745366\n",
            "[147/ 146] loss: 0.715312, Accuracy: 0.745719\n",
            "[148/ 147] loss: 0.715641, Accuracy: 0.745429\n",
            "[149/ 148] loss: 0.714471, Accuracy: 0.745777\n",
            "[150/ 149] loss: 0.713707, Accuracy: 0.746015\n",
            "[151/ 150] loss: 0.714005, Accuracy: 0.745833\n",
            "[152/ 151] loss: 0.713126, Accuracy: 0.746068\n",
            "[153/ 152] loss: 0.714844, Accuracy: 0.745169\n",
            "[154/ 153] loss: 0.716433, Accuracy: 0.744690\n",
            "[155/ 154] loss: 0.716302, Accuracy: 0.744927\n",
            "[156/ 155] loss: 0.716964, Accuracy: 0.744556\n",
            "[157/ 156] loss: 0.717229, Accuracy: 0.744491\n",
            "[158/ 157] loss: 0.716657, Accuracy: 0.744427\n",
            "[159/ 158] loss: 0.716086, Accuracy: 0.744561\n",
            "[160/ 159] loss: 0.715583, Accuracy: 0.744792\n",
            "[161/ 160] loss: 0.715287, Accuracy: 0.744824\n",
            "[162/ 161] loss: 0.715022, Accuracy: 0.744953\n",
            "[163/ 162] loss: 0.715432, Accuracy: 0.744792\n",
            "[164/ 163] loss: 0.716441, Accuracy: 0.744632\n",
            "[165/ 164] loss: 0.716944, Accuracy: 0.744284\n",
            "[166/ 165] loss: 0.716371, Accuracy: 0.744413\n",
            "[167/ 166] loss: 0.714962, Accuracy: 0.745200\n",
            "[168/ 167] loss: 0.714454, Accuracy: 0.745322\n",
            "[169/ 168] loss: 0.713376, Accuracy: 0.745908\n",
            "[170/ 169] loss: 0.712775, Accuracy: 0.746024\n",
            "[171/ 170] loss: 0.711983, Accuracy: 0.746140\n",
            "[172/ 171] loss: 0.712199, Accuracy: 0.745797\n",
            "[173/ 172] loss: 0.712787, Accuracy: 0.745549\n",
            "[174/ 173] loss: 0.711359, Accuracy: 0.745936\n",
            "[175/ 174] loss: 0.708866, Accuracy: 0.746857\n",
            "[176/ 175] loss: 0.709229, Accuracy: 0.746786\n",
            "[177/ 176] loss: 0.709322, Accuracy: 0.746449\n",
            "[178/ 177] loss: 0.709405, Accuracy: 0.746557\n",
            "[179/ 178] loss: 0.709083, Accuracy: 0.746664\n",
            "[180/ 179] loss: 0.709660, Accuracy: 0.746508\n",
            "[181/ 180] loss: 0.710116, Accuracy: 0.746441\n",
            "[182/ 181] loss: 0.709160, Accuracy: 0.746720\n",
            "[183/ 182] loss: 0.711236, Accuracy: 0.745965\n",
            "[184/ 183] loss: 0.710614, Accuracy: 0.746158\n",
            "[185/ 184] loss: 0.711165, Accuracy: 0.746009\n",
            "[186/ 185] loss: 0.713195, Accuracy: 0.745270\n",
            "[187/ 186] loss: 0.713588, Accuracy: 0.745044\n",
            "[188/ 187] loss: 0.713552, Accuracy: 0.744987\n",
            "[189/ 188] loss: 0.713254, Accuracy: 0.744847\n",
            "[190/ 189] loss: 0.713217, Accuracy: 0.744874\n",
            "[191/ 190] loss: 0.712884, Accuracy: 0.744737\n",
            "[192/ 191] loss: 0.713602, Accuracy: 0.744355\n",
            "[193/ 192] loss: 0.712877, Accuracy: 0.744548\n",
            "[194/ 193] loss: 0.713130, Accuracy: 0.744495\n",
            "[195/ 194] loss: 0.712340, Accuracy: 0.744926\n",
            "[196/ 195] loss: 0.712890, Accuracy: 0.744631\n",
            "[197/ 196] loss: 0.712211, Accuracy: 0.745057\n",
            "[198/ 197] loss: 0.711790, Accuracy: 0.745082\n",
            "[199/ 198] loss: 0.712830, Accuracy: 0.745028\n",
            "[200/ 199] loss: 0.713100, Accuracy: 0.745210\n",
            "[201/ 200] loss: 0.712741, Accuracy: 0.745391\n",
            "[202/ 201] loss: 0.713096, Accuracy: 0.745336\n",
            "[203/ 202] loss: 0.714069, Accuracy: 0.745127\n",
            "[204/ 203] loss: 0.714374, Accuracy: 0.745151\n",
            "[205/ 204] loss: 0.714581, Accuracy: 0.744945\n",
            "[206/ 205] loss: 0.713956, Accuracy: 0.745274\n",
            "[207/ 206] loss: 0.714117, Accuracy: 0.745146\n",
            "[208/ 207] loss: 0.715035, Accuracy: 0.744792\n",
            "[209/ 208] loss: 0.714531, Accuracy: 0.744892\n",
            "[210/ 209] loss: 0.713639, Accuracy: 0.745141\n",
            "[211/ 210] loss: 0.714457, Accuracy: 0.744717\n",
            "[212/ 211] loss: 0.714447, Accuracy: 0.744594\n",
            "[213/ 212] loss: 0.714444, Accuracy: 0.744620\n",
            "[214/ 213] loss: 0.714864, Accuracy: 0.744425\n",
            "[215/ 214] loss: 0.714746, Accuracy: 0.744524\n",
            "[216/ 215] loss: 0.714817, Accuracy: 0.744549\n",
            "[217/ 216] loss: 0.715266, Accuracy: 0.744430\n",
            "[218/ 217] loss: 0.716120, Accuracy: 0.744096\n",
            "[219/ 218] loss: 0.717476, Accuracy: 0.743621\n",
            "[220/ 219] loss: 0.716998, Accuracy: 0.744007\n",
            "[221/ 220] loss: 0.717118, Accuracy: 0.743892\n",
            "[222/ 221] loss: 0.716531, Accuracy: 0.744132\n",
            "[223/ 222] loss: 0.716613, Accuracy: 0.744229\n",
            "[224/ 223] loss: 0.717044, Accuracy: 0.743974\n",
            "[225/ 224] loss: 0.717246, Accuracy: 0.744001\n",
            "[226/ 225] loss: 0.716139, Accuracy: 0.744375\n",
            "[227/ 226] loss: 0.716088, Accuracy: 0.744400\n",
            "[228/ 227] loss: 0.716015, Accuracy: 0.744356\n",
            "[229/ 228] loss: 0.715789, Accuracy: 0.744449\n",
            "[230/ 229] loss: 0.716050, Accuracy: 0.744405\n",
            "[231/ 230] loss: 0.715403, Accuracy: 0.744565\n",
            "[232/ 231] loss: 0.716333, Accuracy: 0.744183\n",
            "[233/ 232] loss: 0.715633, Accuracy: 0.744410\n",
            "[234/ 233] loss: 0.715631, Accuracy: 0.744434\n",
            "[235/ 234] loss: 0.715876, Accuracy: 0.744324\n",
            "[236/ 235] loss: 0.714649, Accuracy: 0.744747\n",
            "[237/ 236] loss: 0.712881, Accuracy: 0.745498\n",
            "[238/ 237] loss: 0.713394, Accuracy: 0.745319\n",
            "[239/ 238] loss: 0.712514, Accuracy: 0.745733\n",
            "[240/ 239] loss: 0.712468, Accuracy: 0.745685\n",
            "[241/ 240] loss: 0.711845, Accuracy: 0.745898\n",
            "[242/ 241] loss: 0.712241, Accuracy: 0.745851\n",
            "[243/ 242] loss: 0.713378, Accuracy: 0.745739\n",
            "[244/ 243] loss: 0.713603, Accuracy: 0.745563\n",
            "[245/ 244] loss: 0.713718, Accuracy: 0.745389\n",
            "[246/ 245] loss: 0.714316, Accuracy: 0.745281\n",
            "[247/ 246] loss: 0.715342, Accuracy: 0.744855\n",
            "[248/ 247] loss: 0.715368, Accuracy: 0.744876\n",
            "[249/ 248] loss: 0.715283, Accuracy: 0.744897\n",
            "[250/ 249] loss: 0.715042, Accuracy: 0.744917\n",
            "[251/ 250] loss: 0.714033, Accuracy: 0.745250\n",
            "[252/ 251] loss: 0.713554, Accuracy: 0.745393\n",
            "[253/ 252] loss: 0.713892, Accuracy: 0.745288\n",
            "[254/ 253] loss: 0.713406, Accuracy: 0.745430\n",
            "[255/ 254] loss: 0.713419, Accuracy: 0.745509\n",
            "[256/ 255] loss: 0.713102, Accuracy: 0.745650\n",
            "[257/ 256] loss: 0.713447, Accuracy: 0.745605\n",
            "[258/ 257] loss: 0.714185, Accuracy: 0.745319\n",
            "[259/ 258] loss: 0.713816, Accuracy: 0.745518\n",
            "[260/ 259] loss: 0.713788, Accuracy: 0.745536\n",
            "[261/ 260] loss: 0.713600, Accuracy: 0.745553\n",
            "[262/ 261] loss: 0.713836, Accuracy: 0.745450\n",
            "[263/ 262] loss: 0.713776, Accuracy: 0.745468\n",
            "[264/ 263] loss: 0.712898, Accuracy: 0.745663\n",
            "[265/ 264] loss: 0.712315, Accuracy: 0.745739\n",
            "[266/ 265] loss: 0.712010, Accuracy: 0.745873\n",
            "[267/ 266] loss: 0.712072, Accuracy: 0.745829\n",
            "[268/ 267] loss: 0.713529, Accuracy: 0.745552\n",
            "[269/ 268] loss: 0.712789, Accuracy: 0.745861\n",
            "[270/ 269] loss: 0.712791, Accuracy: 0.745818\n",
            "[271/ 270] loss: 0.712201, Accuracy: 0.746123\n",
            "[272/ 271] loss: 0.712142, Accuracy: 0.746079\n",
            "[273/ 272] loss: 0.712476, Accuracy: 0.745921\n",
            "[274/ 273] loss: 0.711354, Accuracy: 0.746337\n",
            "[275/ 274] loss: 0.712163, Accuracy: 0.746122\n",
            "[276/ 275] loss: 0.712341, Accuracy: 0.746080\n",
            "[277/ 276] loss: 0.712796, Accuracy: 0.745811\n",
            "[278/ 277] loss: 0.713485, Accuracy: 0.745375\n",
            "[279/ 278] loss: 0.713866, Accuracy: 0.745110\n",
            "[280/ 279] loss: 0.713389, Accuracy: 0.745296\n",
            "[281/ 280] loss: 0.713409, Accuracy: 0.745313\n",
            "[282/ 281] loss: 0.714253, Accuracy: 0.744996\n",
            "[283/ 282] loss: 0.714132, Accuracy: 0.745069\n",
            "[284/ 283] loss: 0.713351, Accuracy: 0.745252\n",
            "[285/ 284] loss: 0.713326, Accuracy: 0.745323\n",
            "[286/ 285] loss: 0.712756, Accuracy: 0.745504\n",
            "[287/ 286] loss: 0.713293, Accuracy: 0.745411\n",
            "[288/ 287] loss: 0.713272, Accuracy: 0.745372\n",
            "[289/ 288] loss: 0.713507, Accuracy: 0.745171\n",
            "[290/ 289] loss: 0.713427, Accuracy: 0.745134\n",
            "[291/ 290] loss: 0.713991, Accuracy: 0.744935\n",
            "[292/ 291] loss: 0.714174, Accuracy: 0.744845\n",
            "[293/ 292] loss: 0.713973, Accuracy: 0.744863\n",
            "[294/ 293] loss: 0.714357, Accuracy: 0.744667\n",
            "[295/ 294] loss: 0.714646, Accuracy: 0.744526\n",
            "[296/ 295] loss: 0.713927, Accuracy: 0.744862\n",
            "[297/ 296] loss: 0.713727, Accuracy: 0.745038\n",
            "[298/ 297] loss: 0.713651, Accuracy: 0.745160\n",
            "[299/ 298] loss: 0.713566, Accuracy: 0.745176\n",
            "[300/ 299] loss: 0.712478, Accuracy: 0.745558\n",
            "[301 / 300]  Loss:0.712946, Accuracy:0.745417\n",
            "[301/ 300] loss: 0.712946, Accuracy: 0.745417\n",
            "[302/ 301] loss: 0.713916, Accuracy: 0.745120\n",
            "[303/ 302] loss: 0.714250, Accuracy: 0.745085\n",
            "[304/ 303] loss: 0.714149, Accuracy: 0.745204\n",
            "[305/ 304] loss: 0.714298, Accuracy: 0.745117\n",
            "[306/ 305] loss: 0.714950, Accuracy: 0.744928\n",
            "[307/ 306] loss: 0.715171, Accuracy: 0.744843\n",
            "[308/ 307] loss: 0.715076, Accuracy: 0.744809\n",
            "[309/ 308] loss: 0.715795, Accuracy: 0.744420\n",
            "[310/ 309] loss: 0.716342, Accuracy: 0.744286\n",
            "[311/ 310] loss: 0.716928, Accuracy: 0.744103\n",
            "[312/ 311] loss: 0.716634, Accuracy: 0.744222\n",
            "[313/ 312] loss: 0.717102, Accuracy: 0.744091\n",
            "[314/ 313] loss: 0.716127, Accuracy: 0.744409\n",
            "[315/ 314] loss: 0.715638, Accuracy: 0.744576\n",
            "[316/ 315] loss: 0.716187, Accuracy: 0.744345\n",
            "[317/ 316] loss: 0.716094, Accuracy: 0.744462\n",
            "[318/ 317] loss: 0.716078, Accuracy: 0.744381\n",
            "[319/ 318] loss: 0.716518, Accuracy: 0.744202\n",
            "[320/ 319] loss: 0.716762, Accuracy: 0.744171\n",
            "[321/ 320] loss: 0.717031, Accuracy: 0.743994\n",
            "[322/ 321] loss: 0.716780, Accuracy: 0.744062\n",
            "[323/ 322] loss: 0.716556, Accuracy: 0.744031\n",
            "[324/ 323] loss: 0.716661, Accuracy: 0.744002\n",
            "[325/ 324] loss: 0.717182, Accuracy: 0.743827\n",
            "[326/ 325] loss: 0.716559, Accuracy: 0.744038\n",
            "[327/ 326] loss: 0.717376, Accuracy: 0.743673\n",
            "[328/ 327] loss: 0.717759, Accuracy: 0.743597\n",
            "[329/ 328] loss: 0.717743, Accuracy: 0.743521\n",
            "[330/ 329] loss: 0.718103, Accuracy: 0.743446\n",
            "[331/ 330] loss: 0.718275, Accuracy: 0.743277\n",
            "[332/ 331] loss: 0.718497, Accuracy: 0.743202\n",
            "[333/ 332] loss: 0.718673, Accuracy: 0.743082\n",
            "[334/ 333] loss: 0.719179, Accuracy: 0.742915\n",
            "[335/ 334] loss: 0.719904, Accuracy: 0.742609\n",
            "[336/ 335] loss: 0.719585, Accuracy: 0.742724\n",
            "[337/ 336] loss: 0.719366, Accuracy: 0.742792\n",
            "[338/ 337] loss: 0.718708, Accuracy: 0.743045\n",
            "[339/ 338] loss: 0.718604, Accuracy: 0.743020\n",
            "[340/ 339] loss: 0.718942, Accuracy: 0.742902\n",
            "[341/ 340] loss: 0.718095, Accuracy: 0.743290\n",
            "[342/ 341] loss: 0.717944, Accuracy: 0.743310\n",
            "[343/ 342] loss: 0.717587, Accuracy: 0.743558\n",
            "[344/ 343] loss: 0.717478, Accuracy: 0.743577\n",
            "[345/ 344] loss: 0.717737, Accuracy: 0.743414\n",
            "[346/ 345] loss: 0.716945, Accuracy: 0.743659\n",
            "[347/ 346] loss: 0.717391, Accuracy: 0.743542\n",
            "[348/ 347] loss: 0.717533, Accuracy: 0.743426\n",
            "[349/ 348] loss: 0.716956, Accuracy: 0.743669\n",
            "[350/ 349] loss: 0.717067, Accuracy: 0.743553\n",
            "[351/ 350] loss: 0.717627, Accuracy: 0.743527\n",
            "[352/ 351] loss: 0.717875, Accuracy: 0.743412\n",
            "[353/ 352] loss: 0.717252, Accuracy: 0.743697\n",
            "[354/ 353] loss: 0.717022, Accuracy: 0.743759\n",
            "[355/ 354] loss: 0.716548, Accuracy: 0.743865\n",
            "[356/ 355] loss: 0.716176, Accuracy: 0.743882\n",
            "[357/ 356] loss: 0.715707, Accuracy: 0.744031\n",
            "[358/ 357] loss: 0.715354, Accuracy: 0.744135\n",
            "[359/ 358] loss: 0.715794, Accuracy: 0.744021\n",
            "[360/ 359] loss: 0.715681, Accuracy: 0.744124\n",
            "[361/ 360] loss: 0.715870, Accuracy: 0.743967\n",
            "[362/ 361] loss: 0.715326, Accuracy: 0.744157\n",
            "[363/ 362] loss: 0.715789, Accuracy: 0.744000\n",
            "[364/ 363] loss: 0.715931, Accuracy: 0.743888\n",
            "[365/ 364] loss: 0.715792, Accuracy: 0.743905\n",
            "[366/ 365] loss: 0.715683, Accuracy: 0.743921\n",
            "[367/ 366] loss: 0.715897, Accuracy: 0.743810\n",
            "[368/ 367] loss: 0.715429, Accuracy: 0.743954\n",
            "[369/ 368] loss: 0.715794, Accuracy: 0.743801\n",
            "[370/ 369] loss: 0.715129, Accuracy: 0.744114\n",
            "[371/ 370] loss: 0.715225, Accuracy: 0.744088\n",
            "[372/ 371] loss: 0.715504, Accuracy: 0.743977\n",
            "[373/ 372] loss: 0.715252, Accuracy: 0.744036\n",
            "[374/ 373] loss: 0.715688, Accuracy: 0.743800\n",
            "[375/ 374] loss: 0.715670, Accuracy: 0.743733\n",
            "[376/ 375] loss: 0.715364, Accuracy: 0.743833\n",
            "[377/ 376] loss: 0.715410, Accuracy: 0.743891\n",
            "[378/ 377] loss: 0.715599, Accuracy: 0.743866\n",
            "[379/ 378] loss: 0.715482, Accuracy: 0.743924\n",
            "[380/ 379] loss: 0.715818, Accuracy: 0.743857\n",
            "[381/ 380] loss: 0.715977, Accuracy: 0.743709\n",
            "[382/ 381] loss: 0.715419, Accuracy: 0.743889\n",
            "[383/ 382] loss: 0.715155, Accuracy: 0.743946\n",
            "[384/ 383] loss: 0.715230, Accuracy: 0.743962\n",
            "[385/ 384] loss: 0.715254, Accuracy: 0.743978\n",
            "[386/ 385] loss: 0.715413, Accuracy: 0.743953\n",
            "[387/ 386] loss: 0.714981, Accuracy: 0.744171\n",
            "[388/ 387] loss: 0.715275, Accuracy: 0.744105\n",
            "[389/ 388] loss: 0.715147, Accuracy: 0.744080\n",
            "[390/ 389] loss: 0.714883, Accuracy: 0.744176\n",
            "[391/ 390] loss: 0.714719, Accuracy: 0.744191\n",
            "[392/ 391] loss: 0.714832, Accuracy: 0.744166\n",
            "[393/ 392] loss: 0.714654, Accuracy: 0.744181\n",
            "[394/ 393] loss: 0.714276, Accuracy: 0.744354\n",
            "[395/ 394] loss: 0.713735, Accuracy: 0.744567\n",
            "[396/ 395] loss: 0.713786, Accuracy: 0.744502\n",
            "[397/ 396] loss: 0.713843, Accuracy: 0.744515\n",
            "[398/ 397] loss: 0.713586, Accuracy: 0.744569\n",
            "[399/ 398] loss: 0.714629, Accuracy: 0.744229\n",
            "[400/ 399] loss: 0.713818, Accuracy: 0.744518\n",
            "[401/ 400] loss: 0.713414, Accuracy: 0.744687\n",
            "[402/ 401] loss: 0.713430, Accuracy: 0.744662\n",
            "[403/ 402] loss: 0.713687, Accuracy: 0.744675\n",
            "[404/ 403] loss: 0.713641, Accuracy: 0.744650\n",
            "[405/ 404] loss: 0.713481, Accuracy: 0.744624\n",
            "[406/ 405] loss: 0.712925, Accuracy: 0.744869\n",
            "[407/ 406] loss: 0.712961, Accuracy: 0.744843\n",
            "[408/ 407] loss: 0.712547, Accuracy: 0.744971\n",
            "[409/ 408] loss: 0.713013, Accuracy: 0.744868\n",
            "[410/ 409] loss: 0.712904, Accuracy: 0.744919\n",
            "[411/ 410] loss: 0.712861, Accuracy: 0.744817\n",
            "[412/ 411] loss: 0.712773, Accuracy: 0.744830\n",
            "[413/ 412] loss: 0.712455, Accuracy: 0.744956\n",
            "[414/ 413] loss: 0.713182, Accuracy: 0.744666\n",
            "[415/ 414] loss: 0.713569, Accuracy: 0.744527\n",
            "[416/ 415] loss: 0.713199, Accuracy: 0.744691\n",
            "[417/ 416] loss: 0.713714, Accuracy: 0.744479\n",
            "[418/ 417] loss: 0.714190, Accuracy: 0.744230\n",
            "[419/ 418] loss: 0.714908, Accuracy: 0.744019\n",
            "[420/ 419] loss: 0.715927, Accuracy: 0.743661\n",
            "[421/ 420] loss: 0.715584, Accuracy: 0.743750\n",
            "[422/ 421] loss: 0.714998, Accuracy: 0.743950\n",
            "[423/ 422] loss: 0.714505, Accuracy: 0.744187\n",
            "[424/ 423] loss: 0.714506, Accuracy: 0.744127\n",
            "[425/ 424] loss: 0.714405, Accuracy: 0.744177\n",
            "[426/ 425] loss: 0.714114, Accuracy: 0.744265\n",
            "[427/ 426] loss: 0.713964, Accuracy: 0.744278\n",
            "[428/ 427] loss: 0.713503, Accuracy: 0.744438\n",
            "[429/ 428] loss: 0.713505, Accuracy: 0.744451\n",
            "[430/ 429] loss: 0.712943, Accuracy: 0.744682\n",
            "[431/ 430] loss: 0.712653, Accuracy: 0.744767\n",
            "[432/ 431] loss: 0.712616, Accuracy: 0.744743\n",
            "[433/ 432] loss: 0.713022, Accuracy: 0.744611\n",
            "[434/ 433] loss: 0.713012, Accuracy: 0.744623\n",
            "[435/ 434] loss: 0.713134, Accuracy: 0.744600\n",
            "[436/ 435] loss: 0.713629, Accuracy: 0.744432\n",
            "[437/ 436] loss: 0.713561, Accuracy: 0.744589\n",
            "[438/ 437] loss: 0.713718, Accuracy: 0.744458\n",
            "[439/ 438] loss: 0.713479, Accuracy: 0.744542\n",
            "[440/ 439] loss: 0.713446, Accuracy: 0.744448\n",
            "[441/ 440] loss: 0.713621, Accuracy: 0.744425\n",
            "[442/ 441] loss: 0.713445, Accuracy: 0.744544\n",
            "[443/ 442] loss: 0.713483, Accuracy: 0.744662\n",
            "[444/ 443] loss: 0.713459, Accuracy: 0.744745\n",
            "[445/ 444] loss: 0.713442, Accuracy: 0.744686\n",
            "[446/ 445] loss: 0.713374, Accuracy: 0.744593\n",
            "[447/ 446] loss: 0.713814, Accuracy: 0.744605\n",
            "[448/ 447] loss: 0.713459, Accuracy: 0.744757\n",
            "[449/ 448] loss: 0.713336, Accuracy: 0.744838\n",
            "[450/ 449] loss: 0.713177, Accuracy: 0.744884\n",
            "[451/ 450] loss: 0.713377, Accuracy: 0.744896\n",
            "[452/ 451] loss: 0.713532, Accuracy: 0.744769\n",
            "[453/ 452] loss: 0.713802, Accuracy: 0.744711\n",
            "[454/ 453] loss: 0.714065, Accuracy: 0.744585\n",
            "[455/ 454] loss: 0.713987, Accuracy: 0.744665\n",
            "[456/ 455] loss: 0.714785, Accuracy: 0.744437\n",
            "[457/ 456] loss: 0.714822, Accuracy: 0.744380\n",
            "[458/ 457] loss: 0.715060, Accuracy: 0.744359\n",
            "[459/ 458] loss: 0.715168, Accuracy: 0.744303\n",
            "[460/ 459] loss: 0.715081, Accuracy: 0.744349\n",
            "[461/ 460] loss: 0.714642, Accuracy: 0.744463\n",
            "[462/ 461] loss: 0.714418, Accuracy: 0.744577\n",
            "[463/ 462] loss: 0.714290, Accuracy: 0.744623\n",
            "[464/ 463] loss: 0.714449, Accuracy: 0.744567\n",
            "[465/ 464] loss: 0.714955, Accuracy: 0.744376\n",
            "[466/ 465] loss: 0.714811, Accuracy: 0.744388\n",
            "[467/ 466] loss: 0.714826, Accuracy: 0.744434\n",
            "[468/ 467] loss: 0.714830, Accuracy: 0.744446\n",
            "[469/ 468] loss: 0.715198, Accuracy: 0.744291\n",
            "[470/ 469] loss: 0.715465, Accuracy: 0.744203\n",
            "[471/ 470] loss: 0.715283, Accuracy: 0.744282\n",
            "[472/ 471] loss: 0.715591, Accuracy: 0.744161\n",
            "[473/ 472] loss: 0.715259, Accuracy: 0.744306\n",
            "[474/ 473] loss: 0.715351, Accuracy: 0.744186\n",
            "[475/ 474] loss: 0.715092, Accuracy: 0.744264\n",
            "[476/ 475] loss: 0.715384, Accuracy: 0.744145\n",
            "[477/ 476] loss: 0.715168, Accuracy: 0.744223\n",
            "[478/ 477] loss: 0.715021, Accuracy: 0.744268\n",
            "[479/ 478] loss: 0.715307, Accuracy: 0.744182\n",
            "[480/ 479] loss: 0.714915, Accuracy: 0.744324\n",
            "[481/ 480] loss: 0.714670, Accuracy: 0.744401\n",
            "[482/ 481] loss: 0.714564, Accuracy: 0.744380\n",
            "[483/ 482] loss: 0.714281, Accuracy: 0.744489\n",
            "[484/ 483] loss: 0.714632, Accuracy: 0.744436\n",
            "[485/ 484] loss: 0.715137, Accuracy: 0.744286\n",
            "[486/ 485] loss: 0.715344, Accuracy: 0.744201\n",
            "[487/ 486] loss: 0.714566, Accuracy: 0.744470\n",
            "[488/ 487] loss: 0.714492, Accuracy: 0.744514\n",
            "[489/ 488] loss: 0.714498, Accuracy: 0.744493\n",
            "[490/ 489] loss: 0.714820, Accuracy: 0.744376\n",
            "[491/ 490] loss: 0.714928, Accuracy: 0.744260\n",
            "[492/ 491] loss: 0.715014, Accuracy: 0.744240\n",
            "[493/ 492] loss: 0.715521, Accuracy: 0.744125\n",
            "[494/ 493] loss: 0.715937, Accuracy: 0.743946\n",
            "[495/ 494] loss: 0.716064, Accuracy: 0.743895\n",
            "[496/ 495] loss: 0.716273, Accuracy: 0.743782\n",
            "[497/ 496] loss: 0.715884, Accuracy: 0.743983\n",
            "[498/ 497] loss: 0.715785, Accuracy: 0.744027\n",
            "[499/ 498] loss: 0.715634, Accuracy: 0.744164\n",
            "[500/ 499] loss: 0.715498, Accuracy: 0.744207\n",
            "[501/ 500] loss: 0.715964, Accuracy: 0.744000\n",
            "[502/ 501] loss: 0.715830, Accuracy: 0.744043\n",
            "[503/ 502] loss: 0.715732, Accuracy: 0.744148\n",
            "[504/ 503] loss: 0.715876, Accuracy: 0.744098\n",
            "[505/ 504] loss: 0.715768, Accuracy: 0.744203\n",
            "[506/ 505] loss: 0.715819, Accuracy: 0.744152\n",
            "[507/ 506] loss: 0.715956, Accuracy: 0.744195\n",
            "[508/ 507] loss: 0.715790, Accuracy: 0.744268\n",
            "[509/ 508] loss: 0.715291, Accuracy: 0.744464\n",
            "[510/ 509] loss: 0.715088, Accuracy: 0.744567\n",
            "[511/ 510] loss: 0.715206, Accuracy: 0.744516\n",
            "[512/ 511] loss: 0.715457, Accuracy: 0.744496\n",
            "[513/ 512] loss: 0.715700, Accuracy: 0.744415\n",
            "[514/ 513] loss: 0.715874, Accuracy: 0.744396\n",
            "[515/ 514] loss: 0.715764, Accuracy: 0.744376\n",
            "[516/ 515] loss: 0.715376, Accuracy: 0.744539\n",
            "[517/ 516] loss: 0.715293, Accuracy: 0.744519\n",
            "[518/ 517] loss: 0.715536, Accuracy: 0.744409\n",
            "[519/ 518] loss: 0.715677, Accuracy: 0.744329\n",
            "[520/ 519] loss: 0.715092, Accuracy: 0.744581\n",
            "[521/ 520] loss: 0.715343, Accuracy: 0.744501\n",
            "[522/ 521] loss: 0.715425, Accuracy: 0.744482\n",
            "[523/ 522] loss: 0.714937, Accuracy: 0.744672\n",
            "[524/ 523] loss: 0.714802, Accuracy: 0.744772\n",
            "[525/ 524] loss: 0.715070, Accuracy: 0.744752\n",
            "[526/ 525] loss: 0.715134, Accuracy: 0.744702\n",
            "[527/ 526] loss: 0.715273, Accuracy: 0.744623\n",
            "[528/ 527] loss: 0.715286, Accuracy: 0.744634\n",
            "[529/ 528] loss: 0.715721, Accuracy: 0.744466\n",
            "[530/ 529] loss: 0.716122, Accuracy: 0.744358\n",
            "[531/ 530] loss: 0.715947, Accuracy: 0.744428\n",
            "[532/ 531] loss: 0.716020, Accuracy: 0.744439\n",
            "[533/ 532] loss: 0.716515, Accuracy: 0.744243\n",
            "[534/ 533] loss: 0.716096, Accuracy: 0.744401\n",
            "[535/ 534] loss: 0.716161, Accuracy: 0.744411\n",
            "[536/ 535] loss: 0.716362, Accuracy: 0.744334\n",
            "[537/ 536] loss: 0.716345, Accuracy: 0.744345\n",
            "[538/ 537] loss: 0.716530, Accuracy: 0.744268\n",
            "[539/ 538] loss: 0.716580, Accuracy: 0.744279\n",
            "[540/ 539] loss: 0.716259, Accuracy: 0.744434\n",
            "[541/ 540] loss: 0.716358, Accuracy: 0.744416\n",
            "[542/ 541] loss: 0.716109, Accuracy: 0.744455\n",
            "[543/ 542] loss: 0.715547, Accuracy: 0.744609\n",
            "[544/ 543] loss: 0.715376, Accuracy: 0.744734\n",
            "[545/ 544] loss: 0.714974, Accuracy: 0.744830\n",
            "[546/ 545] loss: 0.715175, Accuracy: 0.744811\n",
            "[547/ 546] loss: 0.715196, Accuracy: 0.744734\n",
            "[548/ 547] loss: 0.715206, Accuracy: 0.744773\n",
            "[549/ 548] loss: 0.715525, Accuracy: 0.744668\n",
            "[550/ 549] loss: 0.715349, Accuracy: 0.744792\n",
            "[551/ 550] loss: 0.715255, Accuracy: 0.744801\n",
            "[552/ 551] loss: 0.715112, Accuracy: 0.744867\n",
            "[553/ 552] loss: 0.714739, Accuracy: 0.745103\n",
            "[554/ 553] loss: 0.715427, Accuracy: 0.744914\n",
            "[555/ 554] loss: 0.715346, Accuracy: 0.744951\n",
            "[556/ 555] loss: 0.715494, Accuracy: 0.744961\n",
            "[557/ 556] loss: 0.715582, Accuracy: 0.744998\n",
            "[558/ 557] loss: 0.715565, Accuracy: 0.745007\n",
            "[559/ 558] loss: 0.715933, Accuracy: 0.744848\n",
            "[560/ 559] loss: 0.716013, Accuracy: 0.744801\n",
            "[561/ 560] loss: 0.716038, Accuracy: 0.744810\n",
            "[562/ 561] loss: 0.715880, Accuracy: 0.744931\n",
            "[563/ 562] loss: 0.715960, Accuracy: 0.744912\n",
            "[564/ 563] loss: 0.715952, Accuracy: 0.744949\n",
            "[565/ 564] loss: 0.715968, Accuracy: 0.744902\n",
            "[566/ 565] loss: 0.715706, Accuracy: 0.744967\n",
            "[567/ 566] loss: 0.715214, Accuracy: 0.745114\n",
            "[568/ 567] loss: 0.715378, Accuracy: 0.745040\n",
            "[569/ 568] loss: 0.715282, Accuracy: 0.745021\n",
            "[570/ 569] loss: 0.715573, Accuracy: 0.744947\n",
            "[571/ 570] loss: 0.715879, Accuracy: 0.744901\n",
            "[572/ 571] loss: 0.715791, Accuracy: 0.744938\n",
            "[573/ 572] loss: 0.715948, Accuracy: 0.744892\n",
            "[574/ 573] loss: 0.716224, Accuracy: 0.744819\n",
            "[575/ 574] loss: 0.715842, Accuracy: 0.744964\n",
            "[576/ 575] loss: 0.716180, Accuracy: 0.744837\n",
            "[577/ 576] loss: 0.716356, Accuracy: 0.744819\n",
            "[578/ 577] loss: 0.716507, Accuracy: 0.744855\n",
            "[579/ 578] loss: 0.716349, Accuracy: 0.744891\n",
            "[580/ 579] loss: 0.716294, Accuracy: 0.744873\n",
            "[581/ 580] loss: 0.716251, Accuracy: 0.744908\n",
            "[582/ 581] loss: 0.716376, Accuracy: 0.744837\n",
            "[583/ 582] loss: 0.716341, Accuracy: 0.744765\n",
            "[584/ 583] loss: 0.716590, Accuracy: 0.744693\n",
            "[585/ 584] loss: 0.716534, Accuracy: 0.744756\n",
            "[586/ 585] loss: 0.716588, Accuracy: 0.744738\n",
            "[587/ 586] loss: 0.716771, Accuracy: 0.744641\n",
            "[588/ 587] loss: 0.716521, Accuracy: 0.744730\n",
            "[589/ 588] loss: 0.716255, Accuracy: 0.744818\n",
            "[590/ 589] loss: 0.715910, Accuracy: 0.744986\n",
            "[591/ 590] loss: 0.716132, Accuracy: 0.744889\n",
            "[592/ 591] loss: 0.715991, Accuracy: 0.744897\n",
            "[593/ 592] loss: 0.716418, Accuracy: 0.744695\n",
            "[594/ 593] loss: 0.716272, Accuracy: 0.744757\n",
            "[595/ 594] loss: 0.716587, Accuracy: 0.744608\n",
            "[596/ 595] loss: 0.716524, Accuracy: 0.744617\n",
            "[597/ 596] loss: 0.716887, Accuracy: 0.744495\n",
            "[598/ 597] loss: 0.716835, Accuracy: 0.744530\n",
            "[599/ 598] loss: 0.716962, Accuracy: 0.744461\n",
            "[600/ 599] loss: 0.716775, Accuracy: 0.744626\n",
            "[601 / 600]  Loss:0.716526, Accuracy:0.744661\n",
            "[601/ 600] loss: 0.716526, Accuracy: 0.744661\n",
            "[602/ 601] loss: 0.716735, Accuracy: 0.744566\n",
            "[603/ 602] loss: 0.716531, Accuracy: 0.744653\n",
            "[604/ 603] loss: 0.716288, Accuracy: 0.744740\n",
            "[605/ 604] loss: 0.716121, Accuracy: 0.744800\n",
            "[606/ 605] loss: 0.715692, Accuracy: 0.744964\n",
            "[607/ 606] loss: 0.715566, Accuracy: 0.745024\n",
            "[608/ 607] loss: 0.715658, Accuracy: 0.744929\n",
            "[609/ 608] loss: 0.715191, Accuracy: 0.745091\n",
            "[610/ 609] loss: 0.715129, Accuracy: 0.745100\n",
            "[611/ 610] loss: 0.714846, Accuracy: 0.745184\n",
            "[612/ 611] loss: 0.714508, Accuracy: 0.745320\n",
            "[613/ 612] loss: 0.714827, Accuracy: 0.745251\n",
            "[614/ 613] loss: 0.714999, Accuracy: 0.745233\n",
            "[615/ 614] loss: 0.714726, Accuracy: 0.745292\n",
            "[616/ 615] loss: 0.715140, Accuracy: 0.745122\n",
            "[617/ 616] loss: 0.714990, Accuracy: 0.745155\n",
            "[618/ 617] loss: 0.714946, Accuracy: 0.745188\n",
            "[619/ 618] loss: 0.714610, Accuracy: 0.745272\n",
            "[620/ 619] loss: 0.714686, Accuracy: 0.745280\n",
            "[621/ 620] loss: 0.714319, Accuracy: 0.745413\n",
            "[622/ 621] loss: 0.714093, Accuracy: 0.745471\n",
            "[623/ 622] loss: 0.713843, Accuracy: 0.745554\n",
            "[624/ 623] loss: 0.713568, Accuracy: 0.745636\n",
            "[625/ 624] loss: 0.713232, Accuracy: 0.745768\n",
            "[626/ 625] loss: 0.713093, Accuracy: 0.745825\n",
            "[627/ 626] loss: 0.713248, Accuracy: 0.745707\n",
            "[628/ 627] loss: 0.713240, Accuracy: 0.745764\n",
            "[629/ 628] loss: 0.713138, Accuracy: 0.745770\n",
            "[630/ 629] loss: 0.713158, Accuracy: 0.745727\n",
            "[631/ 630] loss: 0.713252, Accuracy: 0.745660\n",
            "[632/ 631] loss: 0.712982, Accuracy: 0.745741\n",
            "[633/ 632] loss: 0.712968, Accuracy: 0.745698\n",
            "[634/ 633] loss: 0.712759, Accuracy: 0.745779\n",
            "[635/ 634] loss: 0.712657, Accuracy: 0.745761\n",
            "[636/ 635] loss: 0.712660, Accuracy: 0.745743\n",
            "[637/ 636] loss: 0.712854, Accuracy: 0.745701\n",
            "[638/ 637] loss: 0.713373, Accuracy: 0.745462\n",
            "[639/ 638] loss: 0.713303, Accuracy: 0.745469\n",
            "[640/ 639] loss: 0.713681, Accuracy: 0.745403\n",
            "[641/ 640] loss: 0.713518, Accuracy: 0.745483\n",
            "[642/ 641] loss: 0.713695, Accuracy: 0.745442\n",
            "[643/ 642] loss: 0.713495, Accuracy: 0.745497\n",
            "[644/ 643] loss: 0.713549, Accuracy: 0.745456\n",
            "[645/ 644] loss: 0.713597, Accuracy: 0.745487\n",
            "[646/ 645] loss: 0.713658, Accuracy: 0.745422\n",
            "[647/ 646] loss: 0.713443, Accuracy: 0.745525\n",
            "[648/ 647] loss: 0.713609, Accuracy: 0.745484\n",
            "[649/ 648] loss: 0.713287, Accuracy: 0.745587\n",
            "[650/ 649] loss: 0.713103, Accuracy: 0.745666\n",
            "[651/ 650] loss: 0.713208, Accuracy: 0.745601\n",
            "[652/ 651] loss: 0.713382, Accuracy: 0.745584\n",
            "[653/ 652] loss: 0.713261, Accuracy: 0.745614\n",
            "[654/ 653] loss: 0.713250, Accuracy: 0.745645\n",
            "[655/ 654] loss: 0.713427, Accuracy: 0.745580\n",
            "[656/ 655] loss: 0.713407, Accuracy: 0.745491\n",
            "[657/ 656] loss: 0.713685, Accuracy: 0.745474\n",
            "[658/ 657] loss: 0.713799, Accuracy: 0.745386\n",
            "[659/ 658] loss: 0.713741, Accuracy: 0.745417\n",
            "[660/ 659] loss: 0.713881, Accuracy: 0.745377\n",
            "[661/ 660] loss: 0.714266, Accuracy: 0.745265\n",
            "[662/ 661] loss: 0.714667, Accuracy: 0.745107\n",
            "[663/ 662] loss: 0.715023, Accuracy: 0.744973\n",
            "[664/ 663] loss: 0.715086, Accuracy: 0.744910\n",
            "[665/ 664] loss: 0.715338, Accuracy: 0.744870\n",
            "[666/ 665] loss: 0.715400, Accuracy: 0.744854\n",
            "[667/ 666] loss: 0.715492, Accuracy: 0.744862\n",
            "[668/ 667] loss: 0.715039, Accuracy: 0.745034\n",
            "[669/ 668] loss: 0.714828, Accuracy: 0.745158\n",
            "[670/ 669] loss: 0.714953, Accuracy: 0.745095\n",
            "[671/ 670] loss: 0.714990, Accuracy: 0.745056\n",
            "[672/ 671] loss: 0.715036, Accuracy: 0.745040\n",
            "[673/ 672] loss: 0.715196, Accuracy: 0.745001\n",
            "[674/ 673] loss: 0.715120, Accuracy: 0.745055\n",
            "[675/ 674] loss: 0.715246, Accuracy: 0.744969\n",
            "[676/ 675] loss: 0.715343, Accuracy: 0.744954\n",
            "[677/ 676] loss: 0.715343, Accuracy: 0.745007\n",
            "[678/ 677] loss: 0.715081, Accuracy: 0.745061\n",
            "[679/ 678] loss: 0.715130, Accuracy: 0.745045\n",
            "[680/ 679] loss: 0.714945, Accuracy: 0.745052\n",
            "[681/ 680] loss: 0.715000, Accuracy: 0.745014\n",
            "[682/ 681] loss: 0.715194, Accuracy: 0.744975\n",
            "[683/ 682] loss: 0.715012, Accuracy: 0.745005\n",
            "[684/ 683] loss: 0.715000, Accuracy: 0.745013\n",
            "[685/ 684] loss: 0.714890, Accuracy: 0.745066\n",
            "[686/ 685] loss: 0.714837, Accuracy: 0.745073\n",
            "[687/ 686] loss: 0.714768, Accuracy: 0.745057\n",
            "[688/ 687] loss: 0.714565, Accuracy: 0.745110\n",
            "[689/ 688] loss: 0.714235, Accuracy: 0.745231\n",
            "[690/ 689] loss: 0.714005, Accuracy: 0.745328\n",
            "[691/ 690] loss: 0.713932, Accuracy: 0.745380\n",
            "[692/ 691] loss: 0.713570, Accuracy: 0.745478\n",
            "[693/ 692] loss: 0.713662, Accuracy: 0.745507\n",
            "[694/ 693] loss: 0.713773, Accuracy: 0.745468\n",
            "[695/ 694] loss: 0.713716, Accuracy: 0.745475\n",
            "[696/ 695] loss: 0.713986, Accuracy: 0.745369\n",
            "[697/ 696] loss: 0.714291, Accuracy: 0.745308\n",
            "[698/ 697] loss: 0.714118, Accuracy: 0.745337\n",
            "[699/ 698] loss: 0.714035, Accuracy: 0.745389\n",
            "[700/ 699] loss: 0.713933, Accuracy: 0.745373\n",
            "[701/ 700] loss: 0.713974, Accuracy: 0.745313\n",
            "[702/ 701] loss: 0.713848, Accuracy: 0.745341\n",
            "[703/ 702] loss: 0.714065, Accuracy: 0.745259\n",
            "[704/ 703] loss: 0.713948, Accuracy: 0.745310\n",
            "[705/ 704] loss: 0.714464, Accuracy: 0.745095\n",
            "[706/ 705] loss: 0.714220, Accuracy: 0.745257\n",
            "[707/ 706] loss: 0.714159, Accuracy: 0.745242\n",
            "[708/ 707] loss: 0.714259, Accuracy: 0.745248\n",
            "[709/ 708] loss: 0.714529, Accuracy: 0.745189\n",
            "[710/ 709] loss: 0.714548, Accuracy: 0.745174\n",
            "[711/ 710] loss: 0.714387, Accuracy: 0.745291\n",
            "[712/ 711] loss: 0.714190, Accuracy: 0.745341\n",
            "[713/ 712] loss: 0.713929, Accuracy: 0.745479\n",
            "[714/ 713] loss: 0.714150, Accuracy: 0.745442\n",
            "[715/ 714] loss: 0.714433, Accuracy: 0.745361\n",
            "[716/ 715] loss: 0.714857, Accuracy: 0.745170\n",
            "[717/ 716] loss: 0.714504, Accuracy: 0.745286\n",
            "[718/ 717] loss: 0.714369, Accuracy: 0.745336\n",
            "[719/ 718] loss: 0.714481, Accuracy: 0.745299\n",
            "[720/ 719] loss: 0.714389, Accuracy: 0.745349\n",
            "[721/ 720] loss: 0.714405, Accuracy: 0.745334\n",
            "[722/ 721] loss: 0.714170, Accuracy: 0.745406\n",
            "[723/ 722] loss: 0.714002, Accuracy: 0.745499\n",
            "[724/ 723] loss: 0.714056, Accuracy: 0.745462\n",
            "[725/ 724] loss: 0.714368, Accuracy: 0.745317\n",
            "[726/ 725] loss: 0.714062, Accuracy: 0.745453\n",
            "[727/ 726] loss: 0.714363, Accuracy: 0.745330\n",
            "[728/ 727] loss: 0.714473, Accuracy: 0.745315\n",
            "[729/ 728] loss: 0.714338, Accuracy: 0.745321\n",
            "[730/ 729] loss: 0.714303, Accuracy: 0.745328\n",
            "[731/ 730] loss: 0.714562, Accuracy: 0.745227\n",
            "[732/ 731] loss: 0.714468, Accuracy: 0.745233\n",
            "[733/ 732] loss: 0.714600, Accuracy: 0.745176\n",
            "[734/ 733] loss: 0.714701, Accuracy: 0.745140\n",
            "[735/ 734] loss: 0.715038, Accuracy: 0.745061\n",
            "[736/ 735] loss: 0.714821, Accuracy: 0.745153\n",
            "[737/ 736] loss: 0.715050, Accuracy: 0.745075\n",
            "[738/ 737] loss: 0.714916, Accuracy: 0.745103\n",
            "[739/ 738] loss: 0.715039, Accuracy: 0.745025\n",
            "[740/ 739] loss: 0.715153, Accuracy: 0.744989\n",
            "[741/ 740] loss: 0.715093, Accuracy: 0.745059\n",
            "[742/ 741] loss: 0.715178, Accuracy: 0.745045\n",
            "[743/ 742] loss: 0.715124, Accuracy: 0.745051\n",
            "[744/ 743] loss: 0.715082, Accuracy: 0.745016\n",
            "[745/ 744] loss: 0.715317, Accuracy: 0.744876\n",
            "[746/ 745] loss: 0.715213, Accuracy: 0.744966\n",
            "[747/ 746] loss: 0.714775, Accuracy: 0.745141\n",
            "[748/ 747] loss: 0.714789, Accuracy: 0.745147\n",
            "[749/ 748] loss: 0.714899, Accuracy: 0.745112\n",
            "[750/ 749] loss: 0.714581, Accuracy: 0.745160\n",
            "[751/ 750] loss: 0.714857, Accuracy: 0.745021\n",
            "[752/ 751] loss: 0.715060, Accuracy: 0.744944\n",
            "[753/ 752] loss: 0.714752, Accuracy: 0.745076\n",
            "[754/ 753] loss: 0.714723, Accuracy: 0.745041\n",
            "[755/ 754] loss: 0.714759, Accuracy: 0.744985\n",
            "[756/ 755] loss: 0.714835, Accuracy: 0.744888\n",
            "[757/ 756] loss: 0.715016, Accuracy: 0.744812\n",
            "[758/ 757] loss: 0.714905, Accuracy: 0.744881\n",
            "[759/ 758] loss: 0.714833, Accuracy: 0.744929\n",
            "[760/ 759] loss: 0.714772, Accuracy: 0.744956\n",
            "[761/ 760] loss: 0.714768, Accuracy: 0.745025\n",
            "[762/ 761] loss: 0.714994, Accuracy: 0.744929\n",
            "[763/ 762] loss: 0.714730, Accuracy: 0.745017\n",
            "[764/ 763] loss: 0.714640, Accuracy: 0.745065\n",
            "[765/ 764] loss: 0.714783, Accuracy: 0.745010\n",
            "[766/ 765] loss: 0.715016, Accuracy: 0.744935\n",
            "[767/ 766] loss: 0.714937, Accuracy: 0.744962\n",
            "[768/ 767] loss: 0.714707, Accuracy: 0.745070\n",
            "[769/ 768] loss: 0.714557, Accuracy: 0.745117\n",
            "[770/ 769] loss: 0.714345, Accuracy: 0.745164\n",
            "[771/ 770] loss: 0.714517, Accuracy: 0.745089\n",
            "[772/ 771] loss: 0.714430, Accuracy: 0.745156\n",
            "[773/ 772] loss: 0.714432, Accuracy: 0.745163\n",
            "[774/ 773] loss: 0.714389, Accuracy: 0.745209\n",
            "[775/ 774] loss: 0.714155, Accuracy: 0.745296\n",
            "[776/ 775] loss: 0.714037, Accuracy: 0.745323\n",
            "[777/ 776] loss: 0.714288, Accuracy: 0.745228\n",
            "[778/ 777] loss: 0.714325, Accuracy: 0.745214\n",
            "[779/ 778] loss: 0.714121, Accuracy: 0.745280\n",
            "[780/ 779] loss: 0.714025, Accuracy: 0.745306\n",
            "[781/ 780] loss: 0.714321, Accuracy: 0.745292\n",
            "[782/ 781] loss: 0.714262, Accuracy: 0.745338\n",
            "[783/ 782] loss: 0.713986, Accuracy: 0.745424\n",
            "[784/ 783] loss: 0.714042, Accuracy: 0.745410\n",
            "[785/ 784] loss: 0.713992, Accuracy: 0.745396\n",
            "[786/ 785] loss: 0.713989, Accuracy: 0.745382\n",
            "[787/ 786] loss: 0.714061, Accuracy: 0.745328\n",
            "[788/ 787] loss: 0.714224, Accuracy: 0.745175\n",
            "[789/ 788] loss: 0.714012, Accuracy: 0.745241\n",
            "[790/ 789] loss: 0.713793, Accuracy: 0.745346\n",
            "[791/ 790] loss: 0.713446, Accuracy: 0.745451\n",
            "[792/ 791] loss: 0.713507, Accuracy: 0.745437\n",
            "[793/ 792] loss: 0.713365, Accuracy: 0.745502\n",
            "[794/ 793] loss: 0.713686, Accuracy: 0.745389\n",
            "[795/ 794] loss: 0.713999, Accuracy: 0.745277\n",
            "[796/ 795] loss: 0.713779, Accuracy: 0.745381\n",
            "[797/ 796] loss: 0.713767, Accuracy: 0.745367\n",
            "[798/ 797] loss: 0.713503, Accuracy: 0.745412\n",
            "[799/ 798] loss: 0.713469, Accuracy: 0.745399\n",
            "[800/ 799] loss: 0.713350, Accuracy: 0.745424\n",
            "[801/ 800] loss: 0.713366, Accuracy: 0.745449\n",
            "[802/ 801] loss: 0.713692, Accuracy: 0.745377\n",
            "[803/ 802] loss: 0.713661, Accuracy: 0.745422\n",
            "[804/ 803] loss: 0.713836, Accuracy: 0.745369\n",
            "[805/ 804] loss: 0.713905, Accuracy: 0.745375\n",
            "[806/ 805] loss: 0.713826, Accuracy: 0.745400\n",
            "[807/ 806] loss: 0.714193, Accuracy: 0.745289\n",
            "[808/ 807] loss: 0.714099, Accuracy: 0.745295\n",
            "[809/ 808] loss: 0.714424, Accuracy: 0.745185\n",
            "[810/ 809] loss: 0.714155, Accuracy: 0.745307\n",
            "[811/ 810] loss: 0.713915, Accuracy: 0.745370\n",
            "[812/ 811] loss: 0.714294, Accuracy: 0.745203\n",
            "[813/ 812] loss: 0.714137, Accuracy: 0.745266\n",
            "[814/ 813] loss: 0.713946, Accuracy: 0.745368\n",
            "[815/ 814] loss: 0.713778, Accuracy: 0.745451\n",
            "[816/ 815] loss: 0.713742, Accuracy: 0.745456\n",
            "[817/ 816] loss: 0.713847, Accuracy: 0.745424\n",
            "[818/ 817] loss: 0.713944, Accuracy: 0.745410\n",
            "[819/ 818] loss: 0.714030, Accuracy: 0.745397\n",
            "[820/ 819] loss: 0.713849, Accuracy: 0.745459\n",
            "[821/ 820] loss: 0.713913, Accuracy: 0.745389\n",
            "[822/ 821] loss: 0.713861, Accuracy: 0.745394\n",
            "[823/ 822] loss: 0.714018, Accuracy: 0.745324\n",
            "[824/ 823] loss: 0.714135, Accuracy: 0.745273\n",
            "[825/ 824] loss: 0.714226, Accuracy: 0.745221\n",
            "[826/ 825] loss: 0.714188, Accuracy: 0.745284\n",
            "[827/ 826] loss: 0.714440, Accuracy: 0.745176\n",
            "[828/ 827] loss: 0.714253, Accuracy: 0.745239\n",
            "[829/ 828] loss: 0.714239, Accuracy: 0.745282\n",
            "[830/ 829] loss: 0.713973, Accuracy: 0.745382\n",
            "[831/ 830] loss: 0.713914, Accuracy: 0.745407\n",
            "[832/ 831] loss: 0.714414, Accuracy: 0.745224\n",
            "[833/ 832] loss: 0.714360, Accuracy: 0.745211\n",
            "[834/ 833] loss: 0.714520, Accuracy: 0.745142\n",
            "[835/ 834] loss: 0.714576, Accuracy: 0.745129\n",
            "[836/ 835] loss: 0.714714, Accuracy: 0.745079\n",
            "[837/ 836] loss: 0.714915, Accuracy: 0.744991\n",
            "[838/ 837] loss: 0.714923, Accuracy: 0.744978\n",
            "[839/ 838] loss: 0.714692, Accuracy: 0.745096\n",
            "[840/ 839] loss: 0.714722, Accuracy: 0.745065\n",
            "[841/ 840] loss: 0.714662, Accuracy: 0.745071\n",
            "[842/ 841] loss: 0.714645, Accuracy: 0.745039\n",
            "[843/ 842] loss: 0.714582, Accuracy: 0.745082\n",
            "[844/ 843] loss: 0.714480, Accuracy: 0.745107\n",
            "[845/ 844] loss: 0.714530, Accuracy: 0.745113\n",
            "[846/ 845] loss: 0.714547, Accuracy: 0.745100\n",
            "[847/ 846] loss: 0.714430, Accuracy: 0.745106\n",
            "[848/ 847] loss: 0.714475, Accuracy: 0.745075\n",
            "[849/ 848] loss: 0.714577, Accuracy: 0.745025\n",
            "[850/ 849] loss: 0.714408, Accuracy: 0.745068\n",
            "[851/ 850] loss: 0.714339, Accuracy: 0.745110\n",
            "[852/ 851] loss: 0.714371, Accuracy: 0.745134\n",
            "[853/ 852] loss: 0.714316, Accuracy: 0.745158\n",
            "[854/ 853] loss: 0.714518, Accuracy: 0.745054\n",
            "[855/ 854] loss: 0.714382, Accuracy: 0.745097\n",
            "[856/ 855] loss: 0.714053, Accuracy: 0.745212\n",
            "[857/ 856] loss: 0.714059, Accuracy: 0.745181\n",
            "[858/ 857] loss: 0.714233, Accuracy: 0.745096\n",
            "[859/ 858] loss: 0.714491, Accuracy: 0.744992\n",
            "[860/ 859] loss: 0.714537, Accuracy: 0.744961\n",
            "[861/ 860] loss: 0.714822, Accuracy: 0.744876\n",
            "[862/ 861] loss: 0.714727, Accuracy: 0.744937\n",
            "[863/ 862] loss: 0.714770, Accuracy: 0.744925\n",
            "[864/ 863] loss: 0.714764, Accuracy: 0.744912\n",
            "[865/ 864] loss: 0.714690, Accuracy: 0.744936\n",
            "[866/ 865] loss: 0.714695, Accuracy: 0.744888\n",
            "[867/ 866] loss: 0.714656, Accuracy: 0.744894\n",
            "[868/ 867] loss: 0.714866, Accuracy: 0.744810\n",
            "[869/ 868] loss: 0.714730, Accuracy: 0.744852\n",
            "[870/ 869] loss: 0.714724, Accuracy: 0.744912\n",
            "[871/ 870] loss: 0.714856, Accuracy: 0.744846\n",
            "[872/ 871] loss: 0.714772, Accuracy: 0.744905\n",
            "[873/ 872] loss: 0.714904, Accuracy: 0.744875\n",
            "[874/ 873] loss: 0.714745, Accuracy: 0.744935\n",
            "[875/ 874] loss: 0.714869, Accuracy: 0.744869\n",
            "[876/ 875] loss: 0.714878, Accuracy: 0.744857\n",
            "[877/ 876] loss: 0.714743, Accuracy: 0.744916\n",
            "[878/ 877] loss: 0.714834, Accuracy: 0.744851\n",
            "[879/ 878] loss: 0.715183, Accuracy: 0.744732\n",
            "[880/ 879] loss: 0.715057, Accuracy: 0.744774\n",
            "[881/ 880] loss: 0.715097, Accuracy: 0.744780\n",
            "[882/ 881] loss: 0.715137, Accuracy: 0.744803\n",
            "[883/ 882] loss: 0.715103, Accuracy: 0.744792\n",
            "[884/ 883] loss: 0.714817, Accuracy: 0.744904\n",
            "[885/ 884] loss: 0.714732, Accuracy: 0.744910\n",
            "[886/ 885] loss: 0.714790, Accuracy: 0.744880\n",
            "[887/ 886] loss: 0.714993, Accuracy: 0.744833\n",
            "[888/ 887] loss: 0.715014, Accuracy: 0.744874\n",
            "[889/ 888] loss: 0.715082, Accuracy: 0.744862\n",
            "[890/ 889] loss: 0.714917, Accuracy: 0.744921\n",
            "[891/ 890] loss: 0.715235, Accuracy: 0.744856\n",
            "[892/ 891] loss: 0.715105, Accuracy: 0.744897\n",
            "[893/ 892] loss: 0.715123, Accuracy: 0.744903\n",
            "[894/ 893] loss: 0.715001, Accuracy: 0.744961\n",
            "[895/ 894] loss: 0.715289, Accuracy: 0.744914\n",
            "[896/ 895] loss: 0.715053, Accuracy: 0.744990\n",
            "[897/ 896] loss: 0.715133, Accuracy: 0.744943\n",
            "[898/ 897] loss: 0.715086, Accuracy: 0.744931\n",
            "[899/ 898] loss: 0.715177, Accuracy: 0.744919\n",
            "[900/ 899] loss: 0.715074, Accuracy: 0.744960\n",
            "[901 / 900]  Loss:0.715124, Accuracy:0.744965\n",
            "[901/ 900] loss: 0.715124, Accuracy: 0.744965\n",
            "[902/ 901] loss: 0.715078, Accuracy: 0.744988\n",
            "[903/ 902] loss: 0.715307, Accuracy: 0.744924\n",
            "[904/ 903] loss: 0.715688, Accuracy: 0.744757\n",
            "[905/ 904] loss: 0.715798, Accuracy: 0.744711\n",
            "[906/ 905] loss: 0.715832, Accuracy: 0.744717\n",
            "[907/ 906] loss: 0.716056, Accuracy: 0.744619\n",
            "[908/ 907] loss: 0.716092, Accuracy: 0.744591\n",
            "[909/ 908] loss: 0.716058, Accuracy: 0.744631\n",
            "[910/ 909] loss: 0.716122, Accuracy: 0.744637\n",
            "[911/ 910] loss: 0.716041, Accuracy: 0.744643\n",
            "[912/ 911] loss: 0.716143, Accuracy: 0.744614\n",
            "[913/ 912] loss: 0.716270, Accuracy: 0.744655\n",
            "[914/ 913] loss: 0.716145, Accuracy: 0.744729\n",
            "[915/ 914] loss: 0.716206, Accuracy: 0.744718\n",
            "[916/ 915] loss: 0.716138, Accuracy: 0.744740\n",
            "[917/ 916] loss: 0.716042, Accuracy: 0.744780\n",
            "[918/ 917] loss: 0.715856, Accuracy: 0.744905\n",
            "[919/ 918] loss: 0.715756, Accuracy: 0.744928\n",
            "[920/ 919] loss: 0.715840, Accuracy: 0.744916\n",
            "[921/ 920] loss: 0.715533, Accuracy: 0.745041\n",
            "[922/ 921] loss: 0.715650, Accuracy: 0.744995\n",
            "[923/ 922] loss: 0.715569, Accuracy: 0.745035\n",
            "[924/ 923] loss: 0.715819, Accuracy: 0.744989\n",
            "[925/ 924] loss: 0.715655, Accuracy: 0.745045\n",
            "[926/ 925] loss: 0.715489, Accuracy: 0.745084\n",
            "[927/ 926] loss: 0.715300, Accuracy: 0.745124\n",
            "[928/ 927] loss: 0.715032, Accuracy: 0.745230\n",
            "[929/ 928] loss: 0.714986, Accuracy: 0.745235\n",
            "[930/ 929] loss: 0.714899, Accuracy: 0.745274\n",
            "[931/ 930] loss: 0.714987, Accuracy: 0.745245\n",
            "[932/ 931] loss: 0.715013, Accuracy: 0.745267\n",
            "[933/ 932] loss: 0.714900, Accuracy: 0.745255\n",
            "[934/ 933] loss: 0.715084, Accuracy: 0.745194\n",
            "[935/ 934] loss: 0.715133, Accuracy: 0.745215\n",
            "[936/ 935] loss: 0.715081, Accuracy: 0.745221\n",
            "[937/ 936] loss: 0.714935, Accuracy: 0.745242\n",
            "[938/ 937] loss: 0.715114, Accuracy: 0.745197\n",
            "[939/ 938] loss: 0.714881, Accuracy: 0.745269\n",
            "epochs 7\n",
            "[2/ 1] loss: 0.828608, Accuracy: 0.750000\n",
            "[3/ 2] loss: 0.794658, Accuracy: 0.734375\n",
            "[4/ 3] loss: 0.762170, Accuracy: 0.739583\n",
            "[5/ 4] loss: 0.755377, Accuracy: 0.738281\n",
            "[6/ 5] loss: 0.779143, Accuracy: 0.721875\n",
            "[7/ 6] loss: 0.757479, Accuracy: 0.734375\n",
            "[8/ 7] loss: 0.792835, Accuracy: 0.718750\n",
            "[9/ 8] loss: 0.783810, Accuracy: 0.722656\n",
            "[10/ 9] loss: 0.759658, Accuracy: 0.736111\n",
            "[11/ 10] loss: 0.758171, Accuracy: 0.735937\n",
            "[12/ 11] loss: 0.779714, Accuracy: 0.725852\n",
            "[13/ 12] loss: 0.784882, Accuracy: 0.723958\n",
            "[14/ 13] loss: 0.776814, Accuracy: 0.727163\n",
            "[15/ 14] loss: 0.760767, Accuracy: 0.734375\n",
            "[16/ 15] loss: 0.762700, Accuracy: 0.731250\n",
            "[17/ 16] loss: 0.746576, Accuracy: 0.736328\n",
            "[18/ 17] loss: 0.733635, Accuracy: 0.740809\n",
            "[19/ 18] loss: 0.739676, Accuracy: 0.738715\n",
            "[20/ 19] loss: 0.738394, Accuracy: 0.738487\n",
            "[21/ 20] loss: 0.733060, Accuracy: 0.740625\n",
            "[22/ 21] loss: 0.724738, Accuracy: 0.741815\n",
            "[23/ 22] loss: 0.721570, Accuracy: 0.742898\n",
            "[24/ 23] loss: 0.717384, Accuracy: 0.743207\n",
            "[25/ 24] loss: 0.732177, Accuracy: 0.740885\n",
            "[26/ 25] loss: 0.728542, Accuracy: 0.741250\n",
            "[27/ 26] loss: 0.723576, Accuracy: 0.742788\n",
            "[28/ 27] loss: 0.724310, Accuracy: 0.743634\n",
            "[29/ 28] loss: 0.722549, Accuracy: 0.743304\n",
            "[30/ 29] loss: 0.730745, Accuracy: 0.741379\n",
            "[31/ 30] loss: 0.729306, Accuracy: 0.741146\n",
            "[32/ 31] loss: 0.732401, Accuracy: 0.741431\n",
            "[33/ 32] loss: 0.726234, Accuracy: 0.743164\n",
            "[34/ 33] loss: 0.732203, Accuracy: 0.741004\n",
            "[35/ 34] loss: 0.726148, Accuracy: 0.742647\n",
            "[36/ 35] loss: 0.726123, Accuracy: 0.741964\n",
            "[37/ 36] loss: 0.723544, Accuracy: 0.742188\n",
            "[38/ 37] loss: 0.722957, Accuracy: 0.741976\n",
            "[39/ 38] loss: 0.720392, Accuracy: 0.742599\n",
            "[40/ 39] loss: 0.725839, Accuracy: 0.741987\n",
            "[41/ 40] loss: 0.727785, Accuracy: 0.741016\n",
            "[42/ 41] loss: 0.728812, Accuracy: 0.741616\n",
            "[43/ 42] loss: 0.730042, Accuracy: 0.741071\n",
            "[44/ 43] loss: 0.732049, Accuracy: 0.739826\n",
            "[45/ 44] loss: 0.728031, Accuracy: 0.741122\n",
            "[46/ 45] loss: 0.735276, Accuracy: 0.738889\n",
            "[47/ 46] loss: 0.739540, Accuracy: 0.736753\n",
            "[48/ 47] loss: 0.739347, Accuracy: 0.736037\n",
            "[49/ 48] loss: 0.737849, Accuracy: 0.736003\n",
            "[50/ 49] loss: 0.736297, Accuracy: 0.736607\n",
            "[51/ 50] loss: 0.739180, Accuracy: 0.735625\n",
            "[52/ 51] loss: 0.736013, Accuracy: 0.736213\n",
            "[53/ 52] loss: 0.736176, Accuracy: 0.736178\n",
            "[54/ 53] loss: 0.737234, Accuracy: 0.735259\n",
            "[55/ 54] loss: 0.740974, Accuracy: 0.734664\n",
            "[56/ 55] loss: 0.743382, Accuracy: 0.734091\n",
            "[57/ 56] loss: 0.742675, Accuracy: 0.734933\n",
            "[58/ 57] loss: 0.742832, Accuracy: 0.734101\n",
            "[59/ 58] loss: 0.746437, Accuracy: 0.732759\n",
            "[60/ 59] loss: 0.748827, Accuracy: 0.731992\n",
            "[61/ 60] loss: 0.747006, Accuracy: 0.732031\n",
            "[62/ 61] loss: 0.745883, Accuracy: 0.732070\n",
            "[63/ 62] loss: 0.746622, Accuracy: 0.731855\n",
            "[64/ 63] loss: 0.742999, Accuracy: 0.733383\n",
            "[65/ 64] loss: 0.743948, Accuracy: 0.732666\n",
            "[66/ 65] loss: 0.744676, Accuracy: 0.733173\n",
            "[67/ 66] loss: 0.747137, Accuracy: 0.732008\n",
            "[68/ 67] loss: 0.750777, Accuracy: 0.730877\n",
            "[69/ 68] loss: 0.749627, Accuracy: 0.731158\n",
            "[70/ 69] loss: 0.750065, Accuracy: 0.730978\n",
            "[71/ 70] loss: 0.751828, Accuracy: 0.730357\n",
            "[72/ 71] loss: 0.751202, Accuracy: 0.730194\n",
            "[73/ 72] loss: 0.749830, Accuracy: 0.730686\n",
            "[74/ 73] loss: 0.751450, Accuracy: 0.729880\n",
            "[75/ 74] loss: 0.749617, Accuracy: 0.730363\n",
            "[76/ 75] loss: 0.747937, Accuracy: 0.731042\n",
            "[77/ 76] loss: 0.750113, Accuracy: 0.730469\n",
            "[78/ 77] loss: 0.751114, Accuracy: 0.729911\n",
            "[79/ 78] loss: 0.750392, Accuracy: 0.730168\n",
            "[80/ 79] loss: 0.750962, Accuracy: 0.729233\n",
            "[81/ 80] loss: 0.752114, Accuracy: 0.728711\n",
            "[82/ 81] loss: 0.752871, Accuracy: 0.728395\n",
            "[83/ 82] loss: 0.755074, Accuracy: 0.727515\n",
            "[84/ 83] loss: 0.751688, Accuracy: 0.728916\n",
            "[85/ 84] loss: 0.749664, Accuracy: 0.729353\n",
            "[86/ 85] loss: 0.747147, Accuracy: 0.730147\n",
            "[87/ 86] loss: 0.746552, Accuracy: 0.730560\n",
            "[88/ 87] loss: 0.744932, Accuracy: 0.730963\n",
            "[89/ 88] loss: 0.745328, Accuracy: 0.731001\n",
            "[90/ 89] loss: 0.746845, Accuracy: 0.730688\n",
            "[91/ 90] loss: 0.745285, Accuracy: 0.731250\n",
            "[92/ 91] loss: 0.745453, Accuracy: 0.731284\n",
            "[93/ 92] loss: 0.745820, Accuracy: 0.730808\n",
            "[94/ 93] loss: 0.748350, Accuracy: 0.729335\n",
            "[95/ 94] loss: 0.745350, Accuracy: 0.730552\n",
            "[96/ 95] loss: 0.742992, Accuracy: 0.731908\n",
            "[97/ 96] loss: 0.744938, Accuracy: 0.730632\n",
            "[98/ 97] loss: 0.744061, Accuracy: 0.730831\n",
            "[99/ 98] loss: 0.743440, Accuracy: 0.730708\n",
            "[100/ 99] loss: 0.744756, Accuracy: 0.730271\n",
            "[101/ 100] loss: 0.745226, Accuracy: 0.730000\n",
            "[102/ 101] loss: 0.746831, Accuracy: 0.729734\n",
            "[103/ 102] loss: 0.748464, Accuracy: 0.729320\n",
            "[104/ 103] loss: 0.748116, Accuracy: 0.729217\n",
            "[105/ 104] loss: 0.747034, Accuracy: 0.729567\n",
            "[106/ 105] loss: 0.748747, Accuracy: 0.728571\n",
            "[107/ 106] loss: 0.746920, Accuracy: 0.729511\n",
            "[108/ 107] loss: 0.748077, Accuracy: 0.728826\n",
            "[109/ 108] loss: 0.745999, Accuracy: 0.729601\n",
            "[110/ 109] loss: 0.745406, Accuracy: 0.729788\n",
            "[111/ 110] loss: 0.743394, Accuracy: 0.730398\n",
            "[112/ 111] loss: 0.742966, Accuracy: 0.730434\n",
            "[113/ 112] loss: 0.741961, Accuracy: 0.730748\n",
            "[114/ 113] loss: 0.740967, Accuracy: 0.730780\n",
            "[115/ 114] loss: 0.740203, Accuracy: 0.731086\n",
            "[116/ 115] loss: 0.738425, Accuracy: 0.731658\n",
            "[117/ 116] loss: 0.737422, Accuracy: 0.732355\n",
            "[118/ 117] loss: 0.736054, Accuracy: 0.733040\n",
            "[119/ 118] loss: 0.735465, Accuracy: 0.733051\n",
            "[120/ 119] loss: 0.735303, Accuracy: 0.733325\n",
            "[121/ 120] loss: 0.734652, Accuracy: 0.733724\n",
            "[122/ 121] loss: 0.735300, Accuracy: 0.733600\n",
            "[123/ 122] loss: 0.734604, Accuracy: 0.733607\n",
            "[124/ 123] loss: 0.733606, Accuracy: 0.734121\n",
            "[125/ 124] loss: 0.733178, Accuracy: 0.734249\n",
            "[126/ 125] loss: 0.732942, Accuracy: 0.734250\n",
            "[127/ 126] loss: 0.734356, Accuracy: 0.734003\n",
            "[128/ 127] loss: 0.732285, Accuracy: 0.734744\n",
            "[129/ 128] loss: 0.730874, Accuracy: 0.735229\n",
            "[130/ 129] loss: 0.731505, Accuracy: 0.734859\n",
            "[131/ 130] loss: 0.731090, Accuracy: 0.735096\n",
            "[132/ 131] loss: 0.730464, Accuracy: 0.735210\n",
            "[133/ 132] loss: 0.728480, Accuracy: 0.735914\n",
            "[134/ 133] loss: 0.728410, Accuracy: 0.736020\n",
            "[135/ 134] loss: 0.726985, Accuracy: 0.736357\n",
            "[136/ 135] loss: 0.725991, Accuracy: 0.736690\n",
            "[137/ 136] loss: 0.725488, Accuracy: 0.736903\n",
            "[138/ 137] loss: 0.725900, Accuracy: 0.736770\n",
            "[139/ 138] loss: 0.726025, Accuracy: 0.736866\n",
            "[140/ 139] loss: 0.725179, Accuracy: 0.737185\n",
            "[141/ 140] loss: 0.723284, Accuracy: 0.737835\n",
            "[142/ 141] loss: 0.722349, Accuracy: 0.738032\n",
            "[143/ 142] loss: 0.722399, Accuracy: 0.738006\n",
            "[144/ 143] loss: 0.722646, Accuracy: 0.737981\n",
            "[145/ 144] loss: 0.722492, Accuracy: 0.737847\n",
            "[146/ 145] loss: 0.723903, Accuracy: 0.737392\n",
            "[147/ 146] loss: 0.722816, Accuracy: 0.737800\n",
            "[148/ 147] loss: 0.721726, Accuracy: 0.738202\n",
            "[149/ 148] loss: 0.721681, Accuracy: 0.738492\n",
            "[150/ 149] loss: 0.721062, Accuracy: 0.738570\n",
            "[151/ 150] loss: 0.719802, Accuracy: 0.738854\n",
            "[152/ 151] loss: 0.717983, Accuracy: 0.739652\n",
            "[153/ 152] loss: 0.718770, Accuracy: 0.739104\n",
            "[154/ 153] loss: 0.718909, Accuracy: 0.738971\n",
            "[155/ 154] loss: 0.719427, Accuracy: 0.738839\n",
            "[156/ 155] loss: 0.719696, Accuracy: 0.738710\n",
            "[157/ 156] loss: 0.719254, Accuracy: 0.738682\n",
            "[158/ 157] loss: 0.718975, Accuracy: 0.738754\n",
            "[159/ 158] loss: 0.719139, Accuracy: 0.738726\n",
            "[160/ 159] loss: 0.719080, Accuracy: 0.738797\n",
            "[161/ 160] loss: 0.718677, Accuracy: 0.738965\n",
            "[162/ 161] loss: 0.717795, Accuracy: 0.739130\n",
            "[163/ 162] loss: 0.718178, Accuracy: 0.739294\n",
            "[164/ 163] loss: 0.719116, Accuracy: 0.738880\n",
            "[165/ 164] loss: 0.719467, Accuracy: 0.738853\n",
            "[166/ 165] loss: 0.718525, Accuracy: 0.739489\n",
            "[167/ 166] loss: 0.719849, Accuracy: 0.739081\n",
            "[168/ 167] loss: 0.719858, Accuracy: 0.738866\n",
            "[169/ 168] loss: 0.719892, Accuracy: 0.738839\n",
            "[170/ 169] loss: 0.718740, Accuracy: 0.739460\n",
            "[171/ 170] loss: 0.718809, Accuracy: 0.739246\n",
            "[172/ 171] loss: 0.718364, Accuracy: 0.739218\n",
            "[173/ 172] loss: 0.717912, Accuracy: 0.739281\n",
            "[174/ 173] loss: 0.717064, Accuracy: 0.739794\n",
            "[175/ 174] loss: 0.717742, Accuracy: 0.739583\n",
            "[176/ 175] loss: 0.719505, Accuracy: 0.738839\n",
            "[177/ 176] loss: 0.719133, Accuracy: 0.739080\n",
            "[178/ 177] loss: 0.718095, Accuracy: 0.739407\n",
            "[179/ 178] loss: 0.718240, Accuracy: 0.739291\n",
            "[180/ 179] loss: 0.718522, Accuracy: 0.739263\n",
            "[181/ 180] loss: 0.717073, Accuracy: 0.739757\n",
            "[182/ 181] loss: 0.716854, Accuracy: 0.739900\n",
            "[183/ 182] loss: 0.717860, Accuracy: 0.739354\n",
            "[184/ 183] loss: 0.716727, Accuracy: 0.739839\n",
            "[185/ 184] loss: 0.717509, Accuracy: 0.739725\n",
            "[186/ 185] loss: 0.716906, Accuracy: 0.739780\n",
            "[187/ 186] loss: 0.716969, Accuracy: 0.739667\n",
            "[188/ 187] loss: 0.719099, Accuracy: 0.738887\n",
            "[189/ 188] loss: 0.718844, Accuracy: 0.738946\n",
            "[190/ 189] loss: 0.719726, Accuracy: 0.738591\n",
            "[191/ 190] loss: 0.719674, Accuracy: 0.738651\n",
            "[192/ 191] loss: 0.719111, Accuracy: 0.738711\n",
            "[193/ 192] loss: 0.718345, Accuracy: 0.738932\n",
            "[194/ 193] loss: 0.718841, Accuracy: 0.738828\n",
            "[195/ 194] loss: 0.719035, Accuracy: 0.738805\n",
            "[196/ 195] loss: 0.718448, Accuracy: 0.738942\n",
            "[197/ 196] loss: 0.719028, Accuracy: 0.738760\n",
            "[198/ 197] loss: 0.718557, Accuracy: 0.738817\n",
            "[199/ 198] loss: 0.718691, Accuracy: 0.738794\n",
            "[200/ 199] loss: 0.718351, Accuracy: 0.738850\n",
            "[201/ 200] loss: 0.717792, Accuracy: 0.739219\n",
            "[202/ 201] loss: 0.717695, Accuracy: 0.739272\n",
            "[203/ 202] loss: 0.717824, Accuracy: 0.739093\n",
            "[204/ 203] loss: 0.717636, Accuracy: 0.739532\n",
            "[205/ 204] loss: 0.717733, Accuracy: 0.739507\n",
            "[206/ 205] loss: 0.717039, Accuracy: 0.739710\n",
            "[207/ 206] loss: 0.717932, Accuracy: 0.739154\n",
            "[208/ 207] loss: 0.718656, Accuracy: 0.738828\n",
            "[209/ 208] loss: 0.717982, Accuracy: 0.738957\n",
            "[210/ 209] loss: 0.718648, Accuracy: 0.738487\n",
            "[211/ 210] loss: 0.718394, Accuracy: 0.738616\n",
            "[212/ 211] loss: 0.717097, Accuracy: 0.738966\n",
            "[213/ 212] loss: 0.716227, Accuracy: 0.739460\n",
            "[214/ 213] loss: 0.715271, Accuracy: 0.739730\n",
            "[215/ 214] loss: 0.713913, Accuracy: 0.740216\n",
            "[216/ 215] loss: 0.713672, Accuracy: 0.740407\n",
            "[217/ 216] loss: 0.713555, Accuracy: 0.740451\n",
            "[218/ 217] loss: 0.714671, Accuracy: 0.739847\n",
            "[219/ 218] loss: 0.713152, Accuracy: 0.740396\n",
            "[220/ 219] loss: 0.712999, Accuracy: 0.740582\n",
            "[221/ 220] loss: 0.712588, Accuracy: 0.740696\n",
            "[222/ 221] loss: 0.711672, Accuracy: 0.741092\n",
            "[223/ 222] loss: 0.713245, Accuracy: 0.740569\n",
            "[224/ 223] loss: 0.712641, Accuracy: 0.740611\n",
            "[225/ 224] loss: 0.712628, Accuracy: 0.740792\n",
            "[226/ 225] loss: 0.711148, Accuracy: 0.741319\n",
            "[227/ 226] loss: 0.711287, Accuracy: 0.741220\n",
            "[228/ 227] loss: 0.711115, Accuracy: 0.741396\n",
            "[229/ 228] loss: 0.710990, Accuracy: 0.741571\n",
            "[230/ 229] loss: 0.710057, Accuracy: 0.741744\n",
            "[231/ 230] loss: 0.709704, Accuracy: 0.742052\n",
            "[232/ 231] loss: 0.709418, Accuracy: 0.742154\n",
            "[233/ 232] loss: 0.708552, Accuracy: 0.742390\n",
            "[234/ 233] loss: 0.708664, Accuracy: 0.742489\n",
            "[235/ 234] loss: 0.708666, Accuracy: 0.742588\n",
            "[236/ 235] loss: 0.708001, Accuracy: 0.742886\n",
            "[237/ 236] loss: 0.708296, Accuracy: 0.742783\n",
            "[238/ 237] loss: 0.709505, Accuracy: 0.742484\n",
            "[239/ 238] loss: 0.709570, Accuracy: 0.742581\n",
            "[240/ 239] loss: 0.709815, Accuracy: 0.742547\n",
            "[241/ 240] loss: 0.709330, Accuracy: 0.742839\n",
            "[242/ 241] loss: 0.709376, Accuracy: 0.742868\n",
            "[243/ 242] loss: 0.709809, Accuracy: 0.742898\n",
            "[244/ 243] loss: 0.710176, Accuracy: 0.742670\n",
            "[245/ 244] loss: 0.710163, Accuracy: 0.742636\n",
            "[246/ 245] loss: 0.710315, Accuracy: 0.742666\n",
            "[247/ 246] loss: 0.710680, Accuracy: 0.742315\n",
            "[248/ 247] loss: 0.711198, Accuracy: 0.742346\n",
            "[249/ 248] loss: 0.711820, Accuracy: 0.742124\n",
            "[250/ 249] loss: 0.712766, Accuracy: 0.741968\n",
            "[251/ 250] loss: 0.713668, Accuracy: 0.741750\n",
            "[252/ 251] loss: 0.713013, Accuracy: 0.741970\n",
            "[253/ 252] loss: 0.713187, Accuracy: 0.741939\n",
            "[254/ 253] loss: 0.712481, Accuracy: 0.742218\n",
            "[255/ 254] loss: 0.713159, Accuracy: 0.742126\n",
            "[256/ 255] loss: 0.712437, Accuracy: 0.742341\n",
            "[257/ 256] loss: 0.712783, Accuracy: 0.742188\n",
            "[258/ 257] loss: 0.713786, Accuracy: 0.741853\n",
            "[259/ 258] loss: 0.713806, Accuracy: 0.741703\n",
            "[260/ 259] loss: 0.713881, Accuracy: 0.741614\n",
            "[261/ 260] loss: 0.714181, Accuracy: 0.741587\n",
            "[262/ 261] loss: 0.713649, Accuracy: 0.741798\n",
            "[263/ 262] loss: 0.713679, Accuracy: 0.741710\n",
            "[264/ 263] loss: 0.713678, Accuracy: 0.741623\n",
            "[265/ 264] loss: 0.712855, Accuracy: 0.742010\n",
            "[266/ 265] loss: 0.712791, Accuracy: 0.742040\n",
            "[267/ 266] loss: 0.713196, Accuracy: 0.741953\n",
            "[268/ 267] loss: 0.713307, Accuracy: 0.742100\n",
            "[269/ 268] loss: 0.713644, Accuracy: 0.742013\n",
            "[270/ 269] loss: 0.713973, Accuracy: 0.741868\n",
            "[271/ 270] loss: 0.714724, Accuracy: 0.741551\n",
            "[272/ 271] loss: 0.714454, Accuracy: 0.741640\n",
            "[273/ 272] loss: 0.714230, Accuracy: 0.741728\n",
            "[274/ 273] loss: 0.714439, Accuracy: 0.741701\n",
            "[275/ 274] loss: 0.715103, Accuracy: 0.741560\n",
            "[276/ 275] loss: 0.715956, Accuracy: 0.741364\n",
            "[277/ 276] loss: 0.715216, Accuracy: 0.741565\n",
            "[278/ 277] loss: 0.714668, Accuracy: 0.741821\n",
            "[279/ 278] loss: 0.714354, Accuracy: 0.741850\n",
            "[280/ 279] loss: 0.715276, Accuracy: 0.741487\n",
            "[281/ 280] loss: 0.714065, Accuracy: 0.741908\n",
            "[282/ 281] loss: 0.714137, Accuracy: 0.741826\n",
            "[283/ 282] loss: 0.713930, Accuracy: 0.741855\n",
            "[284/ 283] loss: 0.714380, Accuracy: 0.741884\n",
            "[285/ 284] loss: 0.714232, Accuracy: 0.741967\n",
            "[286/ 285] loss: 0.714205, Accuracy: 0.741886\n",
            "[287/ 286] loss: 0.714835, Accuracy: 0.741696\n",
            "[288/ 287] loss: 0.714826, Accuracy: 0.741670\n",
            "[289/ 288] loss: 0.715100, Accuracy: 0.741536\n",
            "[290/ 289] loss: 0.715115, Accuracy: 0.741782\n",
            "[291/ 290] loss: 0.715539, Accuracy: 0.741595\n",
            "[292/ 291] loss: 0.715377, Accuracy: 0.741624\n",
            "[293/ 292] loss: 0.715409, Accuracy: 0.741652\n",
            "[294/ 293] loss: 0.715212, Accuracy: 0.741681\n",
            "[295/ 294] loss: 0.714575, Accuracy: 0.742028\n",
            "[296/ 295] loss: 0.714448, Accuracy: 0.742161\n",
            "[297/ 296] loss: 0.714417, Accuracy: 0.742293\n",
            "[298/ 297] loss: 0.714790, Accuracy: 0.742161\n",
            "[299/ 298] loss: 0.714767, Accuracy: 0.742135\n",
            "[300/ 299] loss: 0.715135, Accuracy: 0.742109\n",
            "[301 / 300]  Loss:0.714656, Accuracy:0.742292\n",
            "[301/ 300] loss: 0.714656, Accuracy: 0.742292\n",
            "[302/ 301] loss: 0.714291, Accuracy: 0.742421\n",
            "[303/ 302] loss: 0.714295, Accuracy: 0.742343\n",
            "[304/ 303] loss: 0.714348, Accuracy: 0.742368\n",
            "[305/ 304] loss: 0.713735, Accuracy: 0.742650\n",
            "[306/ 305] loss: 0.713009, Accuracy: 0.742930\n",
            "[307/ 306] loss: 0.712616, Accuracy: 0.743005\n",
            "[308/ 307] loss: 0.711850, Accuracy: 0.743231\n",
            "[309/ 308] loss: 0.711857, Accuracy: 0.743304\n",
            "[310/ 309] loss: 0.711259, Accuracy: 0.743477\n",
            "[311/ 310] loss: 0.710155, Accuracy: 0.743952\n",
            "[312/ 311] loss: 0.709688, Accuracy: 0.744072\n",
            "[313/ 312] loss: 0.709628, Accuracy: 0.744141\n",
            "[314/ 313] loss: 0.711010, Accuracy: 0.743810\n",
            "[315/ 314] loss: 0.711419, Accuracy: 0.743680\n",
            "[316/ 315] loss: 0.711426, Accuracy: 0.743651\n",
            "[317/ 316] loss: 0.711976, Accuracy: 0.743374\n",
            "[318/ 317] loss: 0.712118, Accuracy: 0.743395\n",
            "[319/ 318] loss: 0.712073, Accuracy: 0.743416\n",
            "[320/ 319] loss: 0.712190, Accuracy: 0.743339\n",
            "[321/ 320] loss: 0.711970, Accuracy: 0.743506\n",
            "[322/ 321] loss: 0.712423, Accuracy: 0.743429\n",
            "[323/ 322] loss: 0.713057, Accuracy: 0.743207\n",
            "[324/ 323] loss: 0.713428, Accuracy: 0.743082\n",
            "[325/ 324] loss: 0.714025, Accuracy: 0.742863\n",
            "[326/ 325] loss: 0.714142, Accuracy: 0.742885\n",
            "[327/ 326] loss: 0.713874, Accuracy: 0.743050\n",
            "[328/ 327] loss: 0.713481, Accuracy: 0.743263\n",
            "[329/ 328] loss: 0.713184, Accuracy: 0.743426\n",
            "[330/ 329] loss: 0.712555, Accuracy: 0.743684\n",
            "[331/ 330] loss: 0.712458, Accuracy: 0.743655\n",
            "[332/ 331] loss: 0.711825, Accuracy: 0.743910\n",
            "[333/ 332] loss: 0.712694, Accuracy: 0.743694\n",
            "[334/ 333] loss: 0.712664, Accuracy: 0.743759\n",
            "[335/ 334] loss: 0.712357, Accuracy: 0.743872\n",
            "[336/ 335] loss: 0.712589, Accuracy: 0.743797\n",
            "[337/ 336] loss: 0.713203, Accuracy: 0.743815\n",
            "[338/ 337] loss: 0.713338, Accuracy: 0.743833\n",
            "[339/ 338] loss: 0.713460, Accuracy: 0.743759\n",
            "[340/ 339] loss: 0.713102, Accuracy: 0.743916\n",
            "[341/ 340] loss: 0.713808, Accuracy: 0.743704\n",
            "[342/ 341] loss: 0.712867, Accuracy: 0.744043\n",
            "[343/ 342] loss: 0.712907, Accuracy: 0.744061\n",
            "[344/ 343] loss: 0.713224, Accuracy: 0.743896\n",
            "[345/ 344] loss: 0.713108, Accuracy: 0.744004\n",
            "[346/ 345] loss: 0.712904, Accuracy: 0.744022\n",
            "[347/ 346] loss: 0.712901, Accuracy: 0.743949\n",
            "[348/ 347] loss: 0.712407, Accuracy: 0.744101\n",
            "[349/ 348] loss: 0.711847, Accuracy: 0.744343\n",
            "[350/ 349] loss: 0.712543, Accuracy: 0.744225\n",
            "[351/ 350] loss: 0.712178, Accuracy: 0.744330\n",
            "[352/ 351] loss: 0.712249, Accuracy: 0.744168\n",
            "[353/ 352] loss: 0.711905, Accuracy: 0.744274\n",
            "[354/ 353] loss: 0.711742, Accuracy: 0.744379\n",
            "[355/ 354] loss: 0.711314, Accuracy: 0.744615\n",
            "[356/ 355] loss: 0.710906, Accuracy: 0.744718\n",
            "[357/ 356] loss: 0.710917, Accuracy: 0.744777\n",
            "[358/ 357] loss: 0.710946, Accuracy: 0.744748\n",
            "[359/ 358] loss: 0.710643, Accuracy: 0.744806\n",
            "[360/ 359] loss: 0.711379, Accuracy: 0.744473\n",
            "[361/ 360] loss: 0.711504, Accuracy: 0.744358\n",
            "[362/ 361] loss: 0.711442, Accuracy: 0.744330\n",
            "[363/ 362] loss: 0.711087, Accuracy: 0.744518\n",
            "[364/ 363] loss: 0.711406, Accuracy: 0.744404\n",
            "[365/ 364] loss: 0.711630, Accuracy: 0.744420\n",
            "[366/ 365] loss: 0.711138, Accuracy: 0.744606\n",
            "[367/ 366] loss: 0.710586, Accuracy: 0.744749\n",
            "[368/ 367] loss: 0.710693, Accuracy: 0.744721\n",
            "[369/ 368] loss: 0.710223, Accuracy: 0.744905\n",
            "[370/ 369] loss: 0.710170, Accuracy: 0.744961\n",
            "[371/ 370] loss: 0.710273, Accuracy: 0.744932\n",
            "[372/ 371] loss: 0.710434, Accuracy: 0.744988\n",
            "[373/ 372] loss: 0.710646, Accuracy: 0.745044\n",
            "[374/ 373] loss: 0.711135, Accuracy: 0.744931\n",
            "[375/ 374] loss: 0.711688, Accuracy: 0.744694\n",
            "[376/ 375] loss: 0.711597, Accuracy: 0.744750\n",
            "[377/ 376] loss: 0.711619, Accuracy: 0.744764\n",
            "[378/ 377] loss: 0.711189, Accuracy: 0.744944\n",
            "[379/ 378] loss: 0.711049, Accuracy: 0.744998\n",
            "[380/ 379] loss: 0.711121, Accuracy: 0.745012\n",
            "[381/ 380] loss: 0.711727, Accuracy: 0.744737\n",
            "[382/ 381] loss: 0.711750, Accuracy: 0.744669\n",
            "[383/ 382] loss: 0.711597, Accuracy: 0.744764\n",
            "[384/ 383] loss: 0.710890, Accuracy: 0.744941\n",
            "[385/ 384] loss: 0.711089, Accuracy: 0.744792\n",
            "[386/ 385] loss: 0.710671, Accuracy: 0.744927\n",
            "[387/ 386] loss: 0.710614, Accuracy: 0.744900\n",
            "[388/ 387] loss: 0.710939, Accuracy: 0.744671\n",
            "[389/ 388] loss: 0.711925, Accuracy: 0.744282\n",
            "[390/ 389] loss: 0.711889, Accuracy: 0.744336\n",
            "[391/ 390] loss: 0.712476, Accuracy: 0.744071\n",
            "[392/ 391] loss: 0.712565, Accuracy: 0.744046\n",
            "[393/ 392] loss: 0.712049, Accuracy: 0.744260\n",
            "[394/ 393] loss: 0.712310, Accuracy: 0.744156\n",
            "[395/ 394] loss: 0.712726, Accuracy: 0.743893\n",
            "[396/ 395] loss: 0.712270, Accuracy: 0.744027\n",
            "[397/ 396] loss: 0.713146, Accuracy: 0.743726\n",
            "[398/ 397] loss: 0.713841, Accuracy: 0.743506\n",
            "[399/ 398] loss: 0.713716, Accuracy: 0.743562\n",
            "[400/ 399] loss: 0.713944, Accuracy: 0.743421\n",
            "[401/ 400] loss: 0.713500, Accuracy: 0.743594\n",
            "[402/ 401] loss: 0.714085, Accuracy: 0.743415\n",
            "[403/ 402] loss: 0.714110, Accuracy: 0.743392\n",
            "[404/ 403] loss: 0.714289, Accuracy: 0.743409\n",
            "[405/ 404] loss: 0.714363, Accuracy: 0.743386\n",
            "[406/ 405] loss: 0.714950, Accuracy: 0.743056\n",
            "[407/ 406] loss: 0.715165, Accuracy: 0.742996\n",
            "[408/ 407] loss: 0.714892, Accuracy: 0.743128\n",
            "[409/ 408] loss: 0.714744, Accuracy: 0.743222\n",
            "[410/ 409] loss: 0.714790, Accuracy: 0.743391\n",
            "[411/ 410] loss: 0.715098, Accuracy: 0.743369\n",
            "[412/ 411] loss: 0.714787, Accuracy: 0.743461\n",
            "[413/ 412] loss: 0.714540, Accuracy: 0.743629\n",
            "[414/ 413] loss: 0.714802, Accuracy: 0.743644\n",
            "[415/ 414] loss: 0.714560, Accuracy: 0.743697\n",
            "[416/ 415] loss: 0.714320, Accuracy: 0.743788\n",
            "[417/ 416] loss: 0.714514, Accuracy: 0.743690\n",
            "[418/ 417] loss: 0.714121, Accuracy: 0.743892\n",
            "[419/ 418] loss: 0.714697, Accuracy: 0.743608\n",
            "[420/ 419] loss: 0.714696, Accuracy: 0.743549\n",
            "[421/ 420] loss: 0.714964, Accuracy: 0.743378\n",
            "[422/ 421] loss: 0.715533, Accuracy: 0.743171\n",
            "[423/ 422] loss: 0.715534, Accuracy: 0.743335\n",
            "[424/ 423] loss: 0.715267, Accuracy: 0.743425\n",
            "[425/ 424] loss: 0.715086, Accuracy: 0.743514\n",
            "[426/ 425] loss: 0.715133, Accuracy: 0.743419\n",
            "[427/ 426] loss: 0.714909, Accuracy: 0.743435\n",
            "[428/ 427] loss: 0.714468, Accuracy: 0.743596\n",
            "[429/ 428] loss: 0.715058, Accuracy: 0.743356\n",
            "[430/ 429] loss: 0.714877, Accuracy: 0.743408\n",
            "[431/ 430] loss: 0.714888, Accuracy: 0.743387\n",
            "[432/ 431] loss: 0.714717, Accuracy: 0.743402\n",
            "[433/ 432] loss: 0.714125, Accuracy: 0.743634\n",
            "[434/ 433] loss: 0.713802, Accuracy: 0.743757\n",
            "[435/ 434] loss: 0.713511, Accuracy: 0.743808\n",
            "[436/ 435] loss: 0.713101, Accuracy: 0.743894\n",
            "[437/ 436] loss: 0.713651, Accuracy: 0.743800\n",
            "[438/ 437] loss: 0.713851, Accuracy: 0.743743\n",
            "[439/ 438] loss: 0.713385, Accuracy: 0.743900\n",
            "[440/ 439] loss: 0.712909, Accuracy: 0.744127\n",
            "[441/ 440] loss: 0.712939, Accuracy: 0.744176\n",
            "[442/ 441] loss: 0.713469, Accuracy: 0.744048\n",
            "[443/ 442] loss: 0.713143, Accuracy: 0.744096\n",
            "[444/ 443] loss: 0.713082, Accuracy: 0.744110\n",
            "[445/ 444] loss: 0.712622, Accuracy: 0.744264\n",
            "[446/ 445] loss: 0.712504, Accuracy: 0.744277\n",
            "[447/ 446] loss: 0.712203, Accuracy: 0.744395\n",
            "[448/ 447] loss: 0.711885, Accuracy: 0.744477\n",
            "[449/ 448] loss: 0.712034, Accuracy: 0.744420\n",
            "[450/ 449] loss: 0.712086, Accuracy: 0.744328\n",
            "[451/ 450] loss: 0.711947, Accuracy: 0.744410\n",
            "[452/ 451] loss: 0.711552, Accuracy: 0.744630\n",
            "[453/ 452] loss: 0.711380, Accuracy: 0.744711\n",
            "[454/ 453] loss: 0.711508, Accuracy: 0.744723\n",
            "[455/ 454] loss: 0.711279, Accuracy: 0.744838\n",
            "[456/ 455] loss: 0.711142, Accuracy: 0.744883\n",
            "[457/ 456] loss: 0.710889, Accuracy: 0.744997\n",
            "[458/ 457] loss: 0.711365, Accuracy: 0.744803\n",
            "[459/ 458] loss: 0.711363, Accuracy: 0.744849\n",
            "[460/ 459] loss: 0.711013, Accuracy: 0.744928\n",
            "[461/ 460] loss: 0.711066, Accuracy: 0.744939\n",
            "[462/ 461] loss: 0.710867, Accuracy: 0.745018\n",
            "[463/ 462] loss: 0.711015, Accuracy: 0.744893\n",
            "[464/ 463] loss: 0.711119, Accuracy: 0.744837\n",
            "[465/ 464] loss: 0.711369, Accuracy: 0.744780\n",
            "[466/ 465] loss: 0.711397, Accuracy: 0.744691\n",
            "[467/ 466] loss: 0.711350, Accuracy: 0.744702\n",
            "[468/ 467] loss: 0.711334, Accuracy: 0.744747\n",
            "[469/ 468] loss: 0.711552, Accuracy: 0.744625\n",
            "[470/ 469] loss: 0.711769, Accuracy: 0.744570\n",
            "[471/ 470] loss: 0.711858, Accuracy: 0.744548\n",
            "[472/ 471] loss: 0.711866, Accuracy: 0.744559\n",
            "[473/ 472] loss: 0.711758, Accuracy: 0.744604\n",
            "[474/ 473] loss: 0.711479, Accuracy: 0.744682\n",
            "[475/ 474] loss: 0.711168, Accuracy: 0.744792\n",
            "[476/ 475] loss: 0.711089, Accuracy: 0.744770\n",
            "[477/ 476] loss: 0.711273, Accuracy: 0.744715\n",
            "[478/ 477] loss: 0.711716, Accuracy: 0.744595\n",
            "[479/ 478] loss: 0.711402, Accuracy: 0.744704\n",
            "[480/ 479] loss: 0.711255, Accuracy: 0.744716\n",
            "[481/ 480] loss: 0.710801, Accuracy: 0.744889\n",
            "[482/ 481] loss: 0.710779, Accuracy: 0.744900\n",
            "[483/ 482] loss: 0.710582, Accuracy: 0.744911\n",
            "[484/ 483] loss: 0.710865, Accuracy: 0.744759\n",
            "[485/ 484] loss: 0.710618, Accuracy: 0.744964\n",
            "[486/ 485] loss: 0.710195, Accuracy: 0.745135\n",
            "[487/ 486] loss: 0.709733, Accuracy: 0.745435\n",
            "[488/ 487] loss: 0.709932, Accuracy: 0.745316\n",
            "[489/ 488] loss: 0.710136, Accuracy: 0.745229\n",
            "[490/ 489] loss: 0.710253, Accuracy: 0.745143\n",
            "[491/ 490] loss: 0.709919, Accuracy: 0.745281\n",
            "[492/ 491] loss: 0.710190, Accuracy: 0.745195\n",
            "[493/ 492] loss: 0.710345, Accuracy: 0.745109\n",
            "[494/ 493] loss: 0.710340, Accuracy: 0.745119\n",
            "[495/ 494] loss: 0.710530, Accuracy: 0.745034\n",
            "[496/ 495] loss: 0.710828, Accuracy: 0.744950\n",
            "[497/ 496] loss: 0.711122, Accuracy: 0.744771\n",
            "[498/ 497] loss: 0.710515, Accuracy: 0.745064\n",
            "[499/ 498] loss: 0.710227, Accuracy: 0.745200\n",
            "[500/ 499] loss: 0.710284, Accuracy: 0.745178\n",
            "[501/ 500] loss: 0.710515, Accuracy: 0.745125\n",
            "[502/ 501] loss: 0.710293, Accuracy: 0.745166\n",
            "[503/ 502] loss: 0.710152, Accuracy: 0.745144\n",
            "[504/ 503] loss: 0.709832, Accuracy: 0.745247\n",
            "[505/ 504] loss: 0.710280, Accuracy: 0.745164\n",
            "[506/ 505] loss: 0.709816, Accuracy: 0.745390\n",
            "[507/ 506] loss: 0.709845, Accuracy: 0.745399\n",
            "[508/ 507] loss: 0.709726, Accuracy: 0.745377\n",
            "[509/ 508] loss: 0.709535, Accuracy: 0.745448\n",
            "[510/ 509] loss: 0.709177, Accuracy: 0.745641\n",
            "[511/ 510] loss: 0.708916, Accuracy: 0.745680\n",
            "[512/ 511] loss: 0.708882, Accuracy: 0.745719\n",
            "[513/ 512] loss: 0.708907, Accuracy: 0.745728\n",
            "[514/ 513] loss: 0.708644, Accuracy: 0.745797\n",
            "[515/ 514] loss: 0.708739, Accuracy: 0.745835\n",
            "[516/ 515] loss: 0.708463, Accuracy: 0.745904\n",
            "[517/ 516] loss: 0.708594, Accuracy: 0.745852\n",
            "[518/ 517] loss: 0.708166, Accuracy: 0.745980\n",
            "[519/ 518] loss: 0.708260, Accuracy: 0.746049\n",
            "[520/ 519] loss: 0.708119, Accuracy: 0.746056\n",
            "[521/ 520] loss: 0.707486, Accuracy: 0.746304\n",
            "[522/ 521] loss: 0.706940, Accuracy: 0.746491\n",
            "[523/ 522] loss: 0.707060, Accuracy: 0.746498\n",
            "[524/ 523] loss: 0.707008, Accuracy: 0.746564\n",
            "[525/ 524] loss: 0.706871, Accuracy: 0.746690\n",
            "[526/ 525] loss: 0.706735, Accuracy: 0.746756\n",
            "[527/ 526] loss: 0.707118, Accuracy: 0.746584\n",
            "[528/ 527] loss: 0.707195, Accuracy: 0.746561\n",
            "[529/ 528] loss: 0.707280, Accuracy: 0.746597\n",
            "[530/ 529] loss: 0.707653, Accuracy: 0.746426\n",
            "[531/ 530] loss: 0.707562, Accuracy: 0.746462\n",
            "[532/ 531] loss: 0.707461, Accuracy: 0.746469\n",
            "[533/ 532] loss: 0.707900, Accuracy: 0.746387\n",
            "[534/ 533] loss: 0.707803, Accuracy: 0.746365\n",
            "[535/ 534] loss: 0.707480, Accuracy: 0.746489\n",
            "[536/ 535] loss: 0.706960, Accuracy: 0.746671\n",
            "[537/ 536] loss: 0.706996, Accuracy: 0.746618\n",
            "[538/ 537] loss: 0.707153, Accuracy: 0.746479\n",
            "[539/ 538] loss: 0.707353, Accuracy: 0.746399\n",
            "[540/ 539] loss: 0.707094, Accuracy: 0.746463\n",
            "[541/ 540] loss: 0.707378, Accuracy: 0.746325\n",
            "[542/ 541] loss: 0.707583, Accuracy: 0.746274\n",
            "[543/ 542] loss: 0.707695, Accuracy: 0.746195\n",
            "[544/ 543] loss: 0.707582, Accuracy: 0.746230\n",
            "[545/ 544] loss: 0.707488, Accuracy: 0.746295\n",
            "[546/ 545] loss: 0.707864, Accuracy: 0.746158\n",
            "[547/ 546] loss: 0.707896, Accuracy: 0.746079\n",
            "[548/ 547] loss: 0.707715, Accuracy: 0.746172\n",
            "[549/ 548] loss: 0.707208, Accuracy: 0.746293\n",
            "[550/ 549] loss: 0.707367, Accuracy: 0.746215\n",
            "[551/ 550] loss: 0.707142, Accuracy: 0.746278\n",
            "[552/ 551] loss: 0.707007, Accuracy: 0.746285\n",
            "[553/ 552] loss: 0.706930, Accuracy: 0.746292\n",
            "[554/ 553] loss: 0.706836, Accuracy: 0.746299\n",
            "[555/ 554] loss: 0.706741, Accuracy: 0.746305\n",
            "[556/ 555] loss: 0.706596, Accuracy: 0.746312\n",
            "[557/ 556] loss: 0.706810, Accuracy: 0.746234\n",
            "[558/ 557] loss: 0.706729, Accuracy: 0.746269\n",
            "[559/ 558] loss: 0.706784, Accuracy: 0.746276\n",
            "[560/ 559] loss: 0.706953, Accuracy: 0.746199\n",
            "[561/ 560] loss: 0.706973, Accuracy: 0.746205\n",
            "[562/ 561] loss: 0.706567, Accuracy: 0.746351\n",
            "[563/ 562] loss: 0.706691, Accuracy: 0.746302\n",
            "[564/ 563] loss: 0.706442, Accuracy: 0.746337\n",
            "[565/ 564] loss: 0.706531, Accuracy: 0.746288\n",
            "[566/ 565] loss: 0.706706, Accuracy: 0.746184\n",
            "[567/ 566] loss: 0.706867, Accuracy: 0.746108\n",
            "[568/ 567] loss: 0.707134, Accuracy: 0.745921\n",
            "[569/ 568] loss: 0.707144, Accuracy: 0.745901\n",
            "[570/ 569] loss: 0.707416, Accuracy: 0.745826\n",
            "[571/ 570] loss: 0.707456, Accuracy: 0.745833\n",
            "[572/ 571] loss: 0.707318, Accuracy: 0.745895\n",
            "[573/ 572] loss: 0.707274, Accuracy: 0.745848\n",
            "[574/ 573] loss: 0.706826, Accuracy: 0.746019\n",
            "[575/ 574] loss: 0.707124, Accuracy: 0.745862\n",
            "[576/ 575] loss: 0.707225, Accuracy: 0.745761\n",
            "[577/ 576] loss: 0.706910, Accuracy: 0.745877\n",
            "[578/ 577] loss: 0.706804, Accuracy: 0.745938\n",
            "[579/ 578] loss: 0.706287, Accuracy: 0.746107\n",
            "[580/ 579] loss: 0.706221, Accuracy: 0.746087\n",
            "[581/ 580] loss: 0.706279, Accuracy: 0.746148\n",
            "[582/ 581] loss: 0.706814, Accuracy: 0.746047\n",
            "[583/ 582] loss: 0.706811, Accuracy: 0.746080\n",
            "[584/ 583] loss: 0.706593, Accuracy: 0.746167\n",
            "[585/ 584] loss: 0.706436, Accuracy: 0.746201\n",
            "[586/ 585] loss: 0.706255, Accuracy: 0.746261\n",
            "[587/ 586] loss: 0.706222, Accuracy: 0.746294\n",
            "[588/ 587] loss: 0.706323, Accuracy: 0.746247\n",
            "[589/ 588] loss: 0.706343, Accuracy: 0.746200\n",
            "[590/ 589] loss: 0.706469, Accuracy: 0.746153\n",
            "[591/ 590] loss: 0.706414, Accuracy: 0.746107\n",
            "[592/ 591] loss: 0.706515, Accuracy: 0.746034\n",
            "[593/ 592] loss: 0.706560, Accuracy: 0.746015\n",
            "[594/ 593] loss: 0.706773, Accuracy: 0.745969\n",
            "[595/ 594] loss: 0.706572, Accuracy: 0.746002\n",
            "[596/ 595] loss: 0.706794, Accuracy: 0.745877\n",
            "[597/ 596] loss: 0.706897, Accuracy: 0.745858\n",
            "[598/ 597] loss: 0.706762, Accuracy: 0.745943\n",
            "[599/ 598] loss: 0.707017, Accuracy: 0.745846\n",
            "[600/ 599] loss: 0.706788, Accuracy: 0.745905\n",
            "[601 / 600]  Loss:0.706624, Accuracy:0.745938\n",
            "[601/ 600] loss: 0.706624, Accuracy: 0.745938\n",
            "[602/ 601] loss: 0.706648, Accuracy: 0.745996\n",
            "[603/ 602] loss: 0.706736, Accuracy: 0.746029\n",
            "[604/ 603] loss: 0.707065, Accuracy: 0.745880\n",
            "[605/ 604] loss: 0.706994, Accuracy: 0.745990\n",
            "[606/ 605] loss: 0.706798, Accuracy: 0.746074\n",
            "[607/ 606] loss: 0.706830, Accuracy: 0.746107\n",
            "[608/ 607] loss: 0.706592, Accuracy: 0.746216\n",
            "[609/ 608] loss: 0.706691, Accuracy: 0.746222\n",
            "[610/ 609] loss: 0.706813, Accuracy: 0.746100\n",
            "[611/ 610] loss: 0.706763, Accuracy: 0.746081\n",
            "[612/ 611] loss: 0.706581, Accuracy: 0.746113\n",
            "[613/ 612] loss: 0.706327, Accuracy: 0.746145\n",
            "[614/ 613] loss: 0.706183, Accuracy: 0.746228\n",
            "[615/ 614] loss: 0.706148, Accuracy: 0.746234\n",
            "[616/ 615] loss: 0.706009, Accuracy: 0.746265\n",
            "[617/ 616] loss: 0.705742, Accuracy: 0.746373\n",
            "[618/ 617] loss: 0.705888, Accuracy: 0.746277\n",
            "[619/ 618] loss: 0.705707, Accuracy: 0.746385\n",
            "[620/ 619] loss: 0.706097, Accuracy: 0.746188\n",
            "[621/ 620] loss: 0.706038, Accuracy: 0.746195\n",
            "[622/ 621] loss: 0.705729, Accuracy: 0.746301\n",
            "[623/ 622] loss: 0.705383, Accuracy: 0.746433\n",
            "[624/ 623] loss: 0.705170, Accuracy: 0.746514\n",
            "[625/ 624] loss: 0.705314, Accuracy: 0.746469\n",
            "[626/ 625] loss: 0.705458, Accuracy: 0.746400\n",
            "[627/ 626] loss: 0.705545, Accuracy: 0.746356\n",
            "[628/ 627] loss: 0.705729, Accuracy: 0.746312\n",
            "[629/ 628] loss: 0.705851, Accuracy: 0.746268\n",
            "[630/ 629] loss: 0.706122, Accuracy: 0.746100\n",
            "[631/ 630] loss: 0.706100, Accuracy: 0.746081\n",
            "[632/ 631] loss: 0.705920, Accuracy: 0.746137\n",
            "[633/ 632] loss: 0.705981, Accuracy: 0.746143\n",
            "[634/ 633] loss: 0.706123, Accuracy: 0.746075\n",
            "[635/ 634] loss: 0.706277, Accuracy: 0.746057\n",
            "[636/ 635] loss: 0.706138, Accuracy: 0.746137\n",
            "[637/ 636] loss: 0.706385, Accuracy: 0.745995\n",
            "[638/ 637] loss: 0.706537, Accuracy: 0.745928\n",
            "[639/ 638] loss: 0.706369, Accuracy: 0.746008\n",
            "[640/ 639] loss: 0.706192, Accuracy: 0.745990\n",
            "[641/ 640] loss: 0.705988, Accuracy: 0.746069\n",
            "[642/ 641] loss: 0.705863, Accuracy: 0.746173\n",
            "[643/ 642] loss: 0.705947, Accuracy: 0.746130\n",
            "[644/ 643] loss: 0.705819, Accuracy: 0.746185\n",
            "[645/ 644] loss: 0.705806, Accuracy: 0.746118\n",
            "[646/ 645] loss: 0.705782, Accuracy: 0.746100\n",
            "[647/ 646] loss: 0.706049, Accuracy: 0.746033\n",
            "[648/ 647] loss: 0.705891, Accuracy: 0.746064\n",
            "[649/ 648] loss: 0.705686, Accuracy: 0.746190\n",
            "[650/ 649] loss: 0.705299, Accuracy: 0.746316\n",
            "[651/ 650] loss: 0.705179, Accuracy: 0.746322\n",
            "[652/ 651] loss: 0.704784, Accuracy: 0.746424\n",
            "[653/ 652] loss: 0.704544, Accuracy: 0.746501\n",
            "[654/ 653] loss: 0.704510, Accuracy: 0.746578\n",
            "[655/ 654] loss: 0.704192, Accuracy: 0.746679\n",
            "[656/ 655] loss: 0.704255, Accuracy: 0.746660\n",
            "[657/ 656] loss: 0.703811, Accuracy: 0.746808\n",
            "[658/ 657] loss: 0.703805, Accuracy: 0.746766\n",
            "[659/ 658] loss: 0.704247, Accuracy: 0.746604\n",
            "[660/ 659] loss: 0.704421, Accuracy: 0.746562\n",
            "[661/ 660] loss: 0.704734, Accuracy: 0.746449\n",
            "[662/ 661] loss: 0.704672, Accuracy: 0.746454\n",
            "[663/ 662] loss: 0.704869, Accuracy: 0.746389\n",
            "[664/ 663] loss: 0.704750, Accuracy: 0.746441\n",
            "[665/ 664] loss: 0.704931, Accuracy: 0.746376\n",
            "[666/ 665] loss: 0.704878, Accuracy: 0.746405\n",
            "[667/ 666] loss: 0.705042, Accuracy: 0.746387\n",
            "[668/ 667] loss: 0.705129, Accuracy: 0.746369\n",
            "[669/ 668] loss: 0.704919, Accuracy: 0.746421\n",
            "[670/ 669] loss: 0.705006, Accuracy: 0.746427\n",
            "[671/ 670] loss: 0.704549, Accuracy: 0.746572\n",
            "[672/ 671] loss: 0.704575, Accuracy: 0.746530\n",
            "[673/ 672] loss: 0.704441, Accuracy: 0.746629\n",
            "[674/ 673] loss: 0.704245, Accuracy: 0.746703\n",
            "[675/ 674] loss: 0.704472, Accuracy: 0.746639\n",
            "[676/ 675] loss: 0.704138, Accuracy: 0.746759\n",
            "[677/ 676] loss: 0.704320, Accuracy: 0.746718\n",
            "[678/ 677] loss: 0.704218, Accuracy: 0.746723\n",
            "[679/ 678] loss: 0.704369, Accuracy: 0.746704\n",
            "[680/ 679] loss: 0.704531, Accuracy: 0.746709\n",
            "[681/ 680] loss: 0.704712, Accuracy: 0.746645\n",
            "[682/ 681] loss: 0.704917, Accuracy: 0.746558\n",
            "[683/ 682] loss: 0.705145, Accuracy: 0.746426\n",
            "[684/ 683] loss: 0.705267, Accuracy: 0.746408\n",
            "[685/ 684] loss: 0.705261, Accuracy: 0.746482\n",
            "[686/ 685] loss: 0.704667, Accuracy: 0.746715\n",
            "[687/ 686] loss: 0.704579, Accuracy: 0.746743\n",
            "[688/ 687] loss: 0.704744, Accuracy: 0.746634\n",
            "[689/ 688] loss: 0.704884, Accuracy: 0.746548\n",
            "[690/ 689] loss: 0.705079, Accuracy: 0.746508\n",
            "[691/ 690] loss: 0.705179, Accuracy: 0.746445\n",
            "[692/ 691] loss: 0.705234, Accuracy: 0.746427\n",
            "[693/ 692] loss: 0.705125, Accuracy: 0.746478\n",
            "[694/ 693] loss: 0.704977, Accuracy: 0.746505\n",
            "[695/ 694] loss: 0.704819, Accuracy: 0.746623\n",
            "[696/ 695] loss: 0.704632, Accuracy: 0.746673\n",
            "[697/ 696] loss: 0.704703, Accuracy: 0.746700\n",
            "[698/ 697] loss: 0.704671, Accuracy: 0.746682\n",
            "[699/ 698] loss: 0.704890, Accuracy: 0.746642\n",
            "[700/ 699] loss: 0.704984, Accuracy: 0.746625\n",
            "[701/ 700] loss: 0.704938, Accuracy: 0.746585\n",
            "[702/ 701] loss: 0.704781, Accuracy: 0.746634\n",
            "[703/ 702] loss: 0.704520, Accuracy: 0.746706\n",
            "[704/ 703] loss: 0.704598, Accuracy: 0.746666\n",
            "[705/ 704] loss: 0.705022, Accuracy: 0.746515\n",
            "[706/ 705] loss: 0.705104, Accuracy: 0.746432\n",
            "[707/ 706] loss: 0.704932, Accuracy: 0.746459\n",
            "[708/ 707] loss: 0.705423, Accuracy: 0.746265\n",
            "[709/ 708] loss: 0.705368, Accuracy: 0.746270\n",
            "[710/ 709] loss: 0.705288, Accuracy: 0.746298\n",
            "[711/ 710] loss: 0.705073, Accuracy: 0.746369\n",
            "[712/ 711] loss: 0.704957, Accuracy: 0.746374\n",
            "[713/ 712] loss: 0.705021, Accuracy: 0.746357\n",
            "[714/ 713] loss: 0.705301, Accuracy: 0.746231\n",
            "[715/ 714] loss: 0.705330, Accuracy: 0.746236\n",
            "[716/ 715] loss: 0.705559, Accuracy: 0.746132\n",
            "[717/ 716] loss: 0.705528, Accuracy: 0.746159\n",
            "[718/ 717] loss: 0.705456, Accuracy: 0.746230\n",
            "[719/ 718] loss: 0.705458, Accuracy: 0.746257\n",
            "[720/ 719] loss: 0.705314, Accuracy: 0.746306\n",
            "[721/ 720] loss: 0.705520, Accuracy: 0.746224\n",
            "[722/ 721] loss: 0.705397, Accuracy: 0.746294\n",
            "[723/ 722] loss: 0.705610, Accuracy: 0.746213\n",
            "[724/ 723] loss: 0.705751, Accuracy: 0.746153\n",
            "[725/ 724] loss: 0.705783, Accuracy: 0.746115\n",
            "[726/ 725] loss: 0.705701, Accuracy: 0.746207\n",
            "[727/ 726] loss: 0.705535, Accuracy: 0.746212\n",
            "[728/ 727] loss: 0.705584, Accuracy: 0.746196\n",
            "[729/ 728] loss: 0.705469, Accuracy: 0.746201\n",
            "[730/ 729] loss: 0.705452, Accuracy: 0.746142\n",
            "[731/ 730] loss: 0.705531, Accuracy: 0.746104\n",
            "[732/ 731] loss: 0.705645, Accuracy: 0.746024\n",
            "[733/ 732] loss: 0.705454, Accuracy: 0.746072\n",
            "[734/ 733] loss: 0.705558, Accuracy: 0.746014\n",
            "[735/ 734] loss: 0.705633, Accuracy: 0.745977\n",
            "[736/ 735] loss: 0.705696, Accuracy: 0.746046\n",
            "[737/ 736] loss: 0.705415, Accuracy: 0.746200\n",
            "[738/ 737] loss: 0.705173, Accuracy: 0.746290\n",
            "[739/ 738] loss: 0.704939, Accuracy: 0.746358\n",
            "[740/ 739] loss: 0.704820, Accuracy: 0.746385\n",
            "[741/ 740] loss: 0.704743, Accuracy: 0.746410\n",
            "[742/ 741] loss: 0.704846, Accuracy: 0.746415\n",
            "[743/ 742] loss: 0.704908, Accuracy: 0.746357\n",
            "[744/ 743] loss: 0.705244, Accuracy: 0.746299\n",
            "[745/ 744] loss: 0.705151, Accuracy: 0.746325\n",
            "[746/ 745] loss: 0.704937, Accuracy: 0.746393\n",
            "[747/ 746] loss: 0.705444, Accuracy: 0.746209\n",
            "[748/ 747] loss: 0.705424, Accuracy: 0.746214\n",
            "[749/ 748] loss: 0.705125, Accuracy: 0.746344\n",
            "[750/ 749] loss: 0.705134, Accuracy: 0.746308\n",
            "[751/ 750] loss: 0.705055, Accuracy: 0.746354\n",
            "[752/ 751] loss: 0.704890, Accuracy: 0.746421\n",
            "[753/ 752] loss: 0.705086, Accuracy: 0.746364\n",
            "[754/ 753] loss: 0.705210, Accuracy: 0.746306\n",
            "[755/ 754] loss: 0.704956, Accuracy: 0.746415\n",
            "[756/ 755] loss: 0.704904, Accuracy: 0.746420\n",
            "[757/ 756] loss: 0.704699, Accuracy: 0.746486\n",
            "[758/ 757] loss: 0.704818, Accuracy: 0.746429\n",
            "[759/ 758] loss: 0.704685, Accuracy: 0.746496\n",
            "[760/ 759] loss: 0.704537, Accuracy: 0.746521\n",
            "[761/ 760] loss: 0.704521, Accuracy: 0.746505\n",
            "[762/ 761] loss: 0.704661, Accuracy: 0.746427\n",
            "[763/ 762] loss: 0.704975, Accuracy: 0.746309\n",
            "[764/ 763] loss: 0.705095, Accuracy: 0.746232\n",
            "[765/ 764] loss: 0.705369, Accuracy: 0.746155\n",
            "[766/ 765] loss: 0.705319, Accuracy: 0.746160\n",
            "[767/ 766] loss: 0.705278, Accuracy: 0.746165\n",
            "[768/ 767] loss: 0.705422, Accuracy: 0.746109\n",
            "[769/ 768] loss: 0.705298, Accuracy: 0.746155\n",
            "[770/ 769] loss: 0.705353, Accuracy: 0.746119\n",
            "[771/ 770] loss: 0.705372, Accuracy: 0.746124\n",
            "[772/ 771] loss: 0.705331, Accuracy: 0.746170\n",
            "[773/ 772] loss: 0.705366, Accuracy: 0.746175\n",
            "[774/ 773] loss: 0.705500, Accuracy: 0.746099\n",
            "[775/ 774] loss: 0.705594, Accuracy: 0.746003\n",
            "[776/ 775] loss: 0.705837, Accuracy: 0.745867\n",
            "[777/ 776] loss: 0.705865, Accuracy: 0.745832\n",
            "[778/ 777] loss: 0.706073, Accuracy: 0.745797\n",
            "[779/ 778] loss: 0.705986, Accuracy: 0.745803\n",
            "[780/ 779] loss: 0.705954, Accuracy: 0.745808\n",
            "[781/ 780] loss: 0.706008, Accuracy: 0.745793\n",
            "[782/ 781] loss: 0.705427, Accuracy: 0.745979\n",
            "[783/ 782] loss: 0.705365, Accuracy: 0.745984\n",
            "[784/ 783] loss: 0.705229, Accuracy: 0.746029\n",
            "[785/ 784] loss: 0.705089, Accuracy: 0.746154\n",
            "[786/ 785] loss: 0.705281, Accuracy: 0.746099\n",
            "[787/ 786] loss: 0.705342, Accuracy: 0.746044\n",
            "[788/ 787] loss: 0.705366, Accuracy: 0.746089\n",
            "[789/ 788] loss: 0.705278, Accuracy: 0.746114\n",
            "[790/ 789] loss: 0.704998, Accuracy: 0.746218\n",
            "[791/ 790] loss: 0.704902, Accuracy: 0.746262\n",
            "[792/ 791] loss: 0.704855, Accuracy: 0.746247\n",
            "[793/ 792] loss: 0.704901, Accuracy: 0.746212\n",
            "[794/ 793] loss: 0.705138, Accuracy: 0.746118\n",
            "[795/ 794] loss: 0.704768, Accuracy: 0.746261\n",
            "[796/ 795] loss: 0.704874, Accuracy: 0.746266\n",
            "[797/ 796] loss: 0.704581, Accuracy: 0.746369\n",
            "[798/ 797] loss: 0.704603, Accuracy: 0.746373\n",
            "[799/ 798] loss: 0.704566, Accuracy: 0.746378\n",
            "[800/ 799] loss: 0.704739, Accuracy: 0.746284\n",
            "[801/ 800] loss: 0.704757, Accuracy: 0.746289\n",
            "[802/ 801] loss: 0.704884, Accuracy: 0.746235\n",
            "[803/ 802] loss: 0.704784, Accuracy: 0.746240\n",
            "[804/ 803] loss: 0.704868, Accuracy: 0.746186\n",
            "[805/ 804] loss: 0.704895, Accuracy: 0.746191\n",
            "[806/ 805] loss: 0.705080, Accuracy: 0.746118\n",
            "[807/ 806] loss: 0.705104, Accuracy: 0.746123\n",
            "[808/ 807] loss: 0.704990, Accuracy: 0.746166\n",
            "[809/ 808] loss: 0.704987, Accuracy: 0.746152\n",
            "[810/ 809] loss: 0.705156, Accuracy: 0.746118\n",
            "[811/ 810] loss: 0.705110, Accuracy: 0.746142\n",
            "[812/ 811] loss: 0.705325, Accuracy: 0.746050\n",
            "[813/ 812] loss: 0.705225, Accuracy: 0.746094\n",
            "[814/ 813] loss: 0.705072, Accuracy: 0.746118\n",
            "[815/ 814] loss: 0.705144, Accuracy: 0.746103\n",
            "[816/ 815] loss: 0.705164, Accuracy: 0.746089\n",
            "[817/ 816] loss: 0.705340, Accuracy: 0.746017\n",
            "[818/ 817] loss: 0.705216, Accuracy: 0.746118\n",
            "[819/ 818] loss: 0.705369, Accuracy: 0.746084\n",
            "[820/ 819] loss: 0.705232, Accuracy: 0.746127\n",
            "[821/ 820] loss: 0.705483, Accuracy: 0.746037\n",
            "[822/ 821] loss: 0.705338, Accuracy: 0.746080\n",
            "[823/ 822] loss: 0.705187, Accuracy: 0.746141\n",
            "[824/ 823] loss: 0.704918, Accuracy: 0.746241\n",
            "[825/ 824] loss: 0.704845, Accuracy: 0.746302\n",
            "[826/ 825] loss: 0.704905, Accuracy: 0.746307\n",
            "[827/ 826] loss: 0.704910, Accuracy: 0.746273\n",
            "[828/ 827] loss: 0.705246, Accuracy: 0.746184\n",
            "[829/ 828] loss: 0.705230, Accuracy: 0.746226\n",
            "[830/ 829] loss: 0.705355, Accuracy: 0.746212\n",
            "[831/ 830] loss: 0.705238, Accuracy: 0.746254\n",
            "[832/ 831] loss: 0.705549, Accuracy: 0.746183\n",
            "[833/ 832] loss: 0.705609, Accuracy: 0.746169\n",
            "[834/ 833] loss: 0.705683, Accuracy: 0.746211\n",
            "[835/ 834] loss: 0.705814, Accuracy: 0.746216\n",
            "[836/ 835] loss: 0.705507, Accuracy: 0.746314\n",
            "[837/ 836] loss: 0.705183, Accuracy: 0.746430\n",
            "[838/ 837] loss: 0.705269, Accuracy: 0.746360\n",
            "[839/ 838] loss: 0.705076, Accuracy: 0.746439\n",
            "[840/ 839] loss: 0.705273, Accuracy: 0.746331\n",
            "[841/ 840] loss: 0.705235, Accuracy: 0.746336\n",
            "[842/ 841] loss: 0.705142, Accuracy: 0.746377\n",
            "[843/ 842] loss: 0.705113, Accuracy: 0.746418\n",
            "[844/ 843] loss: 0.705114, Accuracy: 0.746386\n",
            "[845/ 844] loss: 0.705063, Accuracy: 0.746353\n",
            "[846/ 845] loss: 0.705179, Accuracy: 0.746339\n",
            "[847/ 846] loss: 0.705410, Accuracy: 0.746269\n",
            "[848/ 847] loss: 0.705447, Accuracy: 0.746237\n",
            "[849/ 848] loss: 0.705393, Accuracy: 0.746260\n",
            "[850/ 849] loss: 0.705713, Accuracy: 0.746098\n",
            "[851/ 850] loss: 0.705584, Accuracy: 0.746140\n",
            "[852/ 851] loss: 0.705635, Accuracy: 0.746126\n",
            "[853/ 852] loss: 0.705643, Accuracy: 0.746112\n",
            "[854/ 853] loss: 0.705539, Accuracy: 0.746153\n",
            "[855/ 854] loss: 0.705323, Accuracy: 0.746268\n",
            "[856/ 855] loss: 0.705367, Accuracy: 0.746235\n",
            "[857/ 856] loss: 0.705400, Accuracy: 0.746258\n",
            "[858/ 857] loss: 0.705226, Accuracy: 0.746335\n",
            "[859/ 858] loss: 0.705000, Accuracy: 0.746412\n",
            "[860/ 859] loss: 0.705005, Accuracy: 0.746417\n",
            "[861/ 860] loss: 0.704786, Accuracy: 0.746475\n",
            "[862/ 861] loss: 0.704608, Accuracy: 0.746534\n",
            "[863/ 862] loss: 0.704762, Accuracy: 0.746447\n",
            "[864/ 863] loss: 0.704587, Accuracy: 0.746488\n",
            "[865/ 864] loss: 0.704530, Accuracy: 0.746528\n",
            "[866/ 865] loss: 0.704835, Accuracy: 0.746441\n",
            "[867/ 866] loss: 0.704746, Accuracy: 0.746500\n",
            "[868/ 867] loss: 0.704650, Accuracy: 0.746504\n",
            "[869/ 868] loss: 0.704695, Accuracy: 0.746472\n",
            "[870/ 869] loss: 0.704477, Accuracy: 0.746548\n",
            "[871/ 870] loss: 0.704367, Accuracy: 0.746588\n",
            "[872/ 871] loss: 0.704455, Accuracy: 0.746520\n",
            "[873/ 872] loss: 0.704380, Accuracy: 0.746542\n",
            "[874/ 873] loss: 0.704436, Accuracy: 0.746528\n",
            "[875/ 874] loss: 0.704367, Accuracy: 0.746514\n",
            "[876/ 875] loss: 0.704477, Accuracy: 0.746411\n",
            "[877/ 876] loss: 0.704242, Accuracy: 0.746504\n",
            "[878/ 877] loss: 0.704149, Accuracy: 0.746508\n",
            "[879/ 878] loss: 0.704066, Accuracy: 0.746530\n",
            "[880/ 879] loss: 0.704165, Accuracy: 0.746498\n",
            "[881/ 880] loss: 0.703983, Accuracy: 0.746555\n",
            "[882/ 881] loss: 0.703957, Accuracy: 0.746559\n",
            "[883/ 882] loss: 0.704047, Accuracy: 0.746492\n",
            "[884/ 883] loss: 0.704368, Accuracy: 0.746355\n",
            "[885/ 884] loss: 0.704447, Accuracy: 0.746306\n",
            "[886/ 885] loss: 0.704483, Accuracy: 0.746275\n",
            "[887/ 886] loss: 0.704246, Accuracy: 0.746367\n",
            "[888/ 887] loss: 0.704029, Accuracy: 0.746459\n",
            "[889/ 888] loss: 0.703966, Accuracy: 0.746463\n",
            "[890/ 889] loss: 0.703990, Accuracy: 0.746485\n",
            "[891/ 890] loss: 0.704066, Accuracy: 0.746401\n",
            "[892/ 891] loss: 0.704022, Accuracy: 0.746423\n",
            "[893/ 892] loss: 0.704058, Accuracy: 0.746392\n",
            "[894/ 893] loss: 0.704275, Accuracy: 0.746308\n",
            "[895/ 894] loss: 0.704104, Accuracy: 0.746400\n",
            "[896/ 895] loss: 0.704098, Accuracy: 0.746421\n",
            "[897/ 896] loss: 0.704012, Accuracy: 0.746408\n",
            "[898/ 897] loss: 0.703825, Accuracy: 0.746499\n",
            "[899/ 898] loss: 0.703607, Accuracy: 0.746590\n",
            "[900/ 899] loss: 0.703480, Accuracy: 0.746611\n",
            "[901 / 900]  Loss:0.703603, Accuracy:0.746562\n",
            "[901/ 900] loss: 0.703603, Accuracy: 0.746562\n",
            "[902/ 901] loss: 0.703498, Accuracy: 0.746601\n",
            "[903/ 902] loss: 0.703499, Accuracy: 0.746605\n",
            "[904/ 903] loss: 0.703525, Accuracy: 0.746626\n",
            "[905/ 904] loss: 0.703422, Accuracy: 0.746647\n",
            "[906/ 905] loss: 0.703657, Accuracy: 0.746547\n",
            "[907/ 906] loss: 0.703539, Accuracy: 0.746603\n",
            "[908/ 907] loss: 0.703325, Accuracy: 0.746692\n",
            "[909/ 908] loss: 0.703387, Accuracy: 0.746696\n",
            "[910/ 909] loss: 0.703184, Accuracy: 0.746803\n",
            "[911/ 910] loss: 0.703280, Accuracy: 0.746789\n",
            "[912/ 911] loss: 0.703265, Accuracy: 0.746793\n",
            "[913/ 912] loss: 0.703273, Accuracy: 0.746762\n",
            "[914/ 913] loss: 0.703530, Accuracy: 0.746731\n",
            "[915/ 914] loss: 0.703790, Accuracy: 0.746701\n",
            "[916/ 915] loss: 0.703835, Accuracy: 0.746687\n",
            "[917/ 916] loss: 0.703708, Accuracy: 0.746708\n",
            "[918/ 917] loss: 0.703811, Accuracy: 0.746711\n",
            "[919/ 918] loss: 0.703646, Accuracy: 0.746766\n",
            "[920/ 919] loss: 0.703384, Accuracy: 0.746855\n",
            "[921/ 920] loss: 0.703251, Accuracy: 0.746926\n",
            "[922/ 921] loss: 0.703137, Accuracy: 0.746980\n",
            "[923/ 922] loss: 0.703094, Accuracy: 0.747017\n",
            "[924/ 923] loss: 0.703028, Accuracy: 0.747071\n",
            "[925/ 924] loss: 0.703194, Accuracy: 0.747058\n",
            "[926/ 925] loss: 0.703290, Accuracy: 0.747010\n",
            "[927/ 926] loss: 0.703383, Accuracy: 0.747013\n",
            "[928/ 927] loss: 0.703352, Accuracy: 0.747033\n",
            "[929/ 928] loss: 0.703441, Accuracy: 0.747020\n",
            "[930/ 929] loss: 0.703632, Accuracy: 0.746989\n",
            "[931/ 930] loss: 0.703672, Accuracy: 0.746959\n",
            "[932/ 931] loss: 0.703467, Accuracy: 0.747046\n",
            "[933/ 932] loss: 0.703428, Accuracy: 0.747049\n",
            "[934/ 933] loss: 0.703475, Accuracy: 0.746986\n",
            "[935/ 934] loss: 0.703545, Accuracy: 0.746905\n",
            "[936/ 935] loss: 0.703617, Accuracy: 0.746925\n",
            "[937/ 936] loss: 0.703488, Accuracy: 0.746962\n",
            "[938/ 937] loss: 0.703511, Accuracy: 0.746948\n",
            "[939/ 938] loss: 0.703860, Accuracy: 0.746852\n",
            "epochs 8\n",
            "[2/ 1] loss: 0.621452, Accuracy: 0.734375\n",
            "[3/ 2] loss: 0.738478, Accuracy: 0.710938\n",
            "[4/ 3] loss: 0.695164, Accuracy: 0.739583\n",
            "[5/ 4] loss: 0.710337, Accuracy: 0.742188\n",
            "[6/ 5] loss: 0.725273, Accuracy: 0.740625\n",
            "[7/ 6] loss: 0.731836, Accuracy: 0.734375\n",
            "[8/ 7] loss: 0.683290, Accuracy: 0.752232\n",
            "[9/ 8] loss: 0.722116, Accuracy: 0.740234\n",
            "[10/ 9] loss: 0.732452, Accuracy: 0.736111\n",
            "[11/ 10] loss: 0.686523, Accuracy: 0.753125\n",
            "[12/ 11] loss: 0.703938, Accuracy: 0.745739\n",
            "[13/ 12] loss: 0.695703, Accuracy: 0.748698\n",
            "[14/ 13] loss: 0.694463, Accuracy: 0.748798\n",
            "[15/ 14] loss: 0.699930, Accuracy: 0.747768\n",
            "[16/ 15] loss: 0.696851, Accuracy: 0.746875\n",
            "[17/ 16] loss: 0.692906, Accuracy: 0.745117\n",
            "[18/ 17] loss: 0.685234, Accuracy: 0.750000\n",
            "[19/ 18] loss: 0.672182, Accuracy: 0.754340\n",
            "[20/ 19] loss: 0.674399, Accuracy: 0.753289\n",
            "[21/ 20] loss: 0.667880, Accuracy: 0.753906\n",
            "[22/ 21] loss: 0.677013, Accuracy: 0.750000\n",
            "[23/ 22] loss: 0.679555, Accuracy: 0.749290\n",
            "[24/ 23] loss: 0.670274, Accuracy: 0.752717\n",
            "[25/ 24] loss: 0.664979, Accuracy: 0.755859\n",
            "[26/ 25] loss: 0.655864, Accuracy: 0.759375\n",
            "[27/ 26] loss: 0.661915, Accuracy: 0.757212\n",
            "[28/ 27] loss: 0.654652, Accuracy: 0.759838\n",
            "[29/ 28] loss: 0.656189, Accuracy: 0.759487\n",
            "[30/ 29] loss: 0.657749, Accuracy: 0.760237\n",
            "[31/ 30] loss: 0.656201, Accuracy: 0.760938\n",
            "[32/ 31] loss: 0.655164, Accuracy: 0.761593\n",
            "[33/ 32] loss: 0.658165, Accuracy: 0.760254\n",
            "[34/ 33] loss: 0.660457, Accuracy: 0.759943\n",
            "[35/ 34] loss: 0.661640, Accuracy: 0.758732\n",
            "[36/ 35] loss: 0.670962, Accuracy: 0.755357\n",
            "[37/ 36] loss: 0.673171, Accuracy: 0.755642\n",
            "[38/ 37] loss: 0.678880, Accuracy: 0.752956\n",
            "[39/ 38] loss: 0.673025, Accuracy: 0.754934\n",
            "[40/ 39] loss: 0.667347, Accuracy: 0.757212\n",
            "[41/ 40] loss: 0.668974, Accuracy: 0.756250\n",
            "[42/ 41] loss: 0.670217, Accuracy: 0.754573\n",
            "[43/ 42] loss: 0.673698, Accuracy: 0.753348\n",
            "[44/ 43] loss: 0.670126, Accuracy: 0.754360\n",
            "[45/ 44] loss: 0.674019, Accuracy: 0.753196\n",
            "[46/ 45] loss: 0.677253, Accuracy: 0.752778\n",
            "[47/ 46] loss: 0.679819, Accuracy: 0.751019\n",
            "[48/ 47] loss: 0.677293, Accuracy: 0.751662\n",
            "[49/ 48] loss: 0.677549, Accuracy: 0.751953\n",
            "[50/ 49] loss: 0.676495, Accuracy: 0.752551\n",
            "[51/ 50] loss: 0.674904, Accuracy: 0.753125\n",
            "[52/ 51] loss: 0.674009, Accuracy: 0.753370\n",
            "[53/ 52] loss: 0.675806, Accuracy: 0.752704\n",
            "[54/ 53] loss: 0.675391, Accuracy: 0.752948\n",
            "[55/ 54] loss: 0.677989, Accuracy: 0.752025\n",
            "[56/ 55] loss: 0.680789, Accuracy: 0.750568\n",
            "[57/ 56] loss: 0.679891, Accuracy: 0.750279\n",
            "[58/ 57] loss: 0.683465, Accuracy: 0.749178\n",
            "[59/ 58] loss: 0.684988, Accuracy: 0.747845\n",
            "[60/ 59] loss: 0.685602, Accuracy: 0.748146\n",
            "[61/ 60] loss: 0.684204, Accuracy: 0.749479\n",
            "[62/ 61] loss: 0.684691, Accuracy: 0.749488\n",
            "[63/ 62] loss: 0.685329, Accuracy: 0.749496\n",
            "[64/ 63] loss: 0.686596, Accuracy: 0.749256\n",
            "[65/ 64] loss: 0.683847, Accuracy: 0.750000\n",
            "[66/ 65] loss: 0.685367, Accuracy: 0.749760\n",
            "[67/ 66] loss: 0.684398, Accuracy: 0.749763\n",
            "[68/ 67] loss: 0.685602, Accuracy: 0.748834\n",
            "[69/ 68] loss: 0.684672, Accuracy: 0.749081\n",
            "[70/ 69] loss: 0.684555, Accuracy: 0.748868\n",
            "[71/ 70] loss: 0.685226, Accuracy: 0.748661\n",
            "[72/ 71] loss: 0.683183, Accuracy: 0.750000\n",
            "[73/ 72] loss: 0.683470, Accuracy: 0.749783\n",
            "[74/ 73] loss: 0.684905, Accuracy: 0.749358\n",
            "[75/ 74] loss: 0.682414, Accuracy: 0.750422\n",
            "[76/ 75] loss: 0.681685, Accuracy: 0.750417\n",
            "[77/ 76] loss: 0.682287, Accuracy: 0.750000\n",
            "[78/ 77] loss: 0.685091, Accuracy: 0.749188\n",
            "[79/ 78] loss: 0.690517, Accuracy: 0.746995\n",
            "[80/ 79] loss: 0.692804, Accuracy: 0.746242\n",
            "[81/ 80] loss: 0.691915, Accuracy: 0.746484\n",
            "[82/ 81] loss: 0.691547, Accuracy: 0.747492\n",
            "[83/ 82] loss: 0.693821, Accuracy: 0.746761\n",
            "[84/ 83] loss: 0.692350, Accuracy: 0.747553\n",
            "[85/ 84] loss: 0.690789, Accuracy: 0.747954\n",
            "[86/ 85] loss: 0.691676, Accuracy: 0.747794\n",
            "[87/ 86] loss: 0.690790, Accuracy: 0.748183\n",
            "[88/ 87] loss: 0.691228, Accuracy: 0.748024\n",
            "[89/ 88] loss: 0.689599, Accuracy: 0.748580\n",
            "[90/ 89] loss: 0.691137, Accuracy: 0.748595\n",
            "[91/ 90] loss: 0.694978, Accuracy: 0.746701\n",
            "[92/ 91] loss: 0.697649, Accuracy: 0.745707\n",
            "[93/ 92] loss: 0.695261, Accuracy: 0.746773\n",
            "[94/ 93] loss: 0.693300, Accuracy: 0.747648\n",
            "[95/ 94] loss: 0.692380, Accuracy: 0.748172\n",
            "[96/ 95] loss: 0.696445, Accuracy: 0.747204\n",
            "[97/ 96] loss: 0.695151, Accuracy: 0.747396\n",
            "[98/ 97] loss: 0.694438, Accuracy: 0.747745\n",
            "[99/ 98] loss: 0.692852, Accuracy: 0.748406\n",
            "[100/ 99] loss: 0.691314, Accuracy: 0.748737\n",
            "[101/ 100] loss: 0.688920, Accuracy: 0.749687\n",
            "[102/ 101] loss: 0.686278, Accuracy: 0.750773\n",
            "[103/ 102] loss: 0.685740, Accuracy: 0.751072\n",
            "[104/ 103] loss: 0.686067, Accuracy: 0.751062\n",
            "[105/ 104] loss: 0.685985, Accuracy: 0.750901\n",
            "[106/ 105] loss: 0.684011, Accuracy: 0.751488\n",
            "[107/ 106] loss: 0.684850, Accuracy: 0.751474\n",
            "[108/ 107] loss: 0.686018, Accuracy: 0.751022\n",
            "[109/ 108] loss: 0.687116, Accuracy: 0.750289\n",
            "[110/ 109] loss: 0.686856, Accuracy: 0.750573\n",
            "[111/ 110] loss: 0.687570, Accuracy: 0.750142\n",
            "[112/ 111] loss: 0.688752, Accuracy: 0.749718\n",
            "[113/ 112] loss: 0.688575, Accuracy: 0.749721\n",
            "[114/ 113] loss: 0.690992, Accuracy: 0.748617\n",
            "[115/ 114] loss: 0.690936, Accuracy: 0.748492\n",
            "[116/ 115] loss: 0.691954, Accuracy: 0.747826\n",
            "[117/ 116] loss: 0.690311, Accuracy: 0.748249\n",
            "[118/ 117] loss: 0.689949, Accuracy: 0.748531\n",
            "[119/ 118] loss: 0.691093, Accuracy: 0.748014\n",
            "[120/ 119] loss: 0.690954, Accuracy: 0.748030\n",
            "[121/ 120] loss: 0.689592, Accuracy: 0.748568\n",
            "[122/ 121] loss: 0.689389, Accuracy: 0.748709\n",
            "[123/ 122] loss: 0.688823, Accuracy: 0.749103\n",
            "[124/ 123] loss: 0.690063, Accuracy: 0.748730\n",
            "[125/ 124] loss: 0.690856, Accuracy: 0.748236\n",
            "[126/ 125] loss: 0.689435, Accuracy: 0.749000\n",
            "[127/ 126] loss: 0.687451, Accuracy: 0.749752\n",
            "[128/ 127] loss: 0.688008, Accuracy: 0.749754\n",
            "[129/ 128] loss: 0.689568, Accuracy: 0.749146\n",
            "[130/ 129] loss: 0.690881, Accuracy: 0.748668\n",
            "[131/ 130] loss: 0.689297, Accuracy: 0.749279\n",
            "[132/ 131] loss: 0.690954, Accuracy: 0.748807\n",
            "[133/ 132] loss: 0.692409, Accuracy: 0.748461\n",
            "[134/ 133] loss: 0.692108, Accuracy: 0.748590\n",
            "[135/ 134] loss: 0.691226, Accuracy: 0.748951\n",
            "[136/ 135] loss: 0.692396, Accuracy: 0.748032\n",
            "[137/ 136] loss: 0.694899, Accuracy: 0.747128\n",
            "[138/ 137] loss: 0.694332, Accuracy: 0.747377\n",
            "[139/ 138] loss: 0.693702, Accuracy: 0.747736\n",
            "[140/ 139] loss: 0.693320, Accuracy: 0.747864\n",
            "[141/ 140] loss: 0.693758, Accuracy: 0.747433\n",
            "[142/ 141] loss: 0.694046, Accuracy: 0.747340\n",
            "[143/ 142] loss: 0.693736, Accuracy: 0.747359\n",
            "[144/ 143] loss: 0.695034, Accuracy: 0.746722\n",
            "[145/ 144] loss: 0.695953, Accuracy: 0.746311\n",
            "[146/ 145] loss: 0.695970, Accuracy: 0.746121\n",
            "[147/ 146] loss: 0.696011, Accuracy: 0.745933\n",
            "[148/ 147] loss: 0.696988, Accuracy: 0.745536\n",
            "[149/ 148] loss: 0.697543, Accuracy: 0.745144\n",
            "[150/ 149] loss: 0.700383, Accuracy: 0.744547\n",
            "[151/ 150] loss: 0.700623, Accuracy: 0.744479\n",
            "[152/ 151] loss: 0.701736, Accuracy: 0.744309\n",
            "[153/ 152] loss: 0.702059, Accuracy: 0.744141\n",
            "[154/ 153] loss: 0.700387, Accuracy: 0.744587\n",
            "[155/ 154] loss: 0.701224, Accuracy: 0.744217\n",
            "[156/ 155] loss: 0.701066, Accuracy: 0.744355\n",
            "[157/ 156] loss: 0.701275, Accuracy: 0.744391\n",
            "[158/ 157] loss: 0.701029, Accuracy: 0.744327\n",
            "[159/ 158] loss: 0.701098, Accuracy: 0.744264\n",
            "[160/ 159] loss: 0.700744, Accuracy: 0.744399\n",
            "[161/ 160] loss: 0.701597, Accuracy: 0.744238\n",
            "[162/ 161] loss: 0.702534, Accuracy: 0.743886\n",
            "[163/ 162] loss: 0.702300, Accuracy: 0.744020\n",
            "[164/ 163] loss: 0.702178, Accuracy: 0.744248\n",
            "[165/ 164] loss: 0.702292, Accuracy: 0.744569\n",
            "[166/ 165] loss: 0.701437, Accuracy: 0.744886\n",
            "[167/ 166] loss: 0.702120, Accuracy: 0.744823\n",
            "[168/ 167] loss: 0.702111, Accuracy: 0.744854\n",
            "[169/ 168] loss: 0.701839, Accuracy: 0.744885\n",
            "[170/ 169] loss: 0.701055, Accuracy: 0.745100\n",
            "[171/ 170] loss: 0.700564, Accuracy: 0.745313\n",
            "[172/ 171] loss: 0.701224, Accuracy: 0.744883\n",
            "[173/ 172] loss: 0.701994, Accuracy: 0.744459\n",
            "[174/ 173] loss: 0.701733, Accuracy: 0.744400\n",
            "[175/ 174] loss: 0.701449, Accuracy: 0.744612\n",
            "[176/ 175] loss: 0.702059, Accuracy: 0.744554\n",
            "[177/ 176] loss: 0.701123, Accuracy: 0.744851\n",
            "[178/ 177] loss: 0.701567, Accuracy: 0.744527\n",
            "[179/ 178] loss: 0.703585, Accuracy: 0.743680\n",
            "[180/ 179] loss: 0.703785, Accuracy: 0.743628\n",
            "[181/ 180] loss: 0.704754, Accuracy: 0.743316\n",
            "[182/ 181] loss: 0.705069, Accuracy: 0.743180\n",
            "[183/ 182] loss: 0.705485, Accuracy: 0.743132\n",
            "[184/ 183] loss: 0.704620, Accuracy: 0.743596\n",
            "[185/ 184] loss: 0.704864, Accuracy: 0.743461\n",
            "[186/ 185] loss: 0.705679, Accuracy: 0.743074\n",
            "[187/ 186] loss: 0.706788, Accuracy: 0.742608\n",
            "[188/ 187] loss: 0.706211, Accuracy: 0.742731\n",
            "[189/ 188] loss: 0.706585, Accuracy: 0.742603\n",
            "[190/ 189] loss: 0.706221, Accuracy: 0.742642\n",
            "[191/ 190] loss: 0.706420, Accuracy: 0.742845\n",
            "[192/ 191] loss: 0.706589, Accuracy: 0.742719\n",
            "[193/ 192] loss: 0.705807, Accuracy: 0.743001\n",
            "[194/ 193] loss: 0.705368, Accuracy: 0.743280\n",
            "[195/ 194] loss: 0.704656, Accuracy: 0.743637\n",
            "[196/ 195] loss: 0.704761, Accuracy: 0.743830\n",
            "[197/ 196] loss: 0.704843, Accuracy: 0.743862\n",
            "[198/ 197] loss: 0.703592, Accuracy: 0.744210\n",
            "[199/ 198] loss: 0.705235, Accuracy: 0.744081\n",
            "[200/ 199] loss: 0.705374, Accuracy: 0.744033\n",
            "[201/ 200] loss: 0.703787, Accuracy: 0.744687\n",
            "[202/ 201] loss: 0.705195, Accuracy: 0.744325\n",
            "[203/ 202] loss: 0.705694, Accuracy: 0.744044\n",
            "[204/ 203] loss: 0.705130, Accuracy: 0.744381\n",
            "[205/ 204] loss: 0.706357, Accuracy: 0.743949\n",
            "[206/ 205] loss: 0.706219, Accuracy: 0.744055\n",
            "[207/ 206] loss: 0.705790, Accuracy: 0.744311\n",
            "[208/ 207] loss: 0.705875, Accuracy: 0.744263\n",
            "[209/ 208] loss: 0.704901, Accuracy: 0.744666\n",
            "[210/ 209] loss: 0.703943, Accuracy: 0.745365\n",
            "[211/ 210] loss: 0.702813, Accuracy: 0.745833\n",
            "[212/ 211] loss: 0.703022, Accuracy: 0.745705\n",
            "[213/ 212] loss: 0.701727, Accuracy: 0.746094\n",
            "[214/ 213] loss: 0.702066, Accuracy: 0.745965\n",
            "[215/ 214] loss: 0.701905, Accuracy: 0.745984\n",
            "[216/ 215] loss: 0.700739, Accuracy: 0.746439\n",
            "[217/ 216] loss: 0.700046, Accuracy: 0.746745\n",
            "[218/ 217] loss: 0.701385, Accuracy: 0.746112\n",
            "[219/ 218] loss: 0.701405, Accuracy: 0.746130\n",
            "[220/ 219] loss: 0.701140, Accuracy: 0.746219\n",
            "[221/ 220] loss: 0.701967, Accuracy: 0.745810\n",
            "[222/ 221] loss: 0.702736, Accuracy: 0.745758\n",
            "[223/ 222] loss: 0.702587, Accuracy: 0.745918\n",
            "[224/ 223] loss: 0.702093, Accuracy: 0.746006\n",
            "[225/ 224] loss: 0.701506, Accuracy: 0.746303\n",
            "[226/ 225] loss: 0.701933, Accuracy: 0.746111\n",
            "[227/ 226] loss: 0.702060, Accuracy: 0.746128\n",
            "[228/ 227] loss: 0.701913, Accuracy: 0.746214\n",
            "[229/ 228] loss: 0.702403, Accuracy: 0.746025\n",
            "[230/ 229] loss: 0.702163, Accuracy: 0.746179\n",
            "[231/ 230] loss: 0.702992, Accuracy: 0.745788\n",
            "[232/ 231] loss: 0.703105, Accuracy: 0.745739\n",
            "[233/ 232] loss: 0.701932, Accuracy: 0.746228\n",
            "[234/ 233] loss: 0.701159, Accuracy: 0.746647\n",
            "[235/ 234] loss: 0.700851, Accuracy: 0.746795\n",
            "[236/ 235] loss: 0.699775, Accuracy: 0.747207\n",
            "[237/ 236] loss: 0.699257, Accuracy: 0.747418\n",
            "[238/ 237] loss: 0.699245, Accuracy: 0.747429\n",
            "[239/ 238] loss: 0.700373, Accuracy: 0.747177\n",
            "[240/ 239] loss: 0.701058, Accuracy: 0.747058\n",
            "[241/ 240] loss: 0.701699, Accuracy: 0.746745\n",
            "[242/ 241] loss: 0.701301, Accuracy: 0.746888\n",
            "[243/ 242] loss: 0.701614, Accuracy: 0.746772\n",
            "[244/ 243] loss: 0.701891, Accuracy: 0.746721\n",
            "[245/ 244] loss: 0.701170, Accuracy: 0.746862\n",
            "[246/ 245] loss: 0.700557, Accuracy: 0.747194\n",
            "[247/ 246] loss: 0.699394, Accuracy: 0.747650\n",
            "[248/ 247] loss: 0.698892, Accuracy: 0.747786\n",
            "[249/ 248] loss: 0.697617, Accuracy: 0.748173\n",
            "[250/ 249] loss: 0.697534, Accuracy: 0.748117\n",
            "[251/ 250] loss: 0.698363, Accuracy: 0.747750\n",
            "[252/ 251] loss: 0.698262, Accuracy: 0.747634\n",
            "[253/ 252] loss: 0.698029, Accuracy: 0.747644\n",
            "[254/ 253] loss: 0.698422, Accuracy: 0.747591\n",
            "[255/ 254] loss: 0.698579, Accuracy: 0.747416\n",
            "[256/ 255] loss: 0.698513, Accuracy: 0.747610\n",
            "[257/ 256] loss: 0.698529, Accuracy: 0.747681\n",
            "[258/ 257] loss: 0.697916, Accuracy: 0.747872\n",
            "[259/ 258] loss: 0.698026, Accuracy: 0.747699\n",
            "[260/ 259] loss: 0.697449, Accuracy: 0.747889\n",
            "[261/ 260] loss: 0.697615, Accuracy: 0.747837\n",
            "[262/ 261] loss: 0.697358, Accuracy: 0.748084\n",
            "[263/ 262] loss: 0.696524, Accuracy: 0.748390\n",
            "[264/ 263] loss: 0.695537, Accuracy: 0.748693\n",
            "[265/ 264] loss: 0.695280, Accuracy: 0.748639\n",
            "[266/ 265] loss: 0.695255, Accuracy: 0.748644\n",
            "[267/ 266] loss: 0.695282, Accuracy: 0.748708\n",
            "[268/ 267] loss: 0.695469, Accuracy: 0.748654\n",
            "[269/ 268] loss: 0.695981, Accuracy: 0.748309\n",
            "[270/ 269] loss: 0.696704, Accuracy: 0.748141\n",
            "[271/ 270] loss: 0.696787, Accuracy: 0.748148\n",
            "[272/ 271] loss: 0.696528, Accuracy: 0.748328\n",
            "[273/ 272] loss: 0.695516, Accuracy: 0.748736\n",
            "[274/ 273] loss: 0.694990, Accuracy: 0.748970\n",
            "[275/ 274] loss: 0.694878, Accuracy: 0.749088\n",
            "[276/ 275] loss: 0.693635, Accuracy: 0.749602\n",
            "[277/ 276] loss: 0.693406, Accuracy: 0.749774\n",
            "[278/ 277] loss: 0.693673, Accuracy: 0.749774\n",
            "[279/ 278] loss: 0.693489, Accuracy: 0.749888\n",
            "[280/ 279] loss: 0.693109, Accuracy: 0.750056\n",
            "[281/ 280] loss: 0.693196, Accuracy: 0.750056\n",
            "[282/ 281] loss: 0.693124, Accuracy: 0.750111\n",
            "[283/ 282] loss: 0.693340, Accuracy: 0.750166\n",
            "[284/ 283] loss: 0.693779, Accuracy: 0.750055\n",
            "[285/ 284] loss: 0.693425, Accuracy: 0.750110\n",
            "[286/ 285] loss: 0.693183, Accuracy: 0.750329\n",
            "[287/ 286] loss: 0.692214, Accuracy: 0.750710\n",
            "[288/ 287] loss: 0.691842, Accuracy: 0.750926\n",
            "[289/ 288] loss: 0.691678, Accuracy: 0.750922\n",
            "[290/ 289] loss: 0.691564, Accuracy: 0.750973\n",
            "[291/ 290] loss: 0.691992, Accuracy: 0.750700\n",
            "[292/ 291] loss: 0.692230, Accuracy: 0.750698\n",
            "[293/ 292] loss: 0.691717, Accuracy: 0.750803\n",
            "[294/ 293] loss: 0.691346, Accuracy: 0.750960\n",
            "[295/ 294] loss: 0.691354, Accuracy: 0.750957\n",
            "[296/ 295] loss: 0.691355, Accuracy: 0.750900\n",
            "[297/ 296] loss: 0.691984, Accuracy: 0.750686\n",
            "[298/ 297] loss: 0.691952, Accuracy: 0.751000\n",
            "[299/ 298] loss: 0.691000, Accuracy: 0.751311\n",
            "[300/ 299] loss: 0.690642, Accuracy: 0.751306\n",
            "[301 / 300]  Loss:0.690475, Accuracy:0.751354\n",
            "[301/ 300] loss: 0.690475, Accuracy: 0.751354\n",
            "[302/ 301] loss: 0.690854, Accuracy: 0.751090\n",
            "[303/ 302] loss: 0.690709, Accuracy: 0.751138\n",
            "[304/ 303] loss: 0.690357, Accuracy: 0.751238\n",
            "[305/ 304] loss: 0.690228, Accuracy: 0.751285\n",
            "[306/ 305] loss: 0.690519, Accuracy: 0.751127\n",
            "[307/ 306] loss: 0.690340, Accuracy: 0.751328\n",
            "[308/ 307] loss: 0.690043, Accuracy: 0.751425\n",
            "[309/ 308] loss: 0.691202, Accuracy: 0.750964\n",
            "[310/ 309] loss: 0.690523, Accuracy: 0.751214\n",
            "[311/ 310] loss: 0.690513, Accuracy: 0.751210\n",
            "[312/ 311] loss: 0.690139, Accuracy: 0.751407\n",
            "[313/ 312] loss: 0.690125, Accuracy: 0.751603\n",
            "[314/ 313] loss: 0.690301, Accuracy: 0.751498\n",
            "[315/ 314] loss: 0.690185, Accuracy: 0.751543\n",
            "[316/ 315] loss: 0.689395, Accuracy: 0.751786\n",
            "[317/ 316] loss: 0.689976, Accuracy: 0.751632\n",
            "[318/ 317] loss: 0.690618, Accuracy: 0.751331\n",
            "[319/ 318] loss: 0.691428, Accuracy: 0.751179\n",
            "[320/ 319] loss: 0.691061, Accuracy: 0.751323\n",
            "[321/ 320] loss: 0.691932, Accuracy: 0.750977\n",
            "[322/ 321] loss: 0.691958, Accuracy: 0.750876\n",
            "[323/ 322] loss: 0.692269, Accuracy: 0.750728\n",
            "[324/ 323] loss: 0.692638, Accuracy: 0.750532\n",
            "[325/ 324] loss: 0.693082, Accuracy: 0.750338\n",
            "[326/ 325] loss: 0.693119, Accuracy: 0.750337\n",
            "[327/ 326] loss: 0.692821, Accuracy: 0.750479\n",
            "[328/ 327] loss: 0.692365, Accuracy: 0.750621\n",
            "[329/ 328] loss: 0.691939, Accuracy: 0.750857\n",
            "[330/ 329] loss: 0.692303, Accuracy: 0.750855\n",
            "[331/ 330] loss: 0.693211, Accuracy: 0.750663\n",
            "[332/ 331] loss: 0.692986, Accuracy: 0.750755\n",
            "[333/ 332] loss: 0.692801, Accuracy: 0.750753\n",
            "[334/ 333] loss: 0.693679, Accuracy: 0.750516\n",
            "[335/ 334] loss: 0.692823, Accuracy: 0.750889\n",
            "[336/ 335] loss: 0.692648, Accuracy: 0.751073\n",
            "[337/ 336] loss: 0.692479, Accuracy: 0.751023\n",
            "[338/ 337] loss: 0.692484, Accuracy: 0.751020\n",
            "[339/ 338] loss: 0.692211, Accuracy: 0.751109\n",
            "[340/ 339] loss: 0.692351, Accuracy: 0.750968\n",
            "[341/ 340] loss: 0.692165, Accuracy: 0.751057\n",
            "[342/ 341] loss: 0.692389, Accuracy: 0.750871\n",
            "[343/ 342] loss: 0.693368, Accuracy: 0.750594\n",
            "[344/ 343] loss: 0.693629, Accuracy: 0.750547\n",
            "[345/ 344] loss: 0.693775, Accuracy: 0.750545\n",
            "[346/ 345] loss: 0.693199, Accuracy: 0.750679\n",
            "[347/ 346] loss: 0.692924, Accuracy: 0.750858\n",
            "[348/ 347] loss: 0.693079, Accuracy: 0.750856\n",
            "[349/ 348] loss: 0.693427, Accuracy: 0.750673\n",
            "[350/ 349] loss: 0.693295, Accuracy: 0.750806\n",
            "[351/ 350] loss: 0.693140, Accuracy: 0.750759\n",
            "[352/ 351] loss: 0.693624, Accuracy: 0.750579\n",
            "[353/ 352] loss: 0.693346, Accuracy: 0.750666\n",
            "[354/ 353] loss: 0.693381, Accuracy: 0.750620\n",
            "[355/ 354] loss: 0.693284, Accuracy: 0.750618\n",
            "[356/ 355] loss: 0.692614, Accuracy: 0.750836\n",
            "[357/ 356] loss: 0.692990, Accuracy: 0.750790\n",
            "[358/ 357] loss: 0.692548, Accuracy: 0.750963\n",
            "[359/ 358] loss: 0.692658, Accuracy: 0.750829\n",
            "[360/ 359] loss: 0.693490, Accuracy: 0.750522\n",
            "[361/ 360] loss: 0.693763, Accuracy: 0.750391\n",
            "[362/ 361] loss: 0.694208, Accuracy: 0.750260\n",
            "[363/ 362] loss: 0.694326, Accuracy: 0.750259\n",
            "[364/ 363] loss: 0.693652, Accuracy: 0.750517\n",
            "[365/ 364] loss: 0.694014, Accuracy: 0.750515\n",
            "[366/ 365] loss: 0.693602, Accuracy: 0.750685\n",
            "[367/ 366] loss: 0.693374, Accuracy: 0.750811\n",
            "[368/ 367] loss: 0.693838, Accuracy: 0.750553\n",
            "[369/ 368] loss: 0.693971, Accuracy: 0.750510\n",
            "[370/ 369] loss: 0.693549, Accuracy: 0.750720\n",
            "[371/ 370] loss: 0.693926, Accuracy: 0.750633\n",
            "[372/ 371] loss: 0.694064, Accuracy: 0.750632\n",
            "[373/ 372] loss: 0.693665, Accuracy: 0.750714\n",
            "[374/ 373] loss: 0.693686, Accuracy: 0.750670\n",
            "[375/ 374] loss: 0.693995, Accuracy: 0.750585\n",
            "[376/ 375] loss: 0.694056, Accuracy: 0.750667\n",
            "[377/ 376] loss: 0.694558, Accuracy: 0.750457\n",
            "[378/ 377] loss: 0.695273, Accuracy: 0.750249\n",
            "[379/ 378] loss: 0.694716, Accuracy: 0.750455\n",
            "[380/ 379] loss: 0.694873, Accuracy: 0.750289\n",
            "[381/ 380] loss: 0.695020, Accuracy: 0.750247\n",
            "[382/ 381] loss: 0.695554, Accuracy: 0.750205\n",
            "[383/ 382] loss: 0.695516, Accuracy: 0.750041\n",
            "[384/ 383] loss: 0.695343, Accuracy: 0.750041\n",
            "[385/ 384] loss: 0.694735, Accuracy: 0.750326\n",
            "[386/ 385] loss: 0.695327, Accuracy: 0.750122\n",
            "[387/ 386] loss: 0.694979, Accuracy: 0.750162\n",
            "[388/ 387] loss: 0.695787, Accuracy: 0.749839\n",
            "[389/ 388] loss: 0.695563, Accuracy: 0.749919\n",
            "[390/ 389] loss: 0.695500, Accuracy: 0.749960\n",
            "[391/ 390] loss: 0.695447, Accuracy: 0.750000\n",
            "[392/ 391] loss: 0.695119, Accuracy: 0.750160\n",
            "[393/ 392] loss: 0.694871, Accuracy: 0.750159\n",
            "[394/ 393] loss: 0.695067, Accuracy: 0.750159\n",
            "[395/ 394] loss: 0.695079, Accuracy: 0.750159\n",
            "[396/ 395] loss: 0.695529, Accuracy: 0.749881\n",
            "[397/ 396] loss: 0.695640, Accuracy: 0.749803\n",
            "[398/ 397] loss: 0.696045, Accuracy: 0.749567\n",
            "[399/ 398] loss: 0.695634, Accuracy: 0.749686\n",
            "[400/ 399] loss: 0.695525, Accuracy: 0.749765\n",
            "[401/ 400] loss: 0.695457, Accuracy: 0.749883\n",
            "[402/ 401] loss: 0.695168, Accuracy: 0.749961\n",
            "[403/ 402] loss: 0.695167, Accuracy: 0.749922\n",
            "[404/ 403] loss: 0.695356, Accuracy: 0.749767\n",
            "[405/ 404] loss: 0.695722, Accuracy: 0.749691\n",
            "[406/ 405] loss: 0.695826, Accuracy: 0.749653\n",
            "[407/ 406] loss: 0.695704, Accuracy: 0.749731\n",
            "[408/ 407] loss: 0.696048, Accuracy: 0.749616\n",
            "[409/ 408] loss: 0.696765, Accuracy: 0.749234\n",
            "[410/ 409] loss: 0.697074, Accuracy: 0.749160\n",
            "[411/ 410] loss: 0.697173, Accuracy: 0.749085\n",
            "[412/ 411] loss: 0.697162, Accuracy: 0.749012\n",
            "[413/ 412] loss: 0.696904, Accuracy: 0.749166\n",
            "[414/ 413] loss: 0.696582, Accuracy: 0.749243\n",
            "[415/ 414] loss: 0.696586, Accuracy: 0.749283\n",
            "[416/ 415] loss: 0.696786, Accuracy: 0.749247\n",
            "[417/ 416] loss: 0.697081, Accuracy: 0.749211\n",
            "[418/ 417] loss: 0.696884, Accuracy: 0.749251\n",
            "[419/ 418] loss: 0.696869, Accuracy: 0.749140\n",
            "[420/ 419] loss: 0.696978, Accuracy: 0.749217\n",
            "[421/ 420] loss: 0.696955, Accuracy: 0.749293\n",
            "[422/ 421] loss: 0.697173, Accuracy: 0.749183\n",
            "[423/ 422] loss: 0.696867, Accuracy: 0.749222\n",
            "[424/ 423] loss: 0.696679, Accuracy: 0.749298\n",
            "[425/ 424] loss: 0.697059, Accuracy: 0.749189\n",
            "[426/ 425] loss: 0.696736, Accuracy: 0.749265\n",
            "[427/ 426] loss: 0.696862, Accuracy: 0.749303\n",
            "[428/ 427] loss: 0.696497, Accuracy: 0.749488\n",
            "[429/ 428] loss: 0.696785, Accuracy: 0.749306\n",
            "[430/ 429] loss: 0.696666, Accuracy: 0.749344\n",
            "[431/ 430] loss: 0.696693, Accuracy: 0.749310\n",
            "[432/ 431] loss: 0.696745, Accuracy: 0.749347\n",
            "[433/ 432] loss: 0.697094, Accuracy: 0.749277\n",
            "[434/ 433] loss: 0.697165, Accuracy: 0.749278\n",
            "[435/ 434] loss: 0.697091, Accuracy: 0.749388\n",
            "[436/ 435] loss: 0.697623, Accuracy: 0.749174\n",
            "[437/ 436] loss: 0.697557, Accuracy: 0.749176\n",
            "[438/ 437] loss: 0.697571, Accuracy: 0.749178\n",
            "[439/ 438] loss: 0.697596, Accuracy: 0.749179\n",
            "[440/ 439] loss: 0.697855, Accuracy: 0.749039\n",
            "[441/ 440] loss: 0.697963, Accuracy: 0.748970\n",
            "[442/ 441] loss: 0.698432, Accuracy: 0.748973\n",
            "[443/ 442] loss: 0.698428, Accuracy: 0.749010\n",
            "[444/ 443] loss: 0.698425, Accuracy: 0.748942\n",
            "[445/ 444] loss: 0.698794, Accuracy: 0.748874\n",
            "[446/ 445] loss: 0.699190, Accuracy: 0.748701\n",
            "[447/ 446] loss: 0.699575, Accuracy: 0.748564\n",
            "[448/ 447] loss: 0.699127, Accuracy: 0.748812\n",
            "[449/ 448] loss: 0.698869, Accuracy: 0.748954\n",
            "[450/ 449] loss: 0.699278, Accuracy: 0.748782\n",
            "[451/ 450] loss: 0.699266, Accuracy: 0.748785\n",
            "[452/ 451] loss: 0.699288, Accuracy: 0.748753\n",
            "[453/ 452] loss: 0.699358, Accuracy: 0.748652\n",
            "[454/ 453] loss: 0.699380, Accuracy: 0.748620\n",
            "[455/ 454] loss: 0.699364, Accuracy: 0.748623\n",
            "[456/ 455] loss: 0.698982, Accuracy: 0.748832\n",
            "[457/ 456] loss: 0.699249, Accuracy: 0.748698\n",
            "[458/ 457] loss: 0.699268, Accuracy: 0.748701\n",
            "[459/ 458] loss: 0.699409, Accuracy: 0.748670\n",
            "[460/ 459] loss: 0.699346, Accuracy: 0.748706\n",
            "[461/ 460] loss: 0.699203, Accuracy: 0.748811\n",
            "[462/ 461] loss: 0.699193, Accuracy: 0.748882\n",
            "[463/ 462] loss: 0.698952, Accuracy: 0.748985\n",
            "[464/ 463] loss: 0.698498, Accuracy: 0.749123\n",
            "[465/ 464] loss: 0.697941, Accuracy: 0.749293\n",
            "[466/ 465] loss: 0.697910, Accuracy: 0.749362\n",
            "[467/ 466] loss: 0.698159, Accuracy: 0.749229\n",
            "[468/ 467] loss: 0.697712, Accuracy: 0.749364\n",
            "[469/ 468] loss: 0.697901, Accuracy: 0.749332\n",
            "[470/ 469] loss: 0.698532, Accuracy: 0.749134\n",
            "[471/ 470] loss: 0.698116, Accuracy: 0.749302\n",
            "[472/ 471] loss: 0.698173, Accuracy: 0.749337\n",
            "[473/ 472] loss: 0.698213, Accuracy: 0.749272\n",
            "[474/ 473] loss: 0.697584, Accuracy: 0.749505\n",
            "[475/ 474] loss: 0.697646, Accuracy: 0.749473\n",
            "[476/ 475] loss: 0.697217, Accuracy: 0.749605\n",
            "[477/ 476] loss: 0.697564, Accuracy: 0.749442\n",
            "[478/ 477] loss: 0.697252, Accuracy: 0.749476\n",
            "[479/ 478] loss: 0.696797, Accuracy: 0.749640\n",
            "[480/ 479] loss: 0.697038, Accuracy: 0.749609\n",
            "[481/ 480] loss: 0.697670, Accuracy: 0.749381\n",
            "[482/ 481] loss: 0.697996, Accuracy: 0.749318\n",
            "[483/ 482] loss: 0.697434, Accuracy: 0.749514\n",
            "[484/ 483] loss: 0.697248, Accuracy: 0.749612\n",
            "[485/ 484] loss: 0.697779, Accuracy: 0.749451\n",
            "[486/ 485] loss: 0.697250, Accuracy: 0.749678\n",
            "[487/ 486] loss: 0.697366, Accuracy: 0.749614\n",
            "[488/ 487] loss: 0.697553, Accuracy: 0.749551\n",
            "[489/ 488] loss: 0.697546, Accuracy: 0.749520\n",
            "[490/ 489] loss: 0.697813, Accuracy: 0.749457\n",
            "[491/ 490] loss: 0.697989, Accuracy: 0.749362\n",
            "[492/ 491] loss: 0.697708, Accuracy: 0.749459\n",
            "[493/ 492] loss: 0.697908, Accuracy: 0.749397\n",
            "[494/ 493] loss: 0.697707, Accuracy: 0.749398\n",
            "[495/ 494] loss: 0.697766, Accuracy: 0.749336\n",
            "[496/ 495] loss: 0.697426, Accuracy: 0.749463\n",
            "[497/ 496] loss: 0.697141, Accuracy: 0.749622\n",
            "[498/ 497] loss: 0.697346, Accuracy: 0.749466\n",
            "[499/ 498] loss: 0.697181, Accuracy: 0.749623\n",
            "[500/ 499] loss: 0.696930, Accuracy: 0.749718\n",
            "[501/ 500] loss: 0.697221, Accuracy: 0.749687\n",
            "[502/ 501] loss: 0.697207, Accuracy: 0.749719\n",
            "[503/ 502] loss: 0.697273, Accuracy: 0.749689\n",
            "[504/ 503] loss: 0.697474, Accuracy: 0.749534\n",
            "[505/ 504] loss: 0.697250, Accuracy: 0.749628\n",
            "[506/ 505] loss: 0.697156, Accuracy: 0.749660\n",
            "[507/ 506] loss: 0.697227, Accuracy: 0.749599\n",
            "[508/ 507] loss: 0.697145, Accuracy: 0.749569\n",
            "[509/ 508] loss: 0.696781, Accuracy: 0.749692\n",
            "[510/ 509] loss: 0.696740, Accuracy: 0.749662\n",
            "[511/ 510] loss: 0.696929, Accuracy: 0.749540\n",
            "[512/ 511] loss: 0.696695, Accuracy: 0.749602\n",
            "[513/ 512] loss: 0.696311, Accuracy: 0.749756\n",
            "[514/ 513] loss: 0.696066, Accuracy: 0.749787\n",
            "[515/ 514] loss: 0.696100, Accuracy: 0.749757\n",
            "[516/ 515] loss: 0.696357, Accuracy: 0.749697\n",
            "[517/ 516] loss: 0.696455, Accuracy: 0.749637\n",
            "[518/ 517] loss: 0.696627, Accuracy: 0.749547\n",
            "[519/ 518] loss: 0.696681, Accuracy: 0.749517\n",
            "[520/ 519] loss: 0.696500, Accuracy: 0.749548\n",
            "[521/ 520] loss: 0.696447, Accuracy: 0.749700\n",
            "[522/ 521] loss: 0.696666, Accuracy: 0.749610\n",
            "[523/ 522] loss: 0.697265, Accuracy: 0.749431\n",
            "[524/ 523] loss: 0.697480, Accuracy: 0.749343\n",
            "[525/ 524] loss: 0.697225, Accuracy: 0.749344\n",
            "[526/ 525] loss: 0.696879, Accuracy: 0.749494\n",
            "[527/ 526] loss: 0.696745, Accuracy: 0.749436\n",
            "[528/ 527] loss: 0.696770, Accuracy: 0.749377\n",
            "[529/ 528] loss: 0.696360, Accuracy: 0.749556\n",
            "[530/ 529] loss: 0.696502, Accuracy: 0.749586\n",
            "[531/ 530] loss: 0.696870, Accuracy: 0.749410\n",
            "[532/ 531] loss: 0.696538, Accuracy: 0.749529\n",
            "[533/ 532] loss: 0.696297, Accuracy: 0.749677\n",
            "[534/ 533] loss: 0.696472, Accuracy: 0.749590\n",
            "[535/ 534] loss: 0.696255, Accuracy: 0.749561\n",
            "[536/ 535] loss: 0.696324, Accuracy: 0.749562\n",
            "[537/ 536] loss: 0.696975, Accuracy: 0.749359\n",
            "[538/ 537] loss: 0.696550, Accuracy: 0.749534\n",
            "[539/ 538] loss: 0.696667, Accuracy: 0.749448\n",
            "[540/ 539] loss: 0.696587, Accuracy: 0.749420\n",
            "[541/ 540] loss: 0.696798, Accuracy: 0.749306\n",
            "[542/ 541] loss: 0.697551, Accuracy: 0.749047\n",
            "[543/ 542] loss: 0.697406, Accuracy: 0.749135\n",
            "[544/ 543] loss: 0.697229, Accuracy: 0.749166\n",
            "[545/ 544] loss: 0.697081, Accuracy: 0.749138\n",
            "[546/ 545] loss: 0.696954, Accuracy: 0.749140\n",
            "[547/ 546] loss: 0.696976, Accuracy: 0.749113\n",
            "[548/ 547] loss: 0.697241, Accuracy: 0.749086\n",
            "[549/ 548] loss: 0.697438, Accuracy: 0.748974\n",
            "[550/ 549] loss: 0.697235, Accuracy: 0.749061\n",
            "[551/ 550] loss: 0.697111, Accuracy: 0.749148\n",
            "[552/ 551] loss: 0.697282, Accuracy: 0.749178\n",
            "[553/ 552] loss: 0.697278, Accuracy: 0.749179\n",
            "[554/ 553] loss: 0.697361, Accuracy: 0.749181\n",
            "[555/ 554] loss: 0.697559, Accuracy: 0.749097\n",
            "[556/ 555] loss: 0.697820, Accuracy: 0.749015\n",
            "[557/ 556] loss: 0.697524, Accuracy: 0.749157\n",
            "[558/ 557] loss: 0.697572, Accuracy: 0.749187\n",
            "[559/ 558] loss: 0.697638, Accuracy: 0.749104\n",
            "[560/ 559] loss: 0.697282, Accuracy: 0.749301\n",
            "[561/ 560] loss: 0.697471, Accuracy: 0.749302\n",
            "[562/ 561] loss: 0.697310, Accuracy: 0.749332\n",
            "[563/ 562] loss: 0.697307, Accuracy: 0.749333\n",
            "[564/ 563] loss: 0.697531, Accuracy: 0.749306\n",
            "[565/ 564] loss: 0.697601, Accuracy: 0.749280\n",
            "[566/ 565] loss: 0.697905, Accuracy: 0.749115\n",
            "[567/ 566] loss: 0.698204, Accuracy: 0.748979\n",
            "[568/ 567] loss: 0.698104, Accuracy: 0.749035\n",
            "[569/ 568] loss: 0.698224, Accuracy: 0.748900\n",
            "[570/ 569] loss: 0.698122, Accuracy: 0.748929\n",
            "[571/ 570] loss: 0.698006, Accuracy: 0.748986\n",
            "[572/ 571] loss: 0.698243, Accuracy: 0.748905\n",
            "[573/ 572] loss: 0.698240, Accuracy: 0.748935\n",
            "[574/ 573] loss: 0.698337, Accuracy: 0.748937\n",
            "[575/ 574] loss: 0.698456, Accuracy: 0.748911\n",
            "[576/ 575] loss: 0.698427, Accuracy: 0.748940\n",
            "[577/ 576] loss: 0.698162, Accuracy: 0.749051\n",
            "[578/ 577] loss: 0.697980, Accuracy: 0.749079\n",
            "[579/ 578] loss: 0.698163, Accuracy: 0.749027\n",
            "[580/ 579] loss: 0.698176, Accuracy: 0.748975\n",
            "[581/ 580] loss: 0.698571, Accuracy: 0.748869\n",
            "[582/ 581] loss: 0.698651, Accuracy: 0.748897\n",
            "[583/ 582] loss: 0.698593, Accuracy: 0.748846\n",
            "[584/ 583] loss: 0.698498, Accuracy: 0.748848\n",
            "[585/ 584] loss: 0.698565, Accuracy: 0.748769\n",
            "[586/ 585] loss: 0.698650, Accuracy: 0.748691\n",
            "[587/ 586] loss: 0.698466, Accuracy: 0.748773\n",
            "[588/ 587] loss: 0.698055, Accuracy: 0.748962\n",
            "[589/ 588] loss: 0.697626, Accuracy: 0.749123\n",
            "[590/ 589] loss: 0.697590, Accuracy: 0.749098\n",
            "[591/ 590] loss: 0.697951, Accuracy: 0.748994\n",
            "[592/ 591] loss: 0.697909, Accuracy: 0.749022\n",
            "[593/ 592] loss: 0.698040, Accuracy: 0.749023\n",
            "[594/ 593] loss: 0.698361, Accuracy: 0.748920\n",
            "[595/ 594] loss: 0.698477, Accuracy: 0.748869\n",
            "[596/ 595] loss: 0.698205, Accuracy: 0.749028\n",
            "[597/ 596] loss: 0.698174, Accuracy: 0.749056\n",
            "[598/ 597] loss: 0.698404, Accuracy: 0.748875\n",
            "[599/ 598] loss: 0.698009, Accuracy: 0.749033\n",
            "[600/ 599] loss: 0.697930, Accuracy: 0.749061\n",
            "[601 / 600]  Loss:0.697718, Accuracy:0.749167\n",
            "[601/ 600] loss: 0.697718, Accuracy: 0.749167\n",
            "[602/ 601] loss: 0.697546, Accuracy: 0.749116\n",
            "[603/ 602] loss: 0.697342, Accuracy: 0.749195\n",
            "[604/ 603] loss: 0.697073, Accuracy: 0.749274\n",
            "[605/ 604] loss: 0.696513, Accuracy: 0.749509\n",
            "[606/ 605] loss: 0.696777, Accuracy: 0.749406\n",
            "[607/ 606] loss: 0.697293, Accuracy: 0.749252\n",
            "[608/ 607] loss: 0.697221, Accuracy: 0.749331\n",
            "[609/ 608] loss: 0.697340, Accuracy: 0.749332\n",
            "[610/ 609] loss: 0.697024, Accuracy: 0.749461\n",
            "[611/ 610] loss: 0.696978, Accuracy: 0.749462\n",
            "[612/ 611] loss: 0.696940, Accuracy: 0.749489\n",
            "[613/ 612] loss: 0.697330, Accuracy: 0.749336\n",
            "[614/ 613] loss: 0.697364, Accuracy: 0.749337\n",
            "[615/ 614] loss: 0.697663, Accuracy: 0.749186\n",
            "[616/ 615] loss: 0.697687, Accuracy: 0.749212\n",
            "[617/ 616] loss: 0.698365, Accuracy: 0.748985\n",
            "[618/ 617] loss: 0.698672, Accuracy: 0.748962\n",
            "[619/ 618] loss: 0.698702, Accuracy: 0.748963\n",
            "[620/ 619] loss: 0.698732, Accuracy: 0.748965\n",
            "[621/ 620] loss: 0.698539, Accuracy: 0.749042\n",
            "[622/ 621] loss: 0.698900, Accuracy: 0.748893\n",
            "[623/ 622] loss: 0.699114, Accuracy: 0.748819\n",
            "[624/ 623] loss: 0.699435, Accuracy: 0.748671\n",
            "[625/ 624] loss: 0.699292, Accuracy: 0.748723\n",
            "[626/ 625] loss: 0.698927, Accuracy: 0.748825\n",
            "[627/ 626] loss: 0.698810, Accuracy: 0.748852\n",
            "[628/ 627] loss: 0.698896, Accuracy: 0.748829\n",
            "[629/ 628] loss: 0.699024, Accuracy: 0.748806\n",
            "[630/ 629] loss: 0.698905, Accuracy: 0.748832\n",
            "[631/ 630] loss: 0.698472, Accuracy: 0.748983\n",
            "[632/ 631] loss: 0.698750, Accuracy: 0.748861\n",
            "[633/ 632] loss: 0.698529, Accuracy: 0.748937\n",
            "[634/ 633] loss: 0.698493, Accuracy: 0.748939\n",
            "[635/ 634] loss: 0.698489, Accuracy: 0.748891\n",
            "[636/ 635] loss: 0.698417, Accuracy: 0.748917\n",
            "[637/ 636] loss: 0.698233, Accuracy: 0.748993\n",
            "[638/ 637] loss: 0.697924, Accuracy: 0.749092\n",
            "[639/ 638] loss: 0.697912, Accuracy: 0.749143\n",
            "[640/ 639] loss: 0.697527, Accuracy: 0.749242\n",
            "[641/ 640] loss: 0.697448, Accuracy: 0.749316\n",
            "[642/ 641] loss: 0.697216, Accuracy: 0.749391\n",
            "[643/ 642] loss: 0.697022, Accuracy: 0.749392\n",
            "[644/ 643] loss: 0.696900, Accuracy: 0.749368\n",
            "[645/ 644] loss: 0.696991, Accuracy: 0.749296\n",
            "[646/ 645] loss: 0.697411, Accuracy: 0.749176\n",
            "[647/ 646] loss: 0.697502, Accuracy: 0.749153\n",
            "[648/ 647] loss: 0.697698, Accuracy: 0.749131\n",
            "[649/ 648] loss: 0.697545, Accuracy: 0.749180\n",
            "[650/ 649] loss: 0.697717, Accuracy: 0.749085\n",
            "[651/ 650] loss: 0.697941, Accuracy: 0.749014\n",
            "[652/ 651] loss: 0.697962, Accuracy: 0.749040\n",
            "[653/ 652] loss: 0.698055, Accuracy: 0.749017\n",
            "[654/ 653] loss: 0.698256, Accuracy: 0.748923\n",
            "[655/ 654] loss: 0.698556, Accuracy: 0.748829\n",
            "[656/ 655] loss: 0.698522, Accuracy: 0.748831\n",
            "[657/ 656] loss: 0.698310, Accuracy: 0.748904\n",
            "[658/ 657] loss: 0.698598, Accuracy: 0.748787\n",
            "[659/ 658] loss: 0.698430, Accuracy: 0.748860\n",
            "[660/ 659] loss: 0.698466, Accuracy: 0.748838\n",
            "[661/ 660] loss: 0.698321, Accuracy: 0.748887\n",
            "[662/ 661] loss: 0.698216, Accuracy: 0.748889\n",
            "[663/ 662] loss: 0.697970, Accuracy: 0.748985\n",
            "[664/ 663] loss: 0.698084, Accuracy: 0.748916\n",
            "[665/ 664] loss: 0.697774, Accuracy: 0.749059\n",
            "[666/ 665] loss: 0.697929, Accuracy: 0.749013\n",
            "[667/ 666] loss: 0.698011, Accuracy: 0.748968\n",
            "[668/ 667] loss: 0.698190, Accuracy: 0.748876\n",
            "[669/ 668] loss: 0.697814, Accuracy: 0.749018\n",
            "[670/ 669] loss: 0.697950, Accuracy: 0.748949\n",
            "[671/ 670] loss: 0.697955, Accuracy: 0.748997\n",
            "[672/ 671] loss: 0.697872, Accuracy: 0.748975\n",
            "[673/ 672] loss: 0.697928, Accuracy: 0.748930\n",
            "[674/ 673] loss: 0.697953, Accuracy: 0.748932\n",
            "[675/ 674] loss: 0.697967, Accuracy: 0.748934\n",
            "[676/ 675] loss: 0.697883, Accuracy: 0.748912\n",
            "[677/ 676] loss: 0.697908, Accuracy: 0.748914\n",
            "[678/ 677] loss: 0.697889, Accuracy: 0.748892\n",
            "[679/ 678] loss: 0.697738, Accuracy: 0.749009\n",
            "[680/ 679] loss: 0.697822, Accuracy: 0.748964\n",
            "[681/ 680] loss: 0.697816, Accuracy: 0.748920\n",
            "[682/ 681] loss: 0.697784, Accuracy: 0.748945\n",
            "[683/ 682] loss: 0.697482, Accuracy: 0.749061\n",
            "[684/ 683] loss: 0.697515, Accuracy: 0.748993\n",
            "[685/ 684] loss: 0.697654, Accuracy: 0.748926\n",
            "[686/ 685] loss: 0.697925, Accuracy: 0.748768\n",
            "[687/ 686] loss: 0.697804, Accuracy: 0.748816\n",
            "[688/ 687] loss: 0.697764, Accuracy: 0.748840\n",
            "[689/ 688] loss: 0.697701, Accuracy: 0.748864\n",
            "[690/ 689] loss: 0.697614, Accuracy: 0.748889\n",
            "[691/ 690] loss: 0.697344, Accuracy: 0.748981\n",
            "[692/ 691] loss: 0.697583, Accuracy: 0.748937\n",
            "[693/ 692] loss: 0.697569, Accuracy: 0.748939\n",
            "[694/ 693] loss: 0.697717, Accuracy: 0.748940\n",
            "[695/ 694] loss: 0.697646, Accuracy: 0.748987\n",
            "[696/ 695] loss: 0.697469, Accuracy: 0.749078\n",
            "[697/ 696] loss: 0.697140, Accuracy: 0.749192\n",
            "[698/ 697] loss: 0.697034, Accuracy: 0.749193\n",
            "[699/ 698] loss: 0.696880, Accuracy: 0.749216\n",
            "[700/ 699] loss: 0.697101, Accuracy: 0.749151\n",
            "[701/ 700] loss: 0.697299, Accuracy: 0.749085\n",
            "[702/ 701] loss: 0.697327, Accuracy: 0.749042\n",
            "[703/ 702] loss: 0.697802, Accuracy: 0.748909\n",
            "[704/ 703] loss: 0.697804, Accuracy: 0.748889\n",
            "[705/ 704] loss: 0.697740, Accuracy: 0.748935\n",
            "[706/ 705] loss: 0.697600, Accuracy: 0.748981\n",
            "[707/ 706] loss: 0.697604, Accuracy: 0.748982\n",
            "[708/ 707] loss: 0.697716, Accuracy: 0.748939\n",
            "[709/ 708] loss: 0.697844, Accuracy: 0.748919\n",
            "[710/ 709] loss: 0.697577, Accuracy: 0.748964\n",
            "[711/ 710] loss: 0.697339, Accuracy: 0.749032\n",
            "[712/ 711] loss: 0.697098, Accuracy: 0.749099\n",
            "[713/ 712] loss: 0.697185, Accuracy: 0.749056\n",
            "[714/ 713] loss: 0.697387, Accuracy: 0.748948\n",
            "[715/ 714] loss: 0.697414, Accuracy: 0.748906\n",
            "[716/ 715] loss: 0.697365, Accuracy: 0.748886\n",
            "[717/ 716] loss: 0.697604, Accuracy: 0.748756\n",
            "[718/ 717] loss: 0.697588, Accuracy: 0.748736\n",
            "[719/ 718] loss: 0.697743, Accuracy: 0.748694\n",
            "[720/ 719] loss: 0.697562, Accuracy: 0.748740\n",
            "[721/ 720] loss: 0.697490, Accuracy: 0.748741\n",
            "[722/ 721] loss: 0.697192, Accuracy: 0.748830\n",
            "[723/ 722] loss: 0.697678, Accuracy: 0.748637\n",
            "[724/ 723] loss: 0.697681, Accuracy: 0.748552\n",
            "[725/ 724] loss: 0.697489, Accuracy: 0.748619\n",
            "[726/ 725] loss: 0.697429, Accuracy: 0.748621\n",
            "[727/ 726] loss: 0.697121, Accuracy: 0.748752\n",
            "[728/ 727] loss: 0.697200, Accuracy: 0.748689\n",
            "[729/ 728] loss: 0.696850, Accuracy: 0.748777\n",
            "[730/ 729] loss: 0.696785, Accuracy: 0.748800\n",
            "[731/ 730] loss: 0.696898, Accuracy: 0.748801\n",
            "[732/ 731] loss: 0.696960, Accuracy: 0.748760\n",
            "[733/ 732] loss: 0.697201, Accuracy: 0.748719\n",
            "[734/ 733] loss: 0.697039, Accuracy: 0.748785\n",
            "[735/ 734] loss: 0.696979, Accuracy: 0.748765\n",
            "[736/ 735] loss: 0.697110, Accuracy: 0.748703\n",
            "[737/ 736] loss: 0.697388, Accuracy: 0.748578\n",
            "[738/ 737] loss: 0.697194, Accuracy: 0.748622\n",
            "[739/ 738] loss: 0.697318, Accuracy: 0.748581\n",
            "[740/ 739] loss: 0.697613, Accuracy: 0.748541\n",
            "[741/ 740] loss: 0.697652, Accuracy: 0.748543\n",
            "[742/ 741] loss: 0.697671, Accuracy: 0.748503\n",
            "[743/ 742] loss: 0.698186, Accuracy: 0.748379\n",
            "[744/ 743] loss: 0.698032, Accuracy: 0.748465\n",
            "[745/ 744] loss: 0.697789, Accuracy: 0.748551\n",
            "[746/ 745] loss: 0.697851, Accuracy: 0.748553\n",
            "[747/ 746] loss: 0.697526, Accuracy: 0.748680\n",
            "[748/ 747] loss: 0.697411, Accuracy: 0.748724\n",
            "[749/ 748] loss: 0.697515, Accuracy: 0.748663\n",
            "[750/ 749] loss: 0.697550, Accuracy: 0.748623\n",
            "[751/ 750] loss: 0.697641, Accuracy: 0.748604\n",
            "[752/ 751] loss: 0.697882, Accuracy: 0.748502\n",
            "[753/ 752] loss: 0.698030, Accuracy: 0.748442\n",
            "[754/ 753] loss: 0.698017, Accuracy: 0.748402\n",
            "[755/ 754] loss: 0.698154, Accuracy: 0.748363\n",
            "[756/ 755] loss: 0.698326, Accuracy: 0.748344\n",
            "[757/ 756] loss: 0.698303, Accuracy: 0.748347\n",
            "[758/ 757] loss: 0.698286, Accuracy: 0.748328\n",
            "[759/ 758] loss: 0.698106, Accuracy: 0.748392\n",
            "[760/ 759] loss: 0.697797, Accuracy: 0.748497\n",
            "[761/ 760] loss: 0.698080, Accuracy: 0.748458\n",
            "[762/ 761] loss: 0.698254, Accuracy: 0.748419\n",
            "[763/ 762] loss: 0.698105, Accuracy: 0.748442\n",
            "[764/ 763] loss: 0.698193, Accuracy: 0.748382\n",
            "[765/ 764] loss: 0.697933, Accuracy: 0.748507\n",
            "[766/ 765] loss: 0.698070, Accuracy: 0.748489\n",
            "[767/ 766] loss: 0.698196, Accuracy: 0.748470\n",
            "[768/ 767] loss: 0.698023, Accuracy: 0.748513\n",
            "[769/ 768] loss: 0.697860, Accuracy: 0.748576\n",
            "[770/ 769] loss: 0.697692, Accuracy: 0.748618\n",
            "[771/ 770] loss: 0.697717, Accuracy: 0.748661\n",
            "[772/ 771] loss: 0.697668, Accuracy: 0.748662\n",
            "[773/ 772] loss: 0.697581, Accuracy: 0.748684\n",
            "[774/ 773] loss: 0.697644, Accuracy: 0.748625\n",
            "[775/ 774] loss: 0.697796, Accuracy: 0.748587\n",
            "[776/ 775] loss: 0.697928, Accuracy: 0.748548\n",
            "[777/ 776] loss: 0.698210, Accuracy: 0.748409\n",
            "[778/ 777] loss: 0.698323, Accuracy: 0.748371\n",
            "[779/ 778] loss: 0.698310, Accuracy: 0.748353\n",
            "[780/ 779] loss: 0.698265, Accuracy: 0.748375\n",
            "[781/ 780] loss: 0.697942, Accuracy: 0.748518\n",
            "[782/ 781] loss: 0.697815, Accuracy: 0.748520\n",
            "[783/ 782] loss: 0.698031, Accuracy: 0.748501\n",
            "[784/ 783] loss: 0.697819, Accuracy: 0.748603\n",
            "[785/ 784] loss: 0.697861, Accuracy: 0.748565\n",
            "[786/ 785] loss: 0.697813, Accuracy: 0.748547\n",
            "[787/ 786] loss: 0.697683, Accuracy: 0.748608\n",
            "[788/ 787] loss: 0.697724, Accuracy: 0.748590\n",
            "[789/ 788] loss: 0.697598, Accuracy: 0.748652\n",
            "[790/ 789] loss: 0.697604, Accuracy: 0.748673\n",
            "[791/ 790] loss: 0.697336, Accuracy: 0.748754\n",
            "[792/ 791] loss: 0.697316, Accuracy: 0.748795\n",
            "[793/ 792] loss: 0.697113, Accuracy: 0.748915\n",
            "[794/ 793] loss: 0.696965, Accuracy: 0.748995\n",
            "[795/ 794] loss: 0.696991, Accuracy: 0.748957\n",
            "[796/ 795] loss: 0.697139, Accuracy: 0.748919\n",
            "[797/ 796] loss: 0.697325, Accuracy: 0.748861\n",
            "[798/ 797] loss: 0.697654, Accuracy: 0.748745\n",
            "[799/ 798] loss: 0.697508, Accuracy: 0.748766\n",
            "[800/ 799] loss: 0.697390, Accuracy: 0.748807\n",
            "[801/ 800] loss: 0.697174, Accuracy: 0.748926\n",
            "[802/ 801] loss: 0.697319, Accuracy: 0.748869\n",
            "[803/ 802] loss: 0.697639, Accuracy: 0.748773\n",
            "[804/ 803] loss: 0.697720, Accuracy: 0.748755\n",
            "[805/ 804] loss: 0.697872, Accuracy: 0.748698\n",
            "[806/ 805] loss: 0.697624, Accuracy: 0.748797\n",
            "[807/ 806] loss: 0.697817, Accuracy: 0.748701\n",
            "[808/ 807] loss: 0.697682, Accuracy: 0.748742\n",
            "[809/ 808] loss: 0.697571, Accuracy: 0.748801\n",
            "[810/ 809] loss: 0.697550, Accuracy: 0.748803\n",
            "[811/ 810] loss: 0.697726, Accuracy: 0.748785\n",
            "[812/ 811] loss: 0.697735, Accuracy: 0.748805\n",
            "[813/ 812] loss: 0.697432, Accuracy: 0.748922\n",
            "[814/ 813] loss: 0.697633, Accuracy: 0.748866\n",
            "[815/ 814] loss: 0.697728, Accuracy: 0.748848\n",
            "[816/ 815] loss: 0.697679, Accuracy: 0.748888\n",
            "[817/ 816] loss: 0.697557, Accuracy: 0.748947\n",
            "[818/ 817] loss: 0.697494, Accuracy: 0.748948\n",
            "[819/ 818] loss: 0.697454, Accuracy: 0.748969\n",
            "[820/ 819] loss: 0.697557, Accuracy: 0.748932\n",
            "[821/ 820] loss: 0.697693, Accuracy: 0.748876\n",
            "[822/ 821] loss: 0.697645, Accuracy: 0.748915\n",
            "[823/ 822] loss: 0.697704, Accuracy: 0.748917\n",
            "[824/ 823] loss: 0.697584, Accuracy: 0.748956\n",
            "[825/ 824] loss: 0.697364, Accuracy: 0.749014\n",
            "[826/ 825] loss: 0.697622, Accuracy: 0.748883\n",
            "[827/ 826] loss: 0.697449, Accuracy: 0.748922\n",
            "[828/ 827] loss: 0.697360, Accuracy: 0.748923\n",
            "[829/ 828] loss: 0.697221, Accuracy: 0.748962\n",
            "[830/ 829] loss: 0.697197, Accuracy: 0.748963\n",
            "[831/ 830] loss: 0.697268, Accuracy: 0.748927\n",
            "[832/ 831] loss: 0.697334, Accuracy: 0.748872\n",
            "[833/ 832] loss: 0.697129, Accuracy: 0.748930\n",
            "[834/ 833] loss: 0.696820, Accuracy: 0.749006\n",
            "[835/ 834] loss: 0.696823, Accuracy: 0.749007\n",
            "[836/ 835] loss: 0.696884, Accuracy: 0.748971\n",
            "[837/ 836] loss: 0.696783, Accuracy: 0.749009\n",
            "[838/ 837] loss: 0.696949, Accuracy: 0.748955\n",
            "[839/ 838] loss: 0.696767, Accuracy: 0.749012\n",
            "[840/ 839] loss: 0.696709, Accuracy: 0.748994\n",
            "[841/ 840] loss: 0.696565, Accuracy: 0.749033\n",
            "[842/ 841] loss: 0.696371, Accuracy: 0.749108\n",
            "[843/ 842] loss: 0.696065, Accuracy: 0.749221\n",
            "[844/ 843] loss: 0.695982, Accuracy: 0.749222\n",
            "[845/ 844] loss: 0.696067, Accuracy: 0.749167\n",
            "[846/ 845] loss: 0.696198, Accuracy: 0.749131\n",
            "[847/ 846] loss: 0.696035, Accuracy: 0.749187\n",
            "[848/ 847] loss: 0.695960, Accuracy: 0.749244\n",
            "[849/ 848] loss: 0.696088, Accuracy: 0.749245\n",
            "[850/ 849] loss: 0.696206, Accuracy: 0.749153\n",
            "[851/ 850] loss: 0.696294, Accuracy: 0.749118\n",
            "[852/ 851] loss: 0.696308, Accuracy: 0.749155\n",
            "[853/ 852] loss: 0.696168, Accuracy: 0.749211\n",
            "[854/ 853] loss: 0.696217, Accuracy: 0.749176\n",
            "[855/ 854] loss: 0.696125, Accuracy: 0.749195\n",
            "[856/ 855] loss: 0.696011, Accuracy: 0.749178\n",
            "[857/ 856] loss: 0.695634, Accuracy: 0.749343\n",
            "[858/ 857] loss: 0.695688, Accuracy: 0.749380\n",
            "[859/ 858] loss: 0.695684, Accuracy: 0.749363\n",
            "[860/ 859] loss: 0.695712, Accuracy: 0.749363\n",
            "[861/ 860] loss: 0.695900, Accuracy: 0.749291\n",
            "[862/ 861] loss: 0.695903, Accuracy: 0.749365\n",
            "[863/ 862] loss: 0.695610, Accuracy: 0.749474\n",
            "[864/ 863] loss: 0.695674, Accuracy: 0.749421\n",
            "[865/ 864] loss: 0.695768, Accuracy: 0.749367\n",
            "[866/ 865] loss: 0.695642, Accuracy: 0.749422\n",
            "[867/ 866] loss: 0.695631, Accuracy: 0.749405\n",
            "[868/ 867] loss: 0.695606, Accuracy: 0.749405\n",
            "[869/ 868] loss: 0.695634, Accuracy: 0.749406\n",
            "[870/ 869] loss: 0.695630, Accuracy: 0.749371\n",
            "[871/ 870] loss: 0.695532, Accuracy: 0.749425\n",
            "[872/ 871] loss: 0.695444, Accuracy: 0.749426\n",
            "[873/ 872] loss: 0.695382, Accuracy: 0.749427\n",
            "[874/ 873] loss: 0.695595, Accuracy: 0.749338\n",
            "[875/ 874] loss: 0.695270, Accuracy: 0.749446\n",
            "[876/ 875] loss: 0.695175, Accuracy: 0.749500\n",
            "[877/ 876] loss: 0.695176, Accuracy: 0.749554\n",
            "[878/ 877] loss: 0.695280, Accuracy: 0.749501\n",
            "[879/ 878] loss: 0.695243, Accuracy: 0.749555\n",
            "[880/ 879] loss: 0.695363, Accuracy: 0.749538\n",
            "[881/ 880] loss: 0.695170, Accuracy: 0.749609\n",
            "[882/ 881] loss: 0.695231, Accuracy: 0.749628\n",
            "[883/ 882] loss: 0.695068, Accuracy: 0.749681\n",
            "[884/ 883] loss: 0.694972, Accuracy: 0.749735\n",
            "[885/ 884] loss: 0.694813, Accuracy: 0.749806\n",
            "[886/ 885] loss: 0.694852, Accuracy: 0.749770\n",
            "[887/ 886] loss: 0.694951, Accuracy: 0.749700\n",
            "[888/ 887] loss: 0.695176, Accuracy: 0.749630\n",
            "[889/ 888] loss: 0.694871, Accuracy: 0.749754\n",
            "[890/ 889] loss: 0.695012, Accuracy: 0.749666\n",
            "[891/ 890] loss: 0.694828, Accuracy: 0.749737\n",
            "[892/ 891] loss: 0.694639, Accuracy: 0.749807\n",
            "[893/ 892] loss: 0.694785, Accuracy: 0.749720\n",
            "[894/ 893] loss: 0.694995, Accuracy: 0.749598\n",
            "[895/ 894] loss: 0.695079, Accuracy: 0.749528\n",
            "[896/ 895] loss: 0.694897, Accuracy: 0.749616\n",
            "[897/ 896] loss: 0.694964, Accuracy: 0.749529\n",
            "[898/ 897] loss: 0.695239, Accuracy: 0.749390\n",
            "[899/ 898] loss: 0.695199, Accuracy: 0.749426\n",
            "[900/ 899] loss: 0.695106, Accuracy: 0.749461\n",
            "[901 / 900]  Loss:0.695002, Accuracy:0.749514\n",
            "[901/ 900] loss: 0.695002, Accuracy: 0.749514\n",
            "[902/ 901] loss: 0.695019, Accuracy: 0.749514\n",
            "[903/ 902] loss: 0.694975, Accuracy: 0.749515\n",
            "[904/ 903] loss: 0.694685, Accuracy: 0.749619\n",
            "[905/ 904] loss: 0.694924, Accuracy: 0.749499\n",
            "[906/ 905] loss: 0.694924, Accuracy: 0.749499\n",
            "[907/ 906] loss: 0.694908, Accuracy: 0.749517\n",
            "[908/ 907] loss: 0.694896, Accuracy: 0.749483\n",
            "[909/ 908] loss: 0.694796, Accuracy: 0.749535\n",
            "[910/ 909] loss: 0.694556, Accuracy: 0.749656\n",
            "[911/ 910] loss: 0.694624, Accuracy: 0.749639\n",
            "[912/ 911] loss: 0.694632, Accuracy: 0.749691\n",
            "[913/ 912] loss: 0.694757, Accuracy: 0.749657\n",
            "[914/ 913] loss: 0.694794, Accuracy: 0.749606\n",
            "[915/ 914] loss: 0.694979, Accuracy: 0.749590\n",
            "[916/ 915] loss: 0.694966, Accuracy: 0.749590\n",
            "[917/ 916] loss: 0.694904, Accuracy: 0.749625\n",
            "[918/ 917] loss: 0.694899, Accuracy: 0.749642\n",
            "[919/ 918] loss: 0.695092, Accuracy: 0.749557\n",
            "[920/ 919] loss: 0.694921, Accuracy: 0.749626\n",
            "[921/ 920] loss: 0.694911, Accuracy: 0.749643\n",
            "[922/ 921] loss: 0.694689, Accuracy: 0.749746\n",
            "[923/ 922] loss: 0.694844, Accuracy: 0.749661\n",
            "[924/ 923] loss: 0.694882, Accuracy: 0.749661\n",
            "[925/ 924] loss: 0.694832, Accuracy: 0.749713\n",
            "[926/ 925] loss: 0.695324, Accuracy: 0.749578\n",
            "[927/ 926] loss: 0.695517, Accuracy: 0.749494\n",
            "[928/ 927] loss: 0.695544, Accuracy: 0.749444\n",
            "[929/ 928] loss: 0.695263, Accuracy: 0.749529\n",
            "[930/ 929] loss: 0.695455, Accuracy: 0.749428\n",
            "[931/ 930] loss: 0.695508, Accuracy: 0.749412\n",
            "[932/ 931] loss: 0.695448, Accuracy: 0.749413\n",
            "[933/ 932] loss: 0.695466, Accuracy: 0.749413\n",
            "[934/ 933] loss: 0.695453, Accuracy: 0.749397\n",
            "[935/ 934] loss: 0.695448, Accuracy: 0.749415\n",
            "[936/ 935] loss: 0.695392, Accuracy: 0.749432\n",
            "[937/ 936] loss: 0.695372, Accuracy: 0.749449\n",
            "[938/ 937] loss: 0.695190, Accuracy: 0.749566\n",
            "[939/ 938] loss: 0.694867, Accuracy: 0.749667\n",
            "epochs 9\n",
            "[2/ 1] loss: 0.487794, Accuracy: 0.796875\n",
            "[3/ 2] loss: 0.635062, Accuracy: 0.765625\n",
            "[4/ 3] loss: 0.720929, Accuracy: 0.750000\n",
            "[5/ 4] loss: 0.642327, Accuracy: 0.773438\n",
            "[6/ 5] loss: 0.639919, Accuracy: 0.771875\n",
            "[7/ 6] loss: 0.661846, Accuracy: 0.768229\n",
            "[8/ 7] loss: 0.653428, Accuracy: 0.770089\n",
            "[9/ 8] loss: 0.689348, Accuracy: 0.753906\n",
            "[10/ 9] loss: 0.679597, Accuracy: 0.756944\n",
            "[11/ 10] loss: 0.716364, Accuracy: 0.745313\n",
            "[12/ 11] loss: 0.719927, Accuracy: 0.745739\n",
            "[13/ 12] loss: 0.730809, Accuracy: 0.740885\n",
            "[14/ 13] loss: 0.721054, Accuracy: 0.745192\n",
            "[15/ 14] loss: 0.719543, Accuracy: 0.744420\n",
            "[16/ 15] loss: 0.723985, Accuracy: 0.741667\n",
            "[17/ 16] loss: 0.724177, Accuracy: 0.739258\n",
            "[18/ 17] loss: 0.734345, Accuracy: 0.734375\n",
            "[19/ 18] loss: 0.738932, Accuracy: 0.733507\n",
            "[20/ 19] loss: 0.728675, Accuracy: 0.737664\n",
            "[21/ 20] loss: 0.730577, Accuracy: 0.735156\n",
            "[22/ 21] loss: 0.727066, Accuracy: 0.737351\n",
            "[23/ 22] loss: 0.721717, Accuracy: 0.740057\n",
            "[24/ 23] loss: 0.717035, Accuracy: 0.740489\n",
            "[25/ 24] loss: 0.718709, Accuracy: 0.739583\n",
            "[26/ 25] loss: 0.714472, Accuracy: 0.740625\n",
            "[27/ 26] loss: 0.710605, Accuracy: 0.742788\n",
            "[28/ 27] loss: 0.708134, Accuracy: 0.744213\n",
            "[29/ 28] loss: 0.714100, Accuracy: 0.742746\n",
            "[30/ 29] loss: 0.708622, Accuracy: 0.744612\n",
            "[31/ 30] loss: 0.713629, Accuracy: 0.743229\n",
            "[32/ 31] loss: 0.709592, Accuracy: 0.744960\n",
            "[33/ 32] loss: 0.701220, Accuracy: 0.748535\n",
            "[34/ 33] loss: 0.707346, Accuracy: 0.746686\n",
            "[35/ 34] loss: 0.706286, Accuracy: 0.747243\n",
            "[36/ 35] loss: 0.700688, Accuracy: 0.749107\n",
            "[37/ 36] loss: 0.700909, Accuracy: 0.750000\n",
            "[38/ 37] loss: 0.700958, Accuracy: 0.750422\n",
            "[39/ 38] loss: 0.694720, Accuracy: 0.752878\n",
            "[40/ 39] loss: 0.698472, Accuracy: 0.751202\n",
            "[41/ 40] loss: 0.697420, Accuracy: 0.750781\n",
            "[42/ 41] loss: 0.699431, Accuracy: 0.749619\n",
            "[43/ 42] loss: 0.702410, Accuracy: 0.748884\n",
            "[44/ 43] loss: 0.697207, Accuracy: 0.750727\n",
            "[45/ 44] loss: 0.700073, Accuracy: 0.750000\n",
            "[46/ 45] loss: 0.693962, Accuracy: 0.751736\n",
            "[47/ 46] loss: 0.689888, Accuracy: 0.752717\n",
            "[48/ 47] loss: 0.686536, Accuracy: 0.753657\n",
            "[49/ 48] loss: 0.691969, Accuracy: 0.751628\n",
            "[50/ 49] loss: 0.695062, Accuracy: 0.750000\n",
            "[51/ 50] loss: 0.695019, Accuracy: 0.749687\n",
            "[52/ 51] loss: 0.689086, Accuracy: 0.751838\n",
            "[53/ 52] loss: 0.687081, Accuracy: 0.753005\n",
            "[54/ 53] loss: 0.683975, Accuracy: 0.753538\n",
            "[55/ 54] loss: 0.683469, Accuracy: 0.753762\n",
            "[56/ 55] loss: 0.683317, Accuracy: 0.753125\n",
            "[57/ 56] loss: 0.683179, Accuracy: 0.752511\n",
            "[58/ 57] loss: 0.684012, Accuracy: 0.751919\n",
            "[59/ 58] loss: 0.684998, Accuracy: 0.751078\n",
            "[60/ 59] loss: 0.686346, Accuracy: 0.750530\n",
            "[61/ 60] loss: 0.685213, Accuracy: 0.750781\n",
            "[62/ 61] loss: 0.683068, Accuracy: 0.752049\n",
            "[63/ 62] loss: 0.681538, Accuracy: 0.753024\n",
            "[64/ 63] loss: 0.683696, Accuracy: 0.752232\n",
            "[65/ 64] loss: 0.688779, Accuracy: 0.750732\n",
            "[66/ 65] loss: 0.689958, Accuracy: 0.750000\n",
            "[67/ 66] loss: 0.684995, Accuracy: 0.751657\n",
            "[68/ 67] loss: 0.688759, Accuracy: 0.750233\n",
            "[69/ 68] loss: 0.686925, Accuracy: 0.750460\n",
            "[70/ 69] loss: 0.689760, Accuracy: 0.749094\n",
            "[71/ 70] loss: 0.693404, Accuracy: 0.747991\n",
            "[72/ 71] loss: 0.694470, Accuracy: 0.747579\n",
            "[73/ 72] loss: 0.694750, Accuracy: 0.747613\n",
            "[74/ 73] loss: 0.698066, Accuracy: 0.746147\n",
            "[75/ 74] loss: 0.696932, Accuracy: 0.746410\n",
            "[76/ 75] loss: 0.694585, Accuracy: 0.747292\n",
            "[77/ 76] loss: 0.694669, Accuracy: 0.747327\n",
            "[78/ 77] loss: 0.692358, Accuracy: 0.748174\n",
            "[79/ 78] loss: 0.690704, Accuracy: 0.748598\n",
            "[80/ 79] loss: 0.687204, Accuracy: 0.750198\n",
            "[81/ 80] loss: 0.686563, Accuracy: 0.750195\n",
            "[82/ 81] loss: 0.686188, Accuracy: 0.750579\n",
            "[83/ 82] loss: 0.685584, Accuracy: 0.750953\n",
            "[84/ 83] loss: 0.684025, Accuracy: 0.751694\n",
            "[85/ 84] loss: 0.682308, Accuracy: 0.751860\n",
            "[86/ 85] loss: 0.680021, Accuracy: 0.752941\n",
            "[87/ 86] loss: 0.678707, Accuracy: 0.753452\n",
            "[88/ 87] loss: 0.678366, Accuracy: 0.753233\n",
            "[89/ 88] loss: 0.676690, Accuracy: 0.754084\n",
            "[90/ 89] loss: 0.677160, Accuracy: 0.753862\n",
            "[91/ 90] loss: 0.678806, Accuracy: 0.753299\n",
            "[92/ 91] loss: 0.678712, Accuracy: 0.753091\n",
            "[93/ 92] loss: 0.677169, Accuracy: 0.754076\n",
            "[94/ 93] loss: 0.678538, Accuracy: 0.753360\n",
            "[95/ 94] loss: 0.674429, Accuracy: 0.754987\n",
            "[96/ 95] loss: 0.677091, Accuracy: 0.753947\n",
            "[97/ 96] loss: 0.675806, Accuracy: 0.754720\n",
            "[98/ 97] loss: 0.674636, Accuracy: 0.754994\n",
            "[99/ 98] loss: 0.678407, Accuracy: 0.753827\n",
            "[100/ 99] loss: 0.677245, Accuracy: 0.754104\n",
            "[101/ 100] loss: 0.678929, Accuracy: 0.753281\n",
            "[102/ 101] loss: 0.679182, Accuracy: 0.753094\n",
            "[103/ 102] loss: 0.680505, Accuracy: 0.752757\n",
            "[104/ 103] loss: 0.679704, Accuracy: 0.752882\n",
            "[105/ 104] loss: 0.679351, Accuracy: 0.753606\n",
            "[106/ 105] loss: 0.678543, Accuracy: 0.753869\n",
            "[107/ 106] loss: 0.676774, Accuracy: 0.754275\n",
            "[108/ 107] loss: 0.674945, Accuracy: 0.754965\n",
            "[109/ 108] loss: 0.672466, Accuracy: 0.756076\n",
            "[110/ 109] loss: 0.675848, Accuracy: 0.755304\n",
            "[111/ 110] loss: 0.676363, Accuracy: 0.755256\n",
            "[112/ 111] loss: 0.675030, Accuracy: 0.756053\n",
            "[113/ 112] loss: 0.675640, Accuracy: 0.755999\n",
            "[114/ 113] loss: 0.677055, Accuracy: 0.755254\n",
            "[115/ 114] loss: 0.674882, Accuracy: 0.755894\n",
            "[116/ 115] loss: 0.677989, Accuracy: 0.755299\n",
            "[117/ 116] loss: 0.677492, Accuracy: 0.755523\n",
            "[118/ 117] loss: 0.676742, Accuracy: 0.755743\n",
            "[119/ 118] loss: 0.677091, Accuracy: 0.755694\n",
            "[120/ 119] loss: 0.677944, Accuracy: 0.755515\n",
            "[121/ 120] loss: 0.678140, Accuracy: 0.755208\n",
            "[122/ 121] loss: 0.679620, Accuracy: 0.754778\n",
            "[123/ 122] loss: 0.678773, Accuracy: 0.755123\n",
            "[124/ 123] loss: 0.678209, Accuracy: 0.755462\n",
            "[125/ 124] loss: 0.679029, Accuracy: 0.755292\n",
            "[126/ 125] loss: 0.678407, Accuracy: 0.755625\n",
            "[127/ 126] loss: 0.677996, Accuracy: 0.755704\n",
            "[128/ 127] loss: 0.677975, Accuracy: 0.755659\n",
            "[129/ 128] loss: 0.678256, Accuracy: 0.755493\n",
            "[130/ 129] loss: 0.678637, Accuracy: 0.755814\n",
            "[131/ 130] loss: 0.679544, Accuracy: 0.755168\n",
            "[132/ 131] loss: 0.680362, Accuracy: 0.754652\n",
            "[133/ 132] loss: 0.678957, Accuracy: 0.754853\n",
            "[134/ 133] loss: 0.680163, Accuracy: 0.754699\n",
            "[135/ 134] loss: 0.678838, Accuracy: 0.755014\n",
            "[136/ 135] loss: 0.678714, Accuracy: 0.754977\n",
            "[137/ 136] loss: 0.677413, Accuracy: 0.755285\n",
            "[138/ 137] loss: 0.678127, Accuracy: 0.754790\n",
            "[139/ 138] loss: 0.677030, Accuracy: 0.755322\n",
            "[140/ 139] loss: 0.676732, Accuracy: 0.755508\n",
            "[141/ 140] loss: 0.676290, Accuracy: 0.755804\n",
            "[142/ 141] loss: 0.677165, Accuracy: 0.755541\n",
            "[143/ 142] loss: 0.676813, Accuracy: 0.755502\n",
            "[144/ 143] loss: 0.676057, Accuracy: 0.755900\n",
            "[145/ 144] loss: 0.676753, Accuracy: 0.755751\n",
            "[146/ 145] loss: 0.675024, Accuracy: 0.756358\n",
            "[147/ 146] loss: 0.674668, Accuracy: 0.756421\n",
            "[148/ 147] loss: 0.675573, Accuracy: 0.755952\n",
            "[149/ 148] loss: 0.675317, Accuracy: 0.756123\n",
            "[150/ 149] loss: 0.675616, Accuracy: 0.755977\n",
            "[151/ 150] loss: 0.676029, Accuracy: 0.755625\n",
            "[152/ 151] loss: 0.676023, Accuracy: 0.755588\n",
            "[153/ 152] loss: 0.676343, Accuracy: 0.755448\n",
            "[154/ 153] loss: 0.676847, Accuracy: 0.755515\n",
            "[155/ 154] loss: 0.678096, Accuracy: 0.755377\n",
            "[156/ 155] loss: 0.678016, Accuracy: 0.755645\n",
            "[157/ 156] loss: 0.678114, Accuracy: 0.755609\n",
            "[158/ 157] loss: 0.678322, Accuracy: 0.755573\n",
            "[159/ 158] loss: 0.678095, Accuracy: 0.755340\n",
            "[160/ 159] loss: 0.679001, Accuracy: 0.755208\n",
            "[161/ 160] loss: 0.677893, Accuracy: 0.755664\n",
            "[162/ 161] loss: 0.677503, Accuracy: 0.755823\n",
            "[163/ 162] loss: 0.677832, Accuracy: 0.755787\n",
            "[164/ 163] loss: 0.679043, Accuracy: 0.755368\n",
            "[165/ 164] loss: 0.679309, Accuracy: 0.755145\n",
            "[166/ 165] loss: 0.680055, Accuracy: 0.755114\n",
            "[167/ 166] loss: 0.680373, Accuracy: 0.755177\n",
            "[168/ 167] loss: 0.679630, Accuracy: 0.755614\n",
            "[169/ 168] loss: 0.680485, Accuracy: 0.755301\n",
            "[170/ 169] loss: 0.682040, Accuracy: 0.754993\n",
            "[171/ 170] loss: 0.683099, Accuracy: 0.754596\n",
            "[172/ 171] loss: 0.683513, Accuracy: 0.754660\n",
            "[173/ 172] loss: 0.684578, Accuracy: 0.754633\n",
            "[174/ 173] loss: 0.684249, Accuracy: 0.754697\n",
            "[175/ 174] loss: 0.684752, Accuracy: 0.754670\n",
            "[176/ 175] loss: 0.684952, Accuracy: 0.754732\n",
            "[177/ 176] loss: 0.686175, Accuracy: 0.753906\n",
            "[178/ 177] loss: 0.685193, Accuracy: 0.754326\n",
            "[179/ 178] loss: 0.685390, Accuracy: 0.754126\n",
            "[180/ 179] loss: 0.685499, Accuracy: 0.754190\n",
            "[181/ 180] loss: 0.685729, Accuracy: 0.753993\n",
            "[182/ 181] loss: 0.685176, Accuracy: 0.754144\n",
            "[183/ 182] loss: 0.685982, Accuracy: 0.753863\n",
            "[184/ 183] loss: 0.687246, Accuracy: 0.753245\n",
            "[185/ 184] loss: 0.686574, Accuracy: 0.753397\n",
            "[186/ 185] loss: 0.688302, Accuracy: 0.752618\n",
            "[187/ 186] loss: 0.688488, Accuracy: 0.752520\n",
            "[188/ 187] loss: 0.688134, Accuracy: 0.752674\n",
            "[189/ 188] loss: 0.687853, Accuracy: 0.752743\n",
            "[190/ 189] loss: 0.687747, Accuracy: 0.752563\n",
            "[191/ 190] loss: 0.686791, Accuracy: 0.752878\n",
            "[192/ 191] loss: 0.687712, Accuracy: 0.752618\n",
            "[193/ 192] loss: 0.686256, Accuracy: 0.753092\n",
            "[194/ 193] loss: 0.686234, Accuracy: 0.753238\n",
            "[195/ 194] loss: 0.687670, Accuracy: 0.752819\n",
            "[196/ 195] loss: 0.687807, Accuracy: 0.752804\n",
            "[197/ 196] loss: 0.687500, Accuracy: 0.753109\n",
            "[198/ 197] loss: 0.687985, Accuracy: 0.752935\n",
            "[199/ 198] loss: 0.688179, Accuracy: 0.752841\n",
            "[200/ 199] loss: 0.688362, Accuracy: 0.752670\n",
            "[201/ 200] loss: 0.689166, Accuracy: 0.752266\n",
            "[202/ 201] loss: 0.689911, Accuracy: 0.751943\n",
            "[203/ 202] loss: 0.690340, Accuracy: 0.751934\n",
            "[204/ 203] loss: 0.689554, Accuracy: 0.752386\n",
            "[205/ 204] loss: 0.688884, Accuracy: 0.752757\n",
            "[206/ 205] loss: 0.688553, Accuracy: 0.752820\n",
            "[207/ 206] loss: 0.687671, Accuracy: 0.753034\n",
            "[208/ 207] loss: 0.688292, Accuracy: 0.752944\n",
            "[209/ 208] loss: 0.688535, Accuracy: 0.752704\n",
            "[210/ 209] loss: 0.688745, Accuracy: 0.752542\n",
            "[211/ 210] loss: 0.689400, Accuracy: 0.751935\n",
            "[212/ 211] loss: 0.689410, Accuracy: 0.751851\n",
            "[213/ 212] loss: 0.690331, Accuracy: 0.751400\n",
            "[214/ 213] loss: 0.690085, Accuracy: 0.751540\n",
            "[215/ 214] loss: 0.690163, Accuracy: 0.751387\n",
            "[216/ 215] loss: 0.690428, Accuracy: 0.751235\n",
            "[217/ 216] loss: 0.689910, Accuracy: 0.751374\n",
            "[218/ 217] loss: 0.689649, Accuracy: 0.751368\n",
            "[219/ 218] loss: 0.690595, Accuracy: 0.751075\n",
            "[220/ 219] loss: 0.690155, Accuracy: 0.751213\n",
            "[221/ 220] loss: 0.689817, Accuracy: 0.751278\n",
            "[222/ 221] loss: 0.690285, Accuracy: 0.751202\n",
            "[223/ 222] loss: 0.690855, Accuracy: 0.750845\n",
            "[224/ 223] loss: 0.690304, Accuracy: 0.751121\n",
            "[225/ 224] loss: 0.689964, Accuracy: 0.751256\n",
            "[226/ 225] loss: 0.691143, Accuracy: 0.750903\n",
            "[227/ 226] loss: 0.689786, Accuracy: 0.751452\n",
            "[228/ 227] loss: 0.689672, Accuracy: 0.751445\n",
            "[229/ 228] loss: 0.689986, Accuracy: 0.751302\n",
            "[230/ 229] loss: 0.689072, Accuracy: 0.751501\n",
            "[231/ 230] loss: 0.688993, Accuracy: 0.751359\n",
            "[232/ 231] loss: 0.688362, Accuracy: 0.751556\n",
            "[233/ 232] loss: 0.688567, Accuracy: 0.751482\n",
            "[234/ 233] loss: 0.688761, Accuracy: 0.751408\n",
            "[235/ 234] loss: 0.688337, Accuracy: 0.751536\n",
            "[236/ 235] loss: 0.687902, Accuracy: 0.751795\n",
            "[237/ 236] loss: 0.687043, Accuracy: 0.752119\n",
            "[238/ 237] loss: 0.688097, Accuracy: 0.751714\n",
            "[239/ 238] loss: 0.687385, Accuracy: 0.752035\n",
            "[240/ 239] loss: 0.686954, Accuracy: 0.752223\n",
            "[241/ 240] loss: 0.687118, Accuracy: 0.752148\n",
            "[242/ 241] loss: 0.686822, Accuracy: 0.752204\n",
            "[243/ 242] loss: 0.687047, Accuracy: 0.752131\n",
            "[244/ 243] loss: 0.686486, Accuracy: 0.752315\n",
            "[245/ 244] loss: 0.687432, Accuracy: 0.751793\n",
            "[246/ 245] loss: 0.687027, Accuracy: 0.751913\n",
            "[247/ 246] loss: 0.687333, Accuracy: 0.751651\n",
            "[248/ 247] loss: 0.687665, Accuracy: 0.751518\n",
            "[249/ 248] loss: 0.687032, Accuracy: 0.751701\n",
            "[250/ 249] loss: 0.687586, Accuracy: 0.751506\n",
            "[251/ 250] loss: 0.687998, Accuracy: 0.751312\n",
            "[252/ 251] loss: 0.687115, Accuracy: 0.751743\n",
            "[253/ 252] loss: 0.687242, Accuracy: 0.751860\n",
            "[254/ 253] loss: 0.688317, Accuracy: 0.751544\n",
            "[255/ 254] loss: 0.688350, Accuracy: 0.751538\n",
            "[256/ 255] loss: 0.688035, Accuracy: 0.751593\n",
            "[257/ 256] loss: 0.688747, Accuracy: 0.751282\n",
            "[258/ 257] loss: 0.689769, Accuracy: 0.750851\n",
            "[259/ 258] loss: 0.690082, Accuracy: 0.750666\n",
            "[260/ 259] loss: 0.690705, Accuracy: 0.750302\n",
            "[261/ 260] loss: 0.690869, Accuracy: 0.750300\n",
            "[262/ 261] loss: 0.690467, Accuracy: 0.750419\n",
            "[263/ 262] loss: 0.690163, Accuracy: 0.750477\n",
            "[264/ 263] loss: 0.690159, Accuracy: 0.750475\n",
            "[265/ 264] loss: 0.690794, Accuracy: 0.750237\n",
            "[266/ 265] loss: 0.691404, Accuracy: 0.750059\n",
            "[267/ 266] loss: 0.691550, Accuracy: 0.750059\n",
            "[268/ 267] loss: 0.691132, Accuracy: 0.750351\n",
            "[269/ 268] loss: 0.690915, Accuracy: 0.750350\n",
            "[270/ 269] loss: 0.690034, Accuracy: 0.750697\n",
            "[271/ 270] loss: 0.689764, Accuracy: 0.750868\n",
            "[272/ 271] loss: 0.689603, Accuracy: 0.750865\n",
            "[273/ 272] loss: 0.689169, Accuracy: 0.750977\n",
            "[274/ 273] loss: 0.688101, Accuracy: 0.751374\n",
            "[275/ 274] loss: 0.687149, Accuracy: 0.751654\n",
            "[276/ 275] loss: 0.687082, Accuracy: 0.751591\n",
            "[277/ 276] loss: 0.686060, Accuracy: 0.751981\n",
            "[278/ 277] loss: 0.685655, Accuracy: 0.752031\n",
            "[279/ 278] loss: 0.685301, Accuracy: 0.752136\n",
            "[280/ 279] loss: 0.685389, Accuracy: 0.752128\n",
            "[281/ 280] loss: 0.685801, Accuracy: 0.752065\n",
            "[282/ 281] loss: 0.685179, Accuracy: 0.752224\n",
            "[283/ 282] loss: 0.685507, Accuracy: 0.752216\n",
            "[284/ 283] loss: 0.684592, Accuracy: 0.752485\n",
            "[285/ 284] loss: 0.685384, Accuracy: 0.752366\n",
            "[286/ 285] loss: 0.685728, Accuracy: 0.752193\n",
            "[287/ 286] loss: 0.685656, Accuracy: 0.752131\n",
            "[288/ 287] loss: 0.685399, Accuracy: 0.752232\n",
            "[289/ 288] loss: 0.685803, Accuracy: 0.752170\n",
            "[290/ 289] loss: 0.685575, Accuracy: 0.752271\n",
            "[291/ 290] loss: 0.684969, Accuracy: 0.752478\n",
            "[292/ 291] loss: 0.685184, Accuracy: 0.752416\n",
            "[293/ 292] loss: 0.685410, Accuracy: 0.752301\n",
            "[294/ 293] loss: 0.686124, Accuracy: 0.751973\n",
            "[295/ 294] loss: 0.685990, Accuracy: 0.752020\n",
            "[296/ 295] loss: 0.685990, Accuracy: 0.752013\n",
            "[297/ 296] loss: 0.685983, Accuracy: 0.752059\n",
            "[298/ 297] loss: 0.686503, Accuracy: 0.751999\n",
            "[299/ 298] loss: 0.686390, Accuracy: 0.751888\n",
            "[300/ 299] loss: 0.687009, Accuracy: 0.751672\n",
            "[301 / 300]  Loss:0.687191, Accuracy:0.751510\n",
            "[301/ 300] loss: 0.687191, Accuracy: 0.751510\n",
            "[302/ 301] loss: 0.687208, Accuracy: 0.751661\n",
            "[303/ 302] loss: 0.687791, Accuracy: 0.751449\n",
            "[304/ 303] loss: 0.688752, Accuracy: 0.751031\n",
            "[305/ 304] loss: 0.688668, Accuracy: 0.751079\n",
            "[306/ 305] loss: 0.688413, Accuracy: 0.751127\n",
            "[307/ 306] loss: 0.687542, Accuracy: 0.751481\n",
            "[308/ 307] loss: 0.686924, Accuracy: 0.751629\n",
            "[309/ 308] loss: 0.686905, Accuracy: 0.751674\n",
            "[310/ 309] loss: 0.687213, Accuracy: 0.751517\n",
            "[311/ 310] loss: 0.687493, Accuracy: 0.751310\n",
            "[312/ 311] loss: 0.686963, Accuracy: 0.751608\n",
            "[313/ 312] loss: 0.686292, Accuracy: 0.751853\n",
            "[314/ 313] loss: 0.686810, Accuracy: 0.751697\n",
            "[315/ 314] loss: 0.687067, Accuracy: 0.751692\n",
            "[316/ 315] loss: 0.686877, Accuracy: 0.751835\n",
            "[317/ 316] loss: 0.687167, Accuracy: 0.751681\n",
            "[318/ 317] loss: 0.687388, Accuracy: 0.751577\n",
            "[319/ 318] loss: 0.687415, Accuracy: 0.751671\n",
            "[320/ 319] loss: 0.687750, Accuracy: 0.751469\n",
            "[321/ 320] loss: 0.687208, Accuracy: 0.751660\n",
            "[322/ 321] loss: 0.687214, Accuracy: 0.751655\n",
            "[323/ 322] loss: 0.687060, Accuracy: 0.751698\n",
            "[324/ 323] loss: 0.687028, Accuracy: 0.751645\n",
            "[325/ 324] loss: 0.687241, Accuracy: 0.751495\n",
            "[326/ 325] loss: 0.687633, Accuracy: 0.751298\n",
            "[327/ 326] loss: 0.687402, Accuracy: 0.751294\n",
            "[328/ 327] loss: 0.687252, Accuracy: 0.751290\n",
            "[329/ 328] loss: 0.687759, Accuracy: 0.751096\n",
            "[330/ 329] loss: 0.687287, Accuracy: 0.751235\n",
            "[331/ 330] loss: 0.687396, Accuracy: 0.751326\n",
            "[332/ 331] loss: 0.687997, Accuracy: 0.751180\n",
            "[333/ 332] loss: 0.687420, Accuracy: 0.751365\n",
            "[334/ 333] loss: 0.687391, Accuracy: 0.751314\n",
            "[335/ 334] loss: 0.687140, Accuracy: 0.751450\n",
            "[336/ 335] loss: 0.687325, Accuracy: 0.751353\n",
            "[337/ 336] loss: 0.687815, Accuracy: 0.751209\n",
            "[338/ 337] loss: 0.688346, Accuracy: 0.751020\n",
            "[339/ 338] loss: 0.687777, Accuracy: 0.751248\n",
            "[340/ 339] loss: 0.688071, Accuracy: 0.751106\n",
            "[341/ 340] loss: 0.687161, Accuracy: 0.751425\n",
            "[342/ 341] loss: 0.687423, Accuracy: 0.751283\n",
            "[343/ 342] loss: 0.686751, Accuracy: 0.751599\n",
            "[344/ 343] loss: 0.687141, Accuracy: 0.751503\n",
            "[345/ 344] loss: 0.687047, Accuracy: 0.751544\n",
            "[346/ 345] loss: 0.686972, Accuracy: 0.751630\n",
            "[347/ 346] loss: 0.686075, Accuracy: 0.751942\n",
            "[348/ 347] loss: 0.686159, Accuracy: 0.751936\n",
            "[349/ 348] loss: 0.686523, Accuracy: 0.751841\n",
            "[350/ 349] loss: 0.686345, Accuracy: 0.751880\n",
            "[351/ 350] loss: 0.686268, Accuracy: 0.751875\n",
            "[352/ 351] loss: 0.686565, Accuracy: 0.751870\n",
            "[353/ 352] loss: 0.686411, Accuracy: 0.751998\n",
            "[354/ 353] loss: 0.686570, Accuracy: 0.751903\n",
            "[355/ 354] loss: 0.686855, Accuracy: 0.751766\n",
            "[356/ 355] loss: 0.686462, Accuracy: 0.751981\n",
            "[357/ 356] loss: 0.685649, Accuracy: 0.752370\n",
            "[358/ 357] loss: 0.685887, Accuracy: 0.752276\n",
            "[359/ 358] loss: 0.686102, Accuracy: 0.752182\n",
            "[360/ 359] loss: 0.685879, Accuracy: 0.752350\n",
            "[361/ 360] loss: 0.685740, Accuracy: 0.752474\n",
            "[362/ 361] loss: 0.685640, Accuracy: 0.752467\n",
            "[363/ 362] loss: 0.685402, Accuracy: 0.752590\n",
            "[364/ 363] loss: 0.685525, Accuracy: 0.752540\n",
            "[365/ 364] loss: 0.686070, Accuracy: 0.752361\n",
            "[366/ 365] loss: 0.686307, Accuracy: 0.752226\n",
            "[367/ 366] loss: 0.686191, Accuracy: 0.752263\n",
            "[368/ 367] loss: 0.686284, Accuracy: 0.752299\n",
            "[369/ 368] loss: 0.686087, Accuracy: 0.752378\n",
            "[370/ 369] loss: 0.686690, Accuracy: 0.752117\n",
            "[371/ 370] loss: 0.686877, Accuracy: 0.752111\n",
            "[372/ 371] loss: 0.687390, Accuracy: 0.751979\n",
            "[373/ 372] loss: 0.686957, Accuracy: 0.752058\n",
            "[374/ 373] loss: 0.686618, Accuracy: 0.752220\n",
            "[375/ 374] loss: 0.686413, Accuracy: 0.752256\n",
            "[376/ 375] loss: 0.686925, Accuracy: 0.752042\n",
            "[377/ 376] loss: 0.686384, Accuracy: 0.752202\n",
            "[378/ 377] loss: 0.686301, Accuracy: 0.752321\n",
            "[379/ 378] loss: 0.685948, Accuracy: 0.752439\n",
            "[380/ 379] loss: 0.685947, Accuracy: 0.752432\n",
            "[381/ 380] loss: 0.685837, Accuracy: 0.752508\n",
            "[382/ 381] loss: 0.685702, Accuracy: 0.752584\n",
            "[383/ 382] loss: 0.685769, Accuracy: 0.752536\n",
            "[384/ 383] loss: 0.685561, Accuracy: 0.752570\n",
            "[385/ 384] loss: 0.685862, Accuracy: 0.752360\n",
            "[386/ 385] loss: 0.685897, Accuracy: 0.752313\n",
            "[387/ 386] loss: 0.685518, Accuracy: 0.752429\n",
            "[388/ 387] loss: 0.685830, Accuracy: 0.752261\n",
            "[389/ 388] loss: 0.685958, Accuracy: 0.752175\n",
            "[390/ 389] loss: 0.686401, Accuracy: 0.752008\n",
            "[391/ 390] loss: 0.686432, Accuracy: 0.751963\n",
            "[392/ 391] loss: 0.686089, Accuracy: 0.752158\n",
            "[393/ 392] loss: 0.686039, Accuracy: 0.752152\n",
            "[394/ 393] loss: 0.685653, Accuracy: 0.752306\n",
            "[395/ 394] loss: 0.685345, Accuracy: 0.752379\n",
            "[396/ 395] loss: 0.685691, Accuracy: 0.752255\n",
            "[397/ 396] loss: 0.685955, Accuracy: 0.752289\n",
            "[398/ 397] loss: 0.686505, Accuracy: 0.752007\n",
            "[399/ 398] loss: 0.686601, Accuracy: 0.752002\n",
            "[400/ 399] loss: 0.686660, Accuracy: 0.751841\n",
            "[401/ 400] loss: 0.686441, Accuracy: 0.751914\n",
            "[402/ 401] loss: 0.686079, Accuracy: 0.752026\n",
            "[403/ 402] loss: 0.686100, Accuracy: 0.752021\n",
            "[404/ 403] loss: 0.685765, Accuracy: 0.752094\n",
            "[405/ 404] loss: 0.685779, Accuracy: 0.752127\n",
            "[406/ 405] loss: 0.685382, Accuracy: 0.752238\n",
            "[407/ 406] loss: 0.685055, Accuracy: 0.752348\n",
            "[408/ 407] loss: 0.684789, Accuracy: 0.752457\n",
            "[409/ 408] loss: 0.685194, Accuracy: 0.752336\n",
            "[410/ 409] loss: 0.684944, Accuracy: 0.752407\n",
            "[411/ 410] loss: 0.685066, Accuracy: 0.752363\n",
            "[412/ 411] loss: 0.684602, Accuracy: 0.752547\n",
            "[413/ 412] loss: 0.684852, Accuracy: 0.752541\n",
            "[414/ 413] loss: 0.685040, Accuracy: 0.752346\n",
            "[415/ 414] loss: 0.685478, Accuracy: 0.752227\n",
            "[416/ 415] loss: 0.684802, Accuracy: 0.752485\n",
            "[417/ 416] loss: 0.684517, Accuracy: 0.752629\n",
            "[418/ 417] loss: 0.684748, Accuracy: 0.752548\n",
            "[419/ 418] loss: 0.684908, Accuracy: 0.752467\n",
            "[420/ 419] loss: 0.684718, Accuracy: 0.752573\n",
            "[421/ 420] loss: 0.684695, Accuracy: 0.752493\n",
            "[422/ 421] loss: 0.685327, Accuracy: 0.752301\n",
            "[423/ 422] loss: 0.685078, Accuracy: 0.752444\n",
            "[424/ 423] loss: 0.684849, Accuracy: 0.752549\n",
            "[425/ 424] loss: 0.684908, Accuracy: 0.752580\n",
            "[426/ 425] loss: 0.684250, Accuracy: 0.752868\n",
            "[427/ 426] loss: 0.684084, Accuracy: 0.752971\n",
            "[428/ 427] loss: 0.684009, Accuracy: 0.753037\n",
            "[429/ 428] loss: 0.684223, Accuracy: 0.752848\n",
            "[430/ 429] loss: 0.684836, Accuracy: 0.752659\n",
            "[431/ 430] loss: 0.684893, Accuracy: 0.752653\n",
            "[432/ 431] loss: 0.684346, Accuracy: 0.752828\n",
            "[433/ 432] loss: 0.683758, Accuracy: 0.753038\n",
            "[434/ 433] loss: 0.683496, Accuracy: 0.753103\n",
            "[435/ 434] loss: 0.683227, Accuracy: 0.753240\n",
            "[436/ 435] loss: 0.683487, Accuracy: 0.753197\n",
            "[437/ 436] loss: 0.683530, Accuracy: 0.753333\n",
            "[438/ 437] loss: 0.683777, Accuracy: 0.753289\n",
            "[439/ 438] loss: 0.683711, Accuracy: 0.753318\n",
            "[440/ 439] loss: 0.684020, Accuracy: 0.753203\n",
            "[441/ 440] loss: 0.683590, Accuracy: 0.753374\n",
            "[442/ 441] loss: 0.683614, Accuracy: 0.753401\n",
            "[443/ 442] loss: 0.684137, Accuracy: 0.753146\n",
            "[444/ 443] loss: 0.684258, Accuracy: 0.753104\n",
            "[445/ 444] loss: 0.684705, Accuracy: 0.753097\n",
            "[446/ 445] loss: 0.684877, Accuracy: 0.752949\n",
            "[447/ 446] loss: 0.684915, Accuracy: 0.752943\n",
            "[448/ 447] loss: 0.685076, Accuracy: 0.752971\n",
            "[449/ 448] loss: 0.684799, Accuracy: 0.753104\n",
            "[450/ 449] loss: 0.684808, Accuracy: 0.753097\n",
            "[451/ 450] loss: 0.684810, Accuracy: 0.753090\n",
            "[452/ 451] loss: 0.684537, Accuracy: 0.753222\n",
            "[453/ 452] loss: 0.684616, Accuracy: 0.753180\n",
            "[454/ 453] loss: 0.684466, Accuracy: 0.753242\n",
            "[455/ 454] loss: 0.684645, Accuracy: 0.753201\n",
            "[456/ 455] loss: 0.684546, Accuracy: 0.753159\n",
            "[457/ 456] loss: 0.684359, Accuracy: 0.753324\n",
            "[458/ 457] loss: 0.684278, Accuracy: 0.753316\n",
            "[459/ 458] loss: 0.683593, Accuracy: 0.753582\n",
            "[460/ 459] loss: 0.683562, Accuracy: 0.753676\n",
            "[461/ 460] loss: 0.683309, Accuracy: 0.753736\n",
            "[462/ 461] loss: 0.683451, Accuracy: 0.753694\n",
            "[463/ 462] loss: 0.683821, Accuracy: 0.753686\n",
            "[464/ 463] loss: 0.684117, Accuracy: 0.753577\n",
            "[465/ 464] loss: 0.684492, Accuracy: 0.753367\n",
            "[466/ 465] loss: 0.684288, Accuracy: 0.753461\n",
            "[467/ 466] loss: 0.684425, Accuracy: 0.753454\n",
            "[468/ 467] loss: 0.684184, Accuracy: 0.753547\n",
            "[469/ 468] loss: 0.684589, Accuracy: 0.753372\n",
            "[470/ 469] loss: 0.684486, Accuracy: 0.753431\n",
            "[471/ 470] loss: 0.684919, Accuracy: 0.753225\n",
            "[472/ 471] loss: 0.685257, Accuracy: 0.753052\n",
            "[473/ 472] loss: 0.684870, Accuracy: 0.753145\n",
            "[474/ 473] loss: 0.684912, Accuracy: 0.753072\n",
            "[475/ 474] loss: 0.684543, Accuracy: 0.753231\n",
            "[476/ 475] loss: 0.684349, Accuracy: 0.753355\n",
            "[477/ 476] loss: 0.684123, Accuracy: 0.753480\n",
            "[478/ 477] loss: 0.684599, Accuracy: 0.753210\n",
            "[479/ 478] loss: 0.684246, Accuracy: 0.753302\n",
            "[480/ 479] loss: 0.684367, Accuracy: 0.753229\n",
            "[481/ 480] loss: 0.684951, Accuracy: 0.753060\n",
            "[482/ 481] loss: 0.684582, Accuracy: 0.753183\n",
            "[483/ 482] loss: 0.684531, Accuracy: 0.753209\n",
            "[484/ 483] loss: 0.684576, Accuracy: 0.753170\n",
            "[485/ 484] loss: 0.684488, Accuracy: 0.753196\n",
            "[486/ 485] loss: 0.684700, Accuracy: 0.753093\n",
            "[487/ 486] loss: 0.684777, Accuracy: 0.753215\n",
            "[488/ 487] loss: 0.684214, Accuracy: 0.753433\n",
            "[489/ 488] loss: 0.684359, Accuracy: 0.753362\n",
            "[490/ 489] loss: 0.684478, Accuracy: 0.753419\n",
            "[491/ 490] loss: 0.684501, Accuracy: 0.753380\n",
            "[492/ 491] loss: 0.684516, Accuracy: 0.753373\n",
            "[493/ 492] loss: 0.684452, Accuracy: 0.753335\n",
            "[494/ 493] loss: 0.683925, Accuracy: 0.753518\n",
            "[495/ 494] loss: 0.683839, Accuracy: 0.753574\n",
            "[496/ 495] loss: 0.684594, Accuracy: 0.753220\n",
            "[497/ 496] loss: 0.684645, Accuracy: 0.753213\n",
            "[498/ 497] loss: 0.684607, Accuracy: 0.753238\n",
            "[499/ 498] loss: 0.684775, Accuracy: 0.753169\n",
            "[500/ 499] loss: 0.684224, Accuracy: 0.753382\n",
            "[501/ 500] loss: 0.683579, Accuracy: 0.753594\n",
            "[502/ 501] loss: 0.683748, Accuracy: 0.753493\n",
            "[503/ 502] loss: 0.683150, Accuracy: 0.753704\n",
            "[504/ 503] loss: 0.683349, Accuracy: 0.753666\n",
            "[505/ 504] loss: 0.683076, Accuracy: 0.753751\n",
            "[506/ 505] loss: 0.682904, Accuracy: 0.753899\n",
            "[507/ 506] loss: 0.683094, Accuracy: 0.753860\n",
            "[508/ 507] loss: 0.683233, Accuracy: 0.753821\n",
            "[509/ 508] loss: 0.683592, Accuracy: 0.753660\n",
            "[510/ 509] loss: 0.683930, Accuracy: 0.753561\n",
            "[511/ 510] loss: 0.684098, Accuracy: 0.753523\n",
            "[512/ 511] loss: 0.684249, Accuracy: 0.753547\n",
            "[513/ 512] loss: 0.683855, Accuracy: 0.753693\n",
            "[514/ 513] loss: 0.684048, Accuracy: 0.753564\n",
            "[515/ 514] loss: 0.684121, Accuracy: 0.753587\n",
            "[516/ 515] loss: 0.684616, Accuracy: 0.753337\n",
            "[517/ 516] loss: 0.684663, Accuracy: 0.753270\n",
            "[518/ 517] loss: 0.685339, Accuracy: 0.753173\n",
            "[519/ 518] loss: 0.685211, Accuracy: 0.753197\n",
            "[520/ 519] loss: 0.685171, Accuracy: 0.753282\n",
            "[521/ 520] loss: 0.685392, Accuracy: 0.753185\n",
            "[522/ 521] loss: 0.685510, Accuracy: 0.753089\n",
            "[523/ 522] loss: 0.685447, Accuracy: 0.753143\n",
            "[524/ 523] loss: 0.685382, Accuracy: 0.753167\n",
            "[525/ 524] loss: 0.685130, Accuracy: 0.753250\n",
            "[526/ 525] loss: 0.686001, Accuracy: 0.752946\n",
            "[527/ 526] loss: 0.686533, Accuracy: 0.752703\n",
            "[528/ 527] loss: 0.686487, Accuracy: 0.752698\n",
            "[529/ 528] loss: 0.686553, Accuracy: 0.752693\n",
            "[530/ 529] loss: 0.686811, Accuracy: 0.752629\n",
            "[531/ 530] loss: 0.686978, Accuracy: 0.752565\n",
            "[532/ 531] loss: 0.687434, Accuracy: 0.752383\n",
            "[533/ 532] loss: 0.688027, Accuracy: 0.752085\n",
            "[534/ 533] loss: 0.688378, Accuracy: 0.751964\n",
            "[535/ 534] loss: 0.688138, Accuracy: 0.752019\n",
            "[536/ 535] loss: 0.687517, Accuracy: 0.752249\n",
            "[537/ 536] loss: 0.687311, Accuracy: 0.752274\n",
            "[538/ 537] loss: 0.687364, Accuracy: 0.752299\n",
            "[539/ 538] loss: 0.687368, Accuracy: 0.752382\n",
            "[540/ 539] loss: 0.687452, Accuracy: 0.752348\n",
            "[541/ 540] loss: 0.686838, Accuracy: 0.752604\n",
            "[542/ 541] loss: 0.687008, Accuracy: 0.752513\n",
            "[543/ 542] loss: 0.687006, Accuracy: 0.752479\n",
            "[544/ 543] loss: 0.686722, Accuracy: 0.752590\n",
            "[545/ 544] loss: 0.686378, Accuracy: 0.752700\n",
            "[546/ 545] loss: 0.686434, Accuracy: 0.752695\n",
            "[547/ 546] loss: 0.686279, Accuracy: 0.752747\n",
            "[548/ 547] loss: 0.686469, Accuracy: 0.752657\n",
            "[549/ 548] loss: 0.686819, Accuracy: 0.752566\n",
            "[550/ 549] loss: 0.687151, Accuracy: 0.752561\n",
            "[551/ 550] loss: 0.686989, Accuracy: 0.752614\n",
            "[552/ 551] loss: 0.686855, Accuracy: 0.752609\n",
            "[553/ 552] loss: 0.686803, Accuracy: 0.752604\n",
            "[554/ 553] loss: 0.686892, Accuracy: 0.752599\n",
            "[555/ 554] loss: 0.686800, Accuracy: 0.752736\n",
            "[556/ 555] loss: 0.686699, Accuracy: 0.752731\n",
            "[557/ 556] loss: 0.686747, Accuracy: 0.752642\n",
            "[558/ 557] loss: 0.686199, Accuracy: 0.752861\n",
            "[559/ 558] loss: 0.686059, Accuracy: 0.752856\n",
            "[560/ 559] loss: 0.686262, Accuracy: 0.752879\n",
            "[561/ 560] loss: 0.686232, Accuracy: 0.752902\n",
            "[562/ 561] loss: 0.686300, Accuracy: 0.752869\n",
            "[563/ 562] loss: 0.686315, Accuracy: 0.752864\n",
            "[564/ 563] loss: 0.686410, Accuracy: 0.752775\n",
            "[565/ 564] loss: 0.686415, Accuracy: 0.752770\n",
            "[566/ 565] loss: 0.686247, Accuracy: 0.752821\n",
            "[567/ 566] loss: 0.686452, Accuracy: 0.752705\n",
            "[568/ 567] loss: 0.686403, Accuracy: 0.752673\n",
            "[569/ 568] loss: 0.686430, Accuracy: 0.752723\n",
            "[570/ 569] loss: 0.686345, Accuracy: 0.752719\n",
            "[571/ 570] loss: 0.686124, Accuracy: 0.752796\n",
            "[572/ 571] loss: 0.686259, Accuracy: 0.752736\n",
            "[573/ 572] loss: 0.685955, Accuracy: 0.752896\n",
            "[574/ 573] loss: 0.685657, Accuracy: 0.753027\n",
            "[575/ 574] loss: 0.685495, Accuracy: 0.753103\n",
            "[576/ 575] loss: 0.685454, Accuracy: 0.753098\n",
            "[577/ 576] loss: 0.685335, Accuracy: 0.753147\n",
            "[578/ 577] loss: 0.685438, Accuracy: 0.753114\n",
            "[579/ 578] loss: 0.685627, Accuracy: 0.753082\n",
            "[580/ 579] loss: 0.685654, Accuracy: 0.753076\n",
            "[581/ 580] loss: 0.685764, Accuracy: 0.753071\n",
            "[582/ 581] loss: 0.685884, Accuracy: 0.753039\n",
            "[583/ 582] loss: 0.685660, Accuracy: 0.753195\n",
            "[584/ 583] loss: 0.685666, Accuracy: 0.753216\n",
            "[585/ 584] loss: 0.685731, Accuracy: 0.753211\n",
            "[586/ 585] loss: 0.685540, Accuracy: 0.753285\n",
            "[587/ 586] loss: 0.685668, Accuracy: 0.753280\n",
            "[588/ 587] loss: 0.685974, Accuracy: 0.753221\n",
            "[589/ 588] loss: 0.685898, Accuracy: 0.753242\n",
            "[590/ 589] loss: 0.686069, Accuracy: 0.753183\n",
            "[591/ 590] loss: 0.686032, Accuracy: 0.753204\n",
            "[592/ 591] loss: 0.686294, Accuracy: 0.753225\n",
            "[593/ 592] loss: 0.686112, Accuracy: 0.753246\n",
            "[594/ 593] loss: 0.686106, Accuracy: 0.753215\n",
            "[595/ 594] loss: 0.686197, Accuracy: 0.753183\n",
            "[596/ 595] loss: 0.685957, Accuracy: 0.753309\n",
            "[597/ 596] loss: 0.685891, Accuracy: 0.753303\n",
            "[598/ 597] loss: 0.686201, Accuracy: 0.753167\n",
            "[599/ 598] loss: 0.686112, Accuracy: 0.753188\n",
            "[600/ 599] loss: 0.685787, Accuracy: 0.753339\n",
            "[601 / 600]  Loss:0.685857, Accuracy:0.753333\n",
            "[601/ 600] loss: 0.685857, Accuracy: 0.753333\n",
            "[602/ 601] loss: 0.685924, Accuracy: 0.753302\n",
            "[603/ 602] loss: 0.685580, Accuracy: 0.753426\n",
            "[604/ 603] loss: 0.685872, Accuracy: 0.753291\n",
            "[605/ 604] loss: 0.685433, Accuracy: 0.753441\n",
            "[606/ 605] loss: 0.685191, Accuracy: 0.753616\n",
            "[607/ 606] loss: 0.685047, Accuracy: 0.753687\n",
            "[608/ 607] loss: 0.684897, Accuracy: 0.753707\n",
            "[609/ 608] loss: 0.684833, Accuracy: 0.753701\n",
            "[610/ 609] loss: 0.684799, Accuracy: 0.753720\n",
            "[611/ 610] loss: 0.684710, Accuracy: 0.753689\n",
            "[612/ 611] loss: 0.684483, Accuracy: 0.753708\n",
            "[613/ 612] loss: 0.684654, Accuracy: 0.753702\n",
            "[614/ 613] loss: 0.684590, Accuracy: 0.753772\n",
            "[615/ 614] loss: 0.684623, Accuracy: 0.753766\n",
            "[616/ 615] loss: 0.684598, Accuracy: 0.753709\n",
            "[617/ 616] loss: 0.684864, Accuracy: 0.753678\n",
            "[618/ 617] loss: 0.685017, Accuracy: 0.753571\n",
            "[619/ 618] loss: 0.684957, Accuracy: 0.753615\n",
            "[620/ 619] loss: 0.684970, Accuracy: 0.753635\n",
            "[621/ 620] loss: 0.685181, Accuracy: 0.753579\n",
            "[622/ 621] loss: 0.685036, Accuracy: 0.753598\n",
            "[623/ 622] loss: 0.684996, Accuracy: 0.753567\n",
            "[624/ 623] loss: 0.684859, Accuracy: 0.753637\n",
            "[625/ 624] loss: 0.684620, Accuracy: 0.753731\n",
            "[626/ 625] loss: 0.684520, Accuracy: 0.753700\n",
            "[627/ 626] loss: 0.684244, Accuracy: 0.753769\n",
            "[628/ 627] loss: 0.684346, Accuracy: 0.753738\n",
            "[629/ 628] loss: 0.684086, Accuracy: 0.753807\n",
            "[630/ 629] loss: 0.684348, Accuracy: 0.753701\n",
            "[631/ 630] loss: 0.684134, Accuracy: 0.753745\n",
            "[632/ 631] loss: 0.683957, Accuracy: 0.753838\n",
            "[633/ 632] loss: 0.683731, Accuracy: 0.753882\n",
            "[634/ 633] loss: 0.683647, Accuracy: 0.753925\n",
            "[635/ 634] loss: 0.683514, Accuracy: 0.753968\n",
            "[636/ 635] loss: 0.683296, Accuracy: 0.754060\n",
            "[637/ 636] loss: 0.683179, Accuracy: 0.754029\n",
            "[638/ 637] loss: 0.683106, Accuracy: 0.754047\n",
            "[639/ 638] loss: 0.683306, Accuracy: 0.753967\n",
            "[640/ 639] loss: 0.682993, Accuracy: 0.754084\n",
            "[641/ 640] loss: 0.682879, Accuracy: 0.754126\n",
            "[642/ 641] loss: 0.682473, Accuracy: 0.754290\n",
            "[643/ 642] loss: 0.682485, Accuracy: 0.754235\n",
            "[644/ 643] loss: 0.682298, Accuracy: 0.754277\n",
            "[645/ 644] loss: 0.682640, Accuracy: 0.754100\n",
            "[646/ 645] loss: 0.683024, Accuracy: 0.754021\n",
            "[647/ 646] loss: 0.682787, Accuracy: 0.754112\n",
            "[648/ 647] loss: 0.682597, Accuracy: 0.754250\n",
            "[649/ 648] loss: 0.682570, Accuracy: 0.754244\n",
            "[650/ 649] loss: 0.682340, Accuracy: 0.754334\n",
            "[651/ 650] loss: 0.682417, Accuracy: 0.754231\n",
            "[652/ 651] loss: 0.682482, Accuracy: 0.754224\n",
            "[653/ 652] loss: 0.682340, Accuracy: 0.754290\n",
            "[654/ 653] loss: 0.682258, Accuracy: 0.754283\n",
            "[655/ 654] loss: 0.682254, Accuracy: 0.754324\n",
            "[656/ 655] loss: 0.682025, Accuracy: 0.754389\n",
            "[657/ 656] loss: 0.681876, Accuracy: 0.754406\n",
            "[658/ 657] loss: 0.682046, Accuracy: 0.754305\n",
            "[659/ 658] loss: 0.681830, Accuracy: 0.754322\n",
            "[660/ 659] loss: 0.681824, Accuracy: 0.754292\n",
            "[661/ 660] loss: 0.681673, Accuracy: 0.754309\n",
            "[662/ 661] loss: 0.682043, Accuracy: 0.754208\n",
            "[663/ 662] loss: 0.682076, Accuracy: 0.754130\n",
            "[664/ 663] loss: 0.682397, Accuracy: 0.754006\n",
            "[665/ 664] loss: 0.682556, Accuracy: 0.753930\n",
            "[666/ 665] loss: 0.682417, Accuracy: 0.753971\n",
            "[667/ 666] loss: 0.682386, Accuracy: 0.753988\n",
            "[668/ 667] loss: 0.682280, Accuracy: 0.754029\n",
            "[669/ 668] loss: 0.682394, Accuracy: 0.754000\n",
            "[670/ 669] loss: 0.682257, Accuracy: 0.754041\n",
            "[671/ 670] loss: 0.682089, Accuracy: 0.754104\n",
            "[672/ 671] loss: 0.682064, Accuracy: 0.754098\n",
            "[673/ 672] loss: 0.681950, Accuracy: 0.754162\n",
            "[674/ 673] loss: 0.681962, Accuracy: 0.754133\n",
            "[675/ 674] loss: 0.682051, Accuracy: 0.754126\n",
            "[676/ 675] loss: 0.682109, Accuracy: 0.754144\n",
            "[677/ 676] loss: 0.681937, Accuracy: 0.754207\n",
            "[678/ 677] loss: 0.681883, Accuracy: 0.754293\n",
            "[679/ 678] loss: 0.682217, Accuracy: 0.754171\n",
            "[680/ 679] loss: 0.682126, Accuracy: 0.754165\n",
            "[681/ 680] loss: 0.682154, Accuracy: 0.754159\n",
            "[682/ 681] loss: 0.682116, Accuracy: 0.754199\n",
            "[683/ 682] loss: 0.682086, Accuracy: 0.754261\n",
            "[684/ 683] loss: 0.682222, Accuracy: 0.754164\n",
            "[685/ 684] loss: 0.682304, Accuracy: 0.754112\n",
            "[686/ 685] loss: 0.682070, Accuracy: 0.754151\n",
            "[687/ 686] loss: 0.681905, Accuracy: 0.754214\n",
            "[688/ 687] loss: 0.681891, Accuracy: 0.754208\n",
            "[689/ 688] loss: 0.682044, Accuracy: 0.754179\n",
            "[690/ 689] loss: 0.681672, Accuracy: 0.754309\n",
            "[691/ 690] loss: 0.681707, Accuracy: 0.754257\n",
            "[692/ 691] loss: 0.681739, Accuracy: 0.754228\n",
            "[693/ 692] loss: 0.681890, Accuracy: 0.754177\n",
            "[694/ 693] loss: 0.681723, Accuracy: 0.754194\n",
            "[695/ 694] loss: 0.681767, Accuracy: 0.754188\n",
            "[696/ 695] loss: 0.681904, Accuracy: 0.754204\n",
            "[697/ 696] loss: 0.682024, Accuracy: 0.754198\n",
            "[698/ 697] loss: 0.682239, Accuracy: 0.754125\n",
            "[699/ 698] loss: 0.682466, Accuracy: 0.754029\n",
            "[700/ 699] loss: 0.682588, Accuracy: 0.754024\n",
            "[701/ 700] loss: 0.682587, Accuracy: 0.754018\n",
            "[702/ 701] loss: 0.682488, Accuracy: 0.754034\n",
            "[703/ 702] loss: 0.682502, Accuracy: 0.754006\n",
            "[704/ 703] loss: 0.682200, Accuracy: 0.754134\n",
            "[705/ 704] loss: 0.681879, Accuracy: 0.754239\n",
            "[706/ 705] loss: 0.682148, Accuracy: 0.754078\n",
            "[707/ 706] loss: 0.682026, Accuracy: 0.754139\n",
            "[708/ 707] loss: 0.682126, Accuracy: 0.754111\n",
            "[709/ 708] loss: 0.681957, Accuracy: 0.754149\n",
            "[710/ 709] loss: 0.681952, Accuracy: 0.754099\n",
            "[711/ 710] loss: 0.681706, Accuracy: 0.754159\n",
            "[712/ 711] loss: 0.681676, Accuracy: 0.754197\n",
            "[713/ 712] loss: 0.681891, Accuracy: 0.754170\n",
            "[714/ 713] loss: 0.682068, Accuracy: 0.754098\n",
            "[715/ 714] loss: 0.682217, Accuracy: 0.754092\n",
            "[716/ 715] loss: 0.682697, Accuracy: 0.753934\n",
            "[717/ 716] loss: 0.682787, Accuracy: 0.753950\n",
            "[718/ 717] loss: 0.682813, Accuracy: 0.753966\n",
            "[719/ 718] loss: 0.682511, Accuracy: 0.754091\n",
            "[720/ 719] loss: 0.682477, Accuracy: 0.754064\n",
            "[721/ 720] loss: 0.682123, Accuracy: 0.754188\n",
            "[722/ 721] loss: 0.681969, Accuracy: 0.754248\n",
            "[723/ 722] loss: 0.681483, Accuracy: 0.754415\n",
            "[724/ 723] loss: 0.681353, Accuracy: 0.754474\n",
            "[725/ 724] loss: 0.681339, Accuracy: 0.754511\n",
            "[726/ 725] loss: 0.681661, Accuracy: 0.754440\n",
            "[727/ 726] loss: 0.681494, Accuracy: 0.754455\n",
            "[728/ 727] loss: 0.681131, Accuracy: 0.754578\n",
            "[729/ 728] loss: 0.681139, Accuracy: 0.754572\n",
            "[730/ 729] loss: 0.681195, Accuracy: 0.754522\n",
            "[731/ 730] loss: 0.681337, Accuracy: 0.754431\n",
            "[732/ 731] loss: 0.681340, Accuracy: 0.754467\n",
            "[733/ 732] loss: 0.681135, Accuracy: 0.754525\n",
            "[734/ 733] loss: 0.681084, Accuracy: 0.754562\n",
            "[735/ 734] loss: 0.681256, Accuracy: 0.754470\n",
            "[736/ 735] loss: 0.681059, Accuracy: 0.754571\n",
            "[737/ 736] loss: 0.681204, Accuracy: 0.754564\n",
            "[738/ 737] loss: 0.681248, Accuracy: 0.754558\n",
            "[739/ 738] loss: 0.681431, Accuracy: 0.754488\n",
            "[740/ 739] loss: 0.681420, Accuracy: 0.754504\n",
            "[741/ 740] loss: 0.681188, Accuracy: 0.754582\n",
            "[742/ 741] loss: 0.681000, Accuracy: 0.754618\n",
            "[743/ 742] loss: 0.680777, Accuracy: 0.754717\n",
            "[744/ 743] loss: 0.681010, Accuracy: 0.754648\n",
            "[745/ 744] loss: 0.680626, Accuracy: 0.754788\n",
            "[746/ 745] loss: 0.680997, Accuracy: 0.754677\n",
            "[747/ 746] loss: 0.681043, Accuracy: 0.754650\n",
            "[748/ 747] loss: 0.681217, Accuracy: 0.754602\n",
            "[749/ 748] loss: 0.681082, Accuracy: 0.754658\n",
            "[750/ 749] loss: 0.681002, Accuracy: 0.754694\n",
            "[751/ 750] loss: 0.681236, Accuracy: 0.754625\n",
            "[752/ 751] loss: 0.681475, Accuracy: 0.754515\n",
            "[753/ 752] loss: 0.681661, Accuracy: 0.754405\n",
            "[754/ 753] loss: 0.681395, Accuracy: 0.754482\n",
            "[755/ 754] loss: 0.681285, Accuracy: 0.754497\n",
            "[756/ 755] loss: 0.681574, Accuracy: 0.754429\n",
            "[757/ 756] loss: 0.681484, Accuracy: 0.754464\n",
            "[758/ 757] loss: 0.681653, Accuracy: 0.754355\n",
            "[759/ 758] loss: 0.681578, Accuracy: 0.754391\n",
            "[760/ 759] loss: 0.681551, Accuracy: 0.754385\n",
            "[761/ 760] loss: 0.681541, Accuracy: 0.754379\n",
            "[762/ 761] loss: 0.681390, Accuracy: 0.754456\n",
            "[763/ 762] loss: 0.681387, Accuracy: 0.754470\n",
            "[764/ 763] loss: 0.681386, Accuracy: 0.754485\n",
            "[765/ 764] loss: 0.681192, Accuracy: 0.754540\n",
            "[766/ 765] loss: 0.681343, Accuracy: 0.754493\n",
            "[767/ 766] loss: 0.681571, Accuracy: 0.754345\n",
            "[768/ 767] loss: 0.681762, Accuracy: 0.754278\n",
            "[769/ 768] loss: 0.681716, Accuracy: 0.754252\n",
            "[770/ 769] loss: 0.681513, Accuracy: 0.754308\n",
            "[771/ 770] loss: 0.681257, Accuracy: 0.754383\n",
            "[772/ 771] loss: 0.680897, Accuracy: 0.754479\n",
            "[773/ 772] loss: 0.680593, Accuracy: 0.754615\n",
            "[774/ 773] loss: 0.680829, Accuracy: 0.754508\n",
            "[775/ 774] loss: 0.680662, Accuracy: 0.754542\n",
            "[776/ 775] loss: 0.681067, Accuracy: 0.754415\n",
            "[777/ 776] loss: 0.681236, Accuracy: 0.754349\n",
            "[778/ 777] loss: 0.680973, Accuracy: 0.754404\n",
            "[779/ 778] loss: 0.681049, Accuracy: 0.754358\n",
            "[780/ 779] loss: 0.681412, Accuracy: 0.754192\n",
            "[781/ 780] loss: 0.681590, Accuracy: 0.754107\n",
            "[782/ 781] loss: 0.681610, Accuracy: 0.754101\n",
            "[783/ 782] loss: 0.681714, Accuracy: 0.754036\n",
            "[784/ 783] loss: 0.681963, Accuracy: 0.753991\n",
            "[785/ 784] loss: 0.681752, Accuracy: 0.754066\n",
            "[786/ 785] loss: 0.682126, Accuracy: 0.753921\n",
            "[787/ 786] loss: 0.682575, Accuracy: 0.753698\n",
            "[788/ 787] loss: 0.682540, Accuracy: 0.753693\n",
            "[789/ 788] loss: 0.682500, Accuracy: 0.753728\n",
            "[790/ 789] loss: 0.682567, Accuracy: 0.753763\n",
            "[791/ 790] loss: 0.682503, Accuracy: 0.753758\n",
            "[792/ 791] loss: 0.682542, Accuracy: 0.753793\n",
            "[793/ 792] loss: 0.682662, Accuracy: 0.753768\n",
            "[794/ 793] loss: 0.683083, Accuracy: 0.753606\n",
            "[795/ 794] loss: 0.682921, Accuracy: 0.753680\n",
            "[796/ 795] loss: 0.683368, Accuracy: 0.753498\n",
            "[797/ 796] loss: 0.683400, Accuracy: 0.753474\n",
            "[798/ 797] loss: 0.683945, Accuracy: 0.753294\n",
            "[799/ 798] loss: 0.683899, Accuracy: 0.753309\n",
            "[800/ 799] loss: 0.683863, Accuracy: 0.753285\n",
            "[801/ 800] loss: 0.683947, Accuracy: 0.753262\n",
            "[802/ 801] loss: 0.683783, Accuracy: 0.753277\n",
            "[803/ 802] loss: 0.684160, Accuracy: 0.753156\n",
            "[804/ 803] loss: 0.684251, Accuracy: 0.753133\n",
            "[805/ 804] loss: 0.684257, Accuracy: 0.753051\n",
            "[806/ 805] loss: 0.684123, Accuracy: 0.753086\n",
            "[807/ 806] loss: 0.684158, Accuracy: 0.753121\n",
            "[808/ 807] loss: 0.683974, Accuracy: 0.753214\n",
            "[809/ 808] loss: 0.683948, Accuracy: 0.753210\n",
            "[810/ 809] loss: 0.683467, Accuracy: 0.753399\n",
            "[811/ 810] loss: 0.683515, Accuracy: 0.753395\n",
            "[812/ 811] loss: 0.683253, Accuracy: 0.753487\n",
            "[813/ 812] loss: 0.683341, Accuracy: 0.753444\n",
            "[814/ 813] loss: 0.683246, Accuracy: 0.753459\n",
            "[815/ 814] loss: 0.682882, Accuracy: 0.753590\n",
            "[816/ 815] loss: 0.682659, Accuracy: 0.753643\n",
            "[817/ 816] loss: 0.682572, Accuracy: 0.753676\n",
            "[818/ 817] loss: 0.682621, Accuracy: 0.753672\n",
            "[819/ 818] loss: 0.682668, Accuracy: 0.753629\n",
            "[820/ 819] loss: 0.682750, Accuracy: 0.753625\n",
            "[821/ 820] loss: 0.683105, Accuracy: 0.753449\n",
            "[822/ 821] loss: 0.682706, Accuracy: 0.753597\n",
            "[823/ 822] loss: 0.682689, Accuracy: 0.753650\n",
            "[824/ 823] loss: 0.682466, Accuracy: 0.753721\n",
            "[825/ 824] loss: 0.682666, Accuracy: 0.753717\n",
            "[826/ 825] loss: 0.682631, Accuracy: 0.753731\n",
            "[827/ 826] loss: 0.682606, Accuracy: 0.753764\n",
            "[828/ 827] loss: 0.682380, Accuracy: 0.753854\n",
            "[829/ 828] loss: 0.682555, Accuracy: 0.753774\n",
            "[830/ 829] loss: 0.682862, Accuracy: 0.753675\n",
            "[831/ 830] loss: 0.682850, Accuracy: 0.753671\n",
            "[832/ 831] loss: 0.682886, Accuracy: 0.753629\n",
            "[833/ 832] loss: 0.682787, Accuracy: 0.753700\n",
            "[834/ 833] loss: 0.682700, Accuracy: 0.753752\n",
            "[835/ 834] loss: 0.682753, Accuracy: 0.753710\n",
            "[836/ 835] loss: 0.683101, Accuracy: 0.753649\n",
            "[837/ 836] loss: 0.682812, Accuracy: 0.753775\n",
            "[838/ 837] loss: 0.682931, Accuracy: 0.753790\n",
            "[839/ 838] loss: 0.682948, Accuracy: 0.753785\n",
            "[840/ 839] loss: 0.682640, Accuracy: 0.753911\n",
            "[841/ 840] loss: 0.682697, Accuracy: 0.753888\n",
            "[842/ 841] loss: 0.682845, Accuracy: 0.753827\n",
            "[843/ 842] loss: 0.682821, Accuracy: 0.753841\n",
            "[844/ 843] loss: 0.682857, Accuracy: 0.753800\n",
            "[845/ 844] loss: 0.682888, Accuracy: 0.753795\n",
            "[846/ 845] loss: 0.682777, Accuracy: 0.753828\n",
            "[847/ 846] loss: 0.682967, Accuracy: 0.753823\n",
            "[848/ 847] loss: 0.682925, Accuracy: 0.753837\n",
            "[849/ 848] loss: 0.683001, Accuracy: 0.753796\n",
            "[850/ 849] loss: 0.683007, Accuracy: 0.753810\n",
            "[851/ 850] loss: 0.683273, Accuracy: 0.753713\n",
            "[852/ 851] loss: 0.683380, Accuracy: 0.753672\n",
            "[853/ 852] loss: 0.683361, Accuracy: 0.753668\n",
            "[854/ 853] loss: 0.683640, Accuracy: 0.753517\n",
            "[855/ 854] loss: 0.683712, Accuracy: 0.753476\n",
            "[856/ 855] loss: 0.683849, Accuracy: 0.753417\n",
            "[857/ 856] loss: 0.683789, Accuracy: 0.753432\n",
            "[858/ 857] loss: 0.683587, Accuracy: 0.753501\n",
            "[859/ 858] loss: 0.683754, Accuracy: 0.753442\n",
            "[860/ 859] loss: 0.683875, Accuracy: 0.753401\n",
            "[861/ 860] loss: 0.683816, Accuracy: 0.753416\n",
            "[862/ 861] loss: 0.683914, Accuracy: 0.753412\n",
            "[863/ 862] loss: 0.683695, Accuracy: 0.753462\n",
            "[864/ 863] loss: 0.683640, Accuracy: 0.753494\n",
            "[865/ 864] loss: 0.683899, Accuracy: 0.753364\n",
            "[866/ 865] loss: 0.683695, Accuracy: 0.753486\n",
            "[867/ 866] loss: 0.683587, Accuracy: 0.753536\n",
            "[868/ 867] loss: 0.683497, Accuracy: 0.753568\n",
            "[869/ 868] loss: 0.683568, Accuracy: 0.753564\n",
            "[870/ 869] loss: 0.683634, Accuracy: 0.753596\n",
            "[871/ 870] loss: 0.683898, Accuracy: 0.753484\n",
            "[872/ 871] loss: 0.683940, Accuracy: 0.753444\n",
            "[873/ 872] loss: 0.683825, Accuracy: 0.753494\n",
            "[874/ 873] loss: 0.683721, Accuracy: 0.753580\n",
            "[875/ 874] loss: 0.683707, Accuracy: 0.753558\n",
            "[876/ 875] loss: 0.683705, Accuracy: 0.753536\n",
            "[877/ 876] loss: 0.683684, Accuracy: 0.753532\n",
            "[878/ 877] loss: 0.683634, Accuracy: 0.753528\n",
            "[879/ 878] loss: 0.683507, Accuracy: 0.753559\n",
            "[880/ 879] loss: 0.683725, Accuracy: 0.753484\n",
            "[881/ 880] loss: 0.683766, Accuracy: 0.753462\n",
            "[882/ 881] loss: 0.683788, Accuracy: 0.753441\n",
            "[883/ 882] loss: 0.683853, Accuracy: 0.753401\n",
            "[884/ 883] loss: 0.684001, Accuracy: 0.753327\n",
            "[885/ 884] loss: 0.683939, Accuracy: 0.753341\n",
            "[886/ 885] loss: 0.684111, Accuracy: 0.753284\n",
            "[887/ 886] loss: 0.684199, Accuracy: 0.753227\n",
            "[888/ 887] loss: 0.684005, Accuracy: 0.753259\n",
            "[889/ 888] loss: 0.684014, Accuracy: 0.753290\n",
            "[890/ 889] loss: 0.684126, Accuracy: 0.753252\n",
            "[891/ 890] loss: 0.684152, Accuracy: 0.753213\n",
            "[892/ 891] loss: 0.683902, Accuracy: 0.753314\n",
            "[893/ 892] loss: 0.683715, Accuracy: 0.753363\n",
            "[894/ 893] loss: 0.683459, Accuracy: 0.753464\n",
            "[895/ 894] loss: 0.683558, Accuracy: 0.753356\n",
            "[896/ 895] loss: 0.683552, Accuracy: 0.753369\n",
            "[897/ 896] loss: 0.683550, Accuracy: 0.753366\n",
            "[898/ 897] loss: 0.683615, Accuracy: 0.753327\n",
            "[899/ 898] loss: 0.683398, Accuracy: 0.753393\n",
            "[900/ 899] loss: 0.683345, Accuracy: 0.753389\n",
            "[901 / 900]  Loss:0.683302, Accuracy:0.753385\n",
            "[901/ 900] loss: 0.683302, Accuracy: 0.753385\n",
            "[902/ 901] loss: 0.683320, Accuracy: 0.753364\n",
            "[903/ 902] loss: 0.683086, Accuracy: 0.753430\n",
            "[904/ 903] loss: 0.683155, Accuracy: 0.753409\n",
            "[905/ 904] loss: 0.683257, Accuracy: 0.753370\n",
            "[906/ 905] loss: 0.683397, Accuracy: 0.753315\n",
            "[907/ 906] loss: 0.683274, Accuracy: 0.753329\n",
            "[908/ 907] loss: 0.683229, Accuracy: 0.753290\n",
            "[909/ 908] loss: 0.683460, Accuracy: 0.753201\n",
            "[910/ 909] loss: 0.683851, Accuracy: 0.753043\n",
            "[911/ 910] loss: 0.683933, Accuracy: 0.753022\n",
            "[912/ 911] loss: 0.683956, Accuracy: 0.753019\n",
            "[913/ 912] loss: 0.683917, Accuracy: 0.753050\n",
            "[914/ 913] loss: 0.684043, Accuracy: 0.752995\n",
            "[915/ 914] loss: 0.684280, Accuracy: 0.752855\n",
            "[916/ 915] loss: 0.684342, Accuracy: 0.752835\n",
            "[917/ 916] loss: 0.684621, Accuracy: 0.752695\n",
            "[918/ 917] loss: 0.684843, Accuracy: 0.752573\n",
            "[919/ 918] loss: 0.684809, Accuracy: 0.752570\n",
            "[920/ 919] loss: 0.684755, Accuracy: 0.752550\n",
            "[921/ 920] loss: 0.684540, Accuracy: 0.752616\n",
            "[922/ 921] loss: 0.684555, Accuracy: 0.752579\n",
            "[923/ 922] loss: 0.684318, Accuracy: 0.752661\n",
            "[924/ 923] loss: 0.684465, Accuracy: 0.752607\n",
            "[925/ 924] loss: 0.684763, Accuracy: 0.752452\n",
            "[926/ 925] loss: 0.684600, Accuracy: 0.752483\n",
            "[927/ 926] loss: 0.684452, Accuracy: 0.752531\n",
            "[928/ 927] loss: 0.684567, Accuracy: 0.752528\n",
            "[929/ 928] loss: 0.684315, Accuracy: 0.752610\n",
            "[930/ 929] loss: 0.684208, Accuracy: 0.752641\n",
            "[931/ 930] loss: 0.684241, Accuracy: 0.752638\n",
            "[932/ 931] loss: 0.684386, Accuracy: 0.752601\n",
            "[933/ 932] loss: 0.684207, Accuracy: 0.752666\n",
            "[934/ 933] loss: 0.684340, Accuracy: 0.752613\n",
            "[935/ 934] loss: 0.684498, Accuracy: 0.752576\n",
            "[936/ 935] loss: 0.684633, Accuracy: 0.752507\n",
            "[937/ 936] loss: 0.684761, Accuracy: 0.752487\n",
            "[938/ 937] loss: 0.684746, Accuracy: 0.752435\n",
            "[939/ 938] loss: 0.684715, Accuracy: 0.752399\n",
            "epochs 10\n",
            "[2/ 1] loss: 0.652023, Accuracy: 0.765625\n",
            "[3/ 2] loss: 0.704233, Accuracy: 0.765625\n",
            "[4/ 3] loss: 0.625106, Accuracy: 0.786458\n",
            "[5/ 4] loss: 0.620055, Accuracy: 0.785156\n",
            "[6/ 5] loss: 0.643204, Accuracy: 0.775000\n",
            "[7/ 6] loss: 0.683001, Accuracy: 0.750000\n",
            "[8/ 7] loss: 0.700823, Accuracy: 0.745536\n",
            "[9/ 8] loss: 0.682552, Accuracy: 0.751953\n",
            "[10/ 9] loss: 0.668338, Accuracy: 0.755208\n",
            "[11/ 10] loss: 0.687435, Accuracy: 0.751562\n",
            "[12/ 11] loss: 0.677023, Accuracy: 0.757102\n",
            "[13/ 12] loss: 0.691669, Accuracy: 0.750000\n",
            "[14/ 13] loss: 0.701357, Accuracy: 0.743990\n",
            "[15/ 14] loss: 0.695092, Accuracy: 0.745536\n",
            "[16/ 15] loss: 0.707140, Accuracy: 0.740625\n",
            "[17/ 16] loss: 0.698891, Accuracy: 0.743164\n",
            "[18/ 17] loss: 0.689810, Accuracy: 0.745404\n",
            "[19/ 18] loss: 0.685269, Accuracy: 0.748264\n",
            "[20/ 19] loss: 0.684199, Accuracy: 0.748355\n",
            "[21/ 20] loss: 0.688506, Accuracy: 0.746094\n",
            "[22/ 21] loss: 0.694739, Accuracy: 0.744792\n",
            "[23/ 22] loss: 0.696265, Accuracy: 0.743608\n",
            "[24/ 23] loss: 0.696559, Accuracy: 0.743886\n",
            "[25/ 24] loss: 0.705873, Accuracy: 0.740885\n",
            "[26/ 25] loss: 0.711258, Accuracy: 0.737500\n",
            "[27/ 26] loss: 0.722735, Accuracy: 0.733774\n",
            "[28/ 27] loss: 0.718812, Accuracy: 0.734954\n",
            "[29/ 28] loss: 0.717197, Accuracy: 0.735491\n",
            "[30/ 29] loss: 0.715094, Accuracy: 0.736530\n",
            "[31/ 30] loss: 0.713759, Accuracy: 0.738542\n",
            "[32/ 31] loss: 0.704763, Accuracy: 0.741431\n",
            "[33/ 32] loss: 0.705541, Accuracy: 0.741699\n",
            "[34/ 33] loss: 0.709086, Accuracy: 0.740057\n",
            "[35/ 34] loss: 0.703669, Accuracy: 0.741728\n",
            "[36/ 35] loss: 0.702802, Accuracy: 0.741518\n",
            "[37/ 36] loss: 0.700618, Accuracy: 0.743056\n",
            "[38/ 37] loss: 0.703293, Accuracy: 0.741554\n",
            "[39/ 38] loss: 0.704963, Accuracy: 0.741365\n",
            "[40/ 39] loss: 0.704357, Accuracy: 0.742388\n",
            "[41/ 40] loss: 0.705793, Accuracy: 0.741406\n",
            "[42/ 41] loss: 0.700481, Accuracy: 0.743902\n",
            "[43/ 42] loss: 0.702205, Accuracy: 0.744420\n",
            "[44/ 43] loss: 0.700445, Accuracy: 0.744913\n",
            "[45/ 44] loss: 0.701643, Accuracy: 0.743608\n",
            "[46/ 45] loss: 0.698681, Accuracy: 0.744792\n",
            "[47/ 46] loss: 0.703129, Accuracy: 0.742867\n",
            "[48/ 47] loss: 0.707620, Accuracy: 0.740359\n",
            "[49/ 48] loss: 0.705650, Accuracy: 0.741211\n",
            "[50/ 49] loss: 0.700608, Accuracy: 0.742666\n",
            "[51/ 50] loss: 0.702174, Accuracy: 0.741875\n",
            "[52/ 51] loss: 0.701801, Accuracy: 0.741728\n",
            "[53/ 52] loss: 0.702545, Accuracy: 0.740986\n",
            "[54/ 53] loss: 0.703713, Accuracy: 0.740566\n",
            "[55/ 54] loss: 0.703508, Accuracy: 0.741030\n",
            "[56/ 55] loss: 0.701221, Accuracy: 0.742045\n",
            "[57/ 56] loss: 0.700556, Accuracy: 0.742746\n",
            "[58/ 57] loss: 0.696271, Accuracy: 0.744518\n",
            "[59/ 58] loss: 0.695199, Accuracy: 0.745420\n",
            "[60/ 59] loss: 0.695743, Accuracy: 0.745233\n",
            "[61/ 60] loss: 0.698850, Accuracy: 0.744271\n",
            "[62/ 61] loss: 0.698473, Accuracy: 0.744877\n",
            "[63/ 62] loss: 0.700801, Accuracy: 0.744204\n",
            "[64/ 63] loss: 0.701627, Accuracy: 0.744296\n",
            "[65/ 64] loss: 0.699155, Accuracy: 0.745117\n",
            "[66/ 65] loss: 0.702187, Accuracy: 0.744471\n",
            "[67/ 66] loss: 0.705682, Accuracy: 0.743134\n",
            "[68/ 67] loss: 0.705458, Accuracy: 0.743004\n",
            "[69/ 68] loss: 0.705063, Accuracy: 0.743336\n",
            "[70/ 69] loss: 0.705046, Accuracy: 0.743886\n",
            "[71/ 70] loss: 0.703795, Accuracy: 0.744420\n",
            "[72/ 71] loss: 0.706716, Accuracy: 0.743398\n",
            "[73/ 72] loss: 0.705780, Accuracy: 0.743490\n",
            "[74/ 73] loss: 0.703844, Accuracy: 0.744221\n",
            "[75/ 74] loss: 0.705426, Accuracy: 0.743877\n",
            "[76/ 75] loss: 0.707161, Accuracy: 0.742708\n",
            "[77/ 76] loss: 0.706427, Accuracy: 0.742599\n",
            "[78/ 77] loss: 0.704220, Accuracy: 0.742898\n",
            "[79/ 78] loss: 0.703053, Accuracy: 0.743389\n",
            "[80/ 79] loss: 0.706937, Accuracy: 0.741495\n",
            "[81/ 80] loss: 0.708141, Accuracy: 0.740820\n",
            "[82/ 81] loss: 0.706991, Accuracy: 0.740934\n",
            "[83/ 82] loss: 0.707099, Accuracy: 0.741044\n",
            "[84/ 83] loss: 0.705952, Accuracy: 0.740964\n",
            "[85/ 84] loss: 0.708094, Accuracy: 0.740513\n",
            "[86/ 85] loss: 0.708175, Accuracy: 0.740625\n",
            "[87/ 86] loss: 0.705657, Accuracy: 0.741642\n",
            "[88/ 87] loss: 0.705334, Accuracy: 0.741559\n",
            "[89/ 88] loss: 0.706966, Accuracy: 0.741300\n",
            "[90/ 89] loss: 0.708048, Accuracy: 0.740695\n",
            "[91/ 90] loss: 0.708132, Accuracy: 0.740625\n",
            "[92/ 91] loss: 0.708720, Accuracy: 0.740213\n",
            "[93/ 92] loss: 0.707816, Accuracy: 0.740319\n",
            "[94/ 93] loss: 0.707489, Accuracy: 0.740255\n",
            "[95/ 94] loss: 0.703567, Accuracy: 0.741689\n",
            "[96/ 95] loss: 0.704314, Accuracy: 0.741776\n",
            "[97/ 96] loss: 0.703578, Accuracy: 0.742025\n",
            "[98/ 97] loss: 0.703034, Accuracy: 0.742268\n",
            "[99/ 98] loss: 0.705094, Accuracy: 0.740912\n",
            "[100/ 99] loss: 0.704578, Accuracy: 0.741162\n",
            "[101/ 100] loss: 0.702790, Accuracy: 0.741875\n",
            "[102/ 101] loss: 0.702333, Accuracy: 0.742110\n",
            "[103/ 102] loss: 0.703346, Accuracy: 0.741575\n",
            "[104/ 103] loss: 0.703182, Accuracy: 0.741657\n",
            "[105/ 104] loss: 0.700925, Accuracy: 0.742638\n",
            "[106/ 105] loss: 0.700146, Accuracy: 0.742857\n",
            "[107/ 106] loss: 0.700671, Accuracy: 0.742630\n",
            "[108/ 107] loss: 0.701864, Accuracy: 0.742407\n",
            "[109/ 108] loss: 0.702265, Accuracy: 0.742766\n",
            "[110/ 109] loss: 0.700195, Accuracy: 0.743549\n",
            "[111/ 110] loss: 0.700949, Accuracy: 0.743182\n",
            "[112/ 111] loss: 0.700549, Accuracy: 0.743243\n",
            "[113/ 112] loss: 0.700415, Accuracy: 0.743304\n",
            "[114/ 113] loss: 0.699699, Accuracy: 0.743501\n",
            "[115/ 114] loss: 0.699705, Accuracy: 0.743421\n",
            "[116/ 115] loss: 0.697420, Accuracy: 0.744293\n",
            "[117/ 116] loss: 0.696167, Accuracy: 0.744747\n",
            "[118/ 117] loss: 0.695058, Accuracy: 0.744925\n",
            "[119/ 118] loss: 0.695025, Accuracy: 0.744703\n",
            "[120/ 119] loss: 0.696277, Accuracy: 0.744617\n",
            "[121/ 120] loss: 0.694625, Accuracy: 0.745182\n",
            "[122/ 121] loss: 0.694131, Accuracy: 0.745480\n",
            "[123/ 122] loss: 0.697112, Accuracy: 0.744877\n",
            "[124/ 123] loss: 0.698109, Accuracy: 0.744411\n",
            "[125/ 124] loss: 0.698477, Accuracy: 0.744204\n",
            "[126/ 125] loss: 0.698048, Accuracy: 0.744375\n",
            "[127/ 126] loss: 0.697504, Accuracy: 0.744544\n",
            "[128/ 127] loss: 0.695560, Accuracy: 0.745325\n",
            "[129/ 128] loss: 0.694837, Accuracy: 0.745605\n",
            "[130/ 129] loss: 0.692563, Accuracy: 0.746609\n",
            "[131/ 130] loss: 0.694498, Accuracy: 0.746274\n",
            "[132/ 131] loss: 0.694839, Accuracy: 0.746183\n",
            "[133/ 132] loss: 0.695010, Accuracy: 0.746094\n",
            "[134/ 133] loss: 0.693843, Accuracy: 0.746711\n",
            "[135/ 134] loss: 0.693090, Accuracy: 0.747085\n",
            "[136/ 135] loss: 0.692970, Accuracy: 0.746991\n",
            "[137/ 136] loss: 0.692909, Accuracy: 0.746783\n",
            "[138/ 137] loss: 0.692887, Accuracy: 0.746578\n",
            "[139/ 138] loss: 0.692511, Accuracy: 0.746716\n",
            "[140/ 139] loss: 0.692455, Accuracy: 0.746965\n",
            "[141/ 140] loss: 0.691735, Accuracy: 0.747210\n",
            "[142/ 141] loss: 0.690865, Accuracy: 0.747451\n",
            "[143/ 142] loss: 0.692496, Accuracy: 0.747029\n",
            "[144/ 143] loss: 0.691191, Accuracy: 0.747487\n",
            "[145/ 144] loss: 0.690698, Accuracy: 0.747721\n",
            "[146/ 145] loss: 0.691765, Accuracy: 0.747629\n",
            "[147/ 146] loss: 0.691664, Accuracy: 0.747860\n",
            "[148/ 147] loss: 0.692417, Accuracy: 0.747662\n",
            "[149/ 148] loss: 0.693280, Accuracy: 0.747149\n",
            "[150/ 149] loss: 0.693489, Accuracy: 0.747064\n",
            "[151/ 150] loss: 0.693247, Accuracy: 0.747187\n",
            "[152/ 151] loss: 0.691236, Accuracy: 0.747827\n",
            "[153/ 152] loss: 0.692683, Accuracy: 0.747430\n",
            "[154/ 153] loss: 0.690655, Accuracy: 0.748264\n",
            "[155/ 154] loss: 0.690760, Accuracy: 0.748377\n",
            "[156/ 155] loss: 0.690795, Accuracy: 0.748488\n",
            "[157/ 156] loss: 0.691986, Accuracy: 0.748197\n",
            "[158/ 157] loss: 0.692357, Accuracy: 0.748109\n",
            "[159/ 158] loss: 0.691599, Accuracy: 0.748319\n",
            "[160/ 159] loss: 0.693005, Accuracy: 0.747740\n",
            "[161/ 160] loss: 0.693847, Accuracy: 0.747363\n",
            "[162/ 161] loss: 0.693495, Accuracy: 0.747477\n",
            "[163/ 162] loss: 0.693246, Accuracy: 0.747685\n",
            "[164/ 163] loss: 0.692469, Accuracy: 0.748179\n",
            "[165/ 164] loss: 0.692990, Accuracy: 0.747904\n",
            "[166/ 165] loss: 0.693356, Accuracy: 0.747633\n",
            "[167/ 166] loss: 0.693770, Accuracy: 0.747364\n",
            "[168/ 167] loss: 0.693847, Accuracy: 0.747287\n",
            "[169/ 168] loss: 0.695028, Accuracy: 0.746745\n",
            "[170/ 169] loss: 0.694987, Accuracy: 0.746672\n",
            "[171/ 170] loss: 0.693666, Accuracy: 0.747151\n",
            "[172/ 171] loss: 0.694602, Accuracy: 0.747076\n",
            "[173/ 172] loss: 0.694344, Accuracy: 0.746911\n",
            "[174/ 173] loss: 0.694532, Accuracy: 0.746658\n",
            "[175/ 174] loss: 0.694530, Accuracy: 0.746408\n",
            "[176/ 175] loss: 0.695266, Accuracy: 0.746161\n",
            "[177/ 176] loss: 0.695171, Accuracy: 0.746271\n",
            "[178/ 177] loss: 0.694400, Accuracy: 0.746557\n",
            "[179/ 178] loss: 0.694186, Accuracy: 0.746752\n",
            "[180/ 179] loss: 0.695076, Accuracy: 0.746421\n",
            "[181/ 180] loss: 0.695327, Accuracy: 0.746267\n",
            "[182/ 181] loss: 0.695555, Accuracy: 0.746202\n",
            "[183/ 182] loss: 0.696164, Accuracy: 0.746137\n",
            "[184/ 183] loss: 0.695445, Accuracy: 0.746414\n",
            "[185/ 184] loss: 0.695763, Accuracy: 0.746179\n",
            "[186/ 185] loss: 0.695904, Accuracy: 0.746284\n",
            "[187/ 186] loss: 0.696889, Accuracy: 0.746136\n",
            "[188/ 187] loss: 0.698430, Accuracy: 0.745822\n",
            "[189/ 188] loss: 0.697184, Accuracy: 0.746260\n",
            "[190/ 189] loss: 0.697113, Accuracy: 0.746114\n",
            "[191/ 190] loss: 0.696792, Accuracy: 0.746299\n",
            "[192/ 191] loss: 0.696202, Accuracy: 0.746646\n",
            "[193/ 192] loss: 0.695954, Accuracy: 0.746582\n",
            "[194/ 193] loss: 0.696302, Accuracy: 0.746519\n",
            "[195/ 194] loss: 0.696462, Accuracy: 0.746376\n",
            "[196/ 195] loss: 0.696628, Accuracy: 0.746314\n",
            "[197/ 196] loss: 0.696466, Accuracy: 0.746333\n",
            "[198/ 197] loss: 0.695859, Accuracy: 0.746510\n",
            "[199/ 198] loss: 0.694984, Accuracy: 0.746765\n",
            "[200/ 199] loss: 0.694541, Accuracy: 0.746859\n",
            "[201/ 200] loss: 0.694866, Accuracy: 0.746641\n",
            "[202/ 201] loss: 0.694191, Accuracy: 0.746891\n",
            "[203/ 202] loss: 0.694012, Accuracy: 0.747061\n",
            "[204/ 203] loss: 0.694007, Accuracy: 0.746998\n",
            "[205/ 204] loss: 0.693773, Accuracy: 0.747166\n",
            "[206/ 205] loss: 0.693869, Accuracy: 0.747104\n",
            "[207/ 206] loss: 0.694059, Accuracy: 0.747118\n",
            "[208/ 207] loss: 0.693292, Accuracy: 0.747434\n",
            "[209/ 208] loss: 0.692841, Accuracy: 0.747596\n",
            "[210/ 209] loss: 0.692403, Accuracy: 0.747682\n",
            "[211/ 210] loss: 0.692239, Accuracy: 0.747693\n",
            "[212/ 211] loss: 0.692102, Accuracy: 0.747778\n",
            "[213/ 212] loss: 0.692619, Accuracy: 0.747568\n",
            "[214/ 213] loss: 0.693041, Accuracy: 0.747579\n",
            "[215/ 214] loss: 0.693573, Accuracy: 0.747152\n",
            "[216/ 215] loss: 0.693690, Accuracy: 0.747093\n",
            "[217/ 216] loss: 0.693155, Accuracy: 0.747251\n",
            "[218/ 217] loss: 0.692962, Accuracy: 0.747192\n",
            "[219/ 218] loss: 0.693105, Accuracy: 0.747133\n",
            "[220/ 219] loss: 0.693353, Accuracy: 0.746932\n",
            "[221/ 220] loss: 0.693155, Accuracy: 0.747017\n",
            "[222/ 221] loss: 0.693555, Accuracy: 0.746960\n",
            "[223/ 222] loss: 0.693667, Accuracy: 0.746974\n",
            "[224/ 223] loss: 0.693576, Accuracy: 0.746917\n",
            "[225/ 224] loss: 0.692564, Accuracy: 0.747280\n",
            "[226/ 225] loss: 0.692017, Accuracy: 0.747431\n",
            "[227/ 226] loss: 0.690946, Accuracy: 0.747788\n",
            "[228/ 227] loss: 0.690038, Accuracy: 0.748210\n",
            "[229/ 228] loss: 0.689819, Accuracy: 0.748287\n",
            "[230/ 229] loss: 0.689238, Accuracy: 0.748499\n",
            "[231/ 230] loss: 0.688219, Accuracy: 0.748913\n",
            "[232/ 231] loss: 0.689210, Accuracy: 0.748647\n",
            "[233/ 232] loss: 0.688225, Accuracy: 0.749124\n",
            "[234/ 233] loss: 0.688757, Accuracy: 0.748793\n",
            "[235/ 234] loss: 0.689357, Accuracy: 0.748598\n",
            "[236/ 235] loss: 0.689468, Accuracy: 0.748670\n",
            "[237/ 236] loss: 0.688912, Accuracy: 0.748676\n",
            "[238/ 237] loss: 0.688595, Accuracy: 0.748879\n",
            "[239/ 238] loss: 0.689185, Accuracy: 0.748818\n",
            "[240/ 239] loss: 0.689308, Accuracy: 0.748758\n",
            "[241/ 240] loss: 0.689300, Accuracy: 0.748698\n",
            "[242/ 241] loss: 0.688820, Accuracy: 0.748963\n",
            "[243/ 242] loss: 0.691541, Accuracy: 0.748192\n",
            "[244/ 243] loss: 0.691223, Accuracy: 0.748135\n",
            "[245/ 244] loss: 0.690674, Accuracy: 0.748335\n",
            "[246/ 245] loss: 0.690344, Accuracy: 0.748406\n",
            "[247/ 246] loss: 0.690338, Accuracy: 0.748476\n",
            "[248/ 247] loss: 0.689835, Accuracy: 0.748608\n",
            "[249/ 248] loss: 0.689796, Accuracy: 0.748614\n",
            "[250/ 249] loss: 0.689042, Accuracy: 0.748870\n",
            "[251/ 250] loss: 0.690125, Accuracy: 0.748500\n",
            "[252/ 251] loss: 0.689725, Accuracy: 0.748630\n",
            "[253/ 252] loss: 0.689486, Accuracy: 0.748698\n",
            "[254/ 253] loss: 0.688776, Accuracy: 0.748950\n",
            "[255/ 254] loss: 0.689525, Accuracy: 0.748647\n",
            "[256/ 255] loss: 0.689635, Accuracy: 0.748713\n",
            "[257/ 256] loss: 0.688788, Accuracy: 0.748962\n",
            "[258/ 257] loss: 0.688545, Accuracy: 0.748906\n",
            "[259/ 258] loss: 0.689302, Accuracy: 0.748668\n",
            "[260/ 259] loss: 0.689907, Accuracy: 0.748612\n",
            "[261/ 260] loss: 0.689458, Accuracy: 0.748738\n",
            "[262/ 261] loss: 0.689304, Accuracy: 0.748743\n",
            "[263/ 262] loss: 0.689072, Accuracy: 0.748748\n",
            "[264/ 263] loss: 0.689103, Accuracy: 0.748812\n",
            "[265/ 264] loss: 0.689199, Accuracy: 0.748816\n",
            "[266/ 265] loss: 0.689041, Accuracy: 0.748762\n",
            "[267/ 266] loss: 0.689191, Accuracy: 0.748708\n",
            "[268/ 267] loss: 0.689426, Accuracy: 0.748654\n",
            "[269/ 268] loss: 0.690224, Accuracy: 0.748368\n",
            "[270/ 269] loss: 0.688875, Accuracy: 0.748954\n",
            "[271/ 270] loss: 0.689091, Accuracy: 0.748958\n",
            "[272/ 271] loss: 0.689407, Accuracy: 0.748732\n",
            "[273/ 272] loss: 0.689120, Accuracy: 0.748794\n",
            "[274/ 273] loss: 0.688458, Accuracy: 0.748970\n",
            "[275/ 274] loss: 0.688352, Accuracy: 0.749031\n",
            "[276/ 275] loss: 0.688146, Accuracy: 0.749091\n",
            "[277/ 276] loss: 0.688046, Accuracy: 0.749151\n",
            "[278/ 277] loss: 0.687763, Accuracy: 0.749267\n",
            "[279/ 278] loss: 0.687541, Accuracy: 0.749269\n",
            "[280/ 279] loss: 0.687559, Accuracy: 0.749328\n",
            "[281/ 280] loss: 0.687907, Accuracy: 0.749275\n",
            "[282/ 281] loss: 0.688156, Accuracy: 0.749166\n",
            "[283/ 282] loss: 0.687534, Accuracy: 0.749501\n",
            "[284/ 283] loss: 0.687392, Accuracy: 0.749669\n",
            "[285/ 284] loss: 0.686344, Accuracy: 0.750055\n",
            "[286/ 285] loss: 0.685759, Accuracy: 0.750110\n",
            "[287/ 286] loss: 0.685935, Accuracy: 0.750000\n",
            "[288/ 287] loss: 0.685931, Accuracy: 0.750000\n",
            "[289/ 288] loss: 0.686014, Accuracy: 0.749892\n",
            "[290/ 289] loss: 0.685469, Accuracy: 0.750108\n",
            "[291/ 290] loss: 0.686189, Accuracy: 0.750000\n",
            "[292/ 291] loss: 0.686201, Accuracy: 0.750000\n",
            "[293/ 292] loss: 0.686294, Accuracy: 0.749946\n",
            "[294/ 293] loss: 0.687149, Accuracy: 0.749627\n",
            "[295/ 294] loss: 0.686743, Accuracy: 0.749734\n",
            "[296/ 295] loss: 0.686951, Accuracy: 0.749682\n",
            "[297/ 296] loss: 0.687529, Accuracy: 0.749367\n",
            "[298/ 297] loss: 0.687474, Accuracy: 0.749474\n",
            "[299/ 298] loss: 0.686897, Accuracy: 0.749685\n",
            "[300/ 299] loss: 0.686212, Accuracy: 0.749896\n",
            "[301 / 300]  Loss:0.686631, Accuracy:0.749740\n",
            "[301/ 300] loss: 0.686631, Accuracy: 0.749740\n",
            "[302/ 301] loss: 0.686826, Accuracy: 0.749637\n",
            "[303/ 302] loss: 0.687387, Accuracy: 0.749483\n",
            "[304/ 303] loss: 0.686796, Accuracy: 0.749691\n",
            "[305/ 304] loss: 0.686849, Accuracy: 0.749743\n",
            "[306/ 305] loss: 0.686935, Accuracy: 0.749795\n",
            "[307/ 306] loss: 0.686560, Accuracy: 0.750051\n",
            "[308/ 307] loss: 0.686604, Accuracy: 0.750051\n",
            "[309/ 308] loss: 0.686281, Accuracy: 0.750254\n",
            "[310/ 309] loss: 0.686640, Accuracy: 0.750101\n",
            "[311/ 310] loss: 0.686670, Accuracy: 0.750000\n",
            "[312/ 311] loss: 0.686335, Accuracy: 0.750100\n",
            "[313/ 312] loss: 0.685739, Accuracy: 0.750250\n",
            "[314/ 313] loss: 0.685770, Accuracy: 0.750250\n",
            "[315/ 314] loss: 0.685562, Accuracy: 0.750199\n",
            "[316/ 315] loss: 0.685265, Accuracy: 0.750347\n",
            "[317/ 316] loss: 0.684587, Accuracy: 0.750643\n",
            "[318/ 317] loss: 0.684891, Accuracy: 0.750542\n",
            "[319/ 318] loss: 0.684910, Accuracy: 0.750491\n",
            "[320/ 319] loss: 0.684916, Accuracy: 0.750441\n",
            "[321/ 320] loss: 0.685696, Accuracy: 0.750049\n",
            "[322/ 321] loss: 0.685707, Accuracy: 0.750097\n",
            "[323/ 322] loss: 0.686289, Accuracy: 0.749806\n",
            "[324/ 323] loss: 0.685881, Accuracy: 0.749807\n",
            "[325/ 324] loss: 0.685715, Accuracy: 0.749952\n",
            "[326/ 325] loss: 0.685373, Accuracy: 0.750048\n",
            "[327/ 326] loss: 0.685475, Accuracy: 0.750000\n",
            "[328/ 327] loss: 0.685602, Accuracy: 0.750000\n",
            "[329/ 328] loss: 0.684814, Accuracy: 0.750238\n",
            "[330/ 329] loss: 0.684791, Accuracy: 0.750285\n",
            "[331/ 330] loss: 0.684740, Accuracy: 0.750331\n",
            "[332/ 331] loss: 0.685455, Accuracy: 0.749953\n",
            "[333/ 332] loss: 0.685226, Accuracy: 0.750000\n",
            "[334/ 333] loss: 0.685966, Accuracy: 0.749718\n",
            "[335/ 334] loss: 0.686127, Accuracy: 0.749673\n",
            "[336/ 335] loss: 0.685548, Accuracy: 0.749907\n",
            "[337/ 336] loss: 0.685633, Accuracy: 0.749907\n",
            "[338/ 337] loss: 0.685095, Accuracy: 0.750093\n",
            "[339/ 338] loss: 0.685279, Accuracy: 0.750185\n",
            "[340/ 339] loss: 0.685524, Accuracy: 0.750000\n",
            "[341/ 340] loss: 0.686316, Accuracy: 0.749816\n",
            "[342/ 341] loss: 0.686527, Accuracy: 0.749771\n",
            "[343/ 342] loss: 0.686435, Accuracy: 0.749909\n",
            "[344/ 343] loss: 0.685994, Accuracy: 0.750137\n",
            "[345/ 344] loss: 0.685667, Accuracy: 0.750363\n",
            "[346/ 345] loss: 0.685300, Accuracy: 0.750498\n",
            "[347/ 346] loss: 0.684916, Accuracy: 0.750587\n",
            "[348/ 347] loss: 0.684792, Accuracy: 0.750630\n",
            "[349/ 348] loss: 0.685012, Accuracy: 0.750539\n",
            "[350/ 349] loss: 0.685090, Accuracy: 0.750582\n",
            "[351/ 350] loss: 0.685347, Accuracy: 0.750536\n",
            "[352/ 351] loss: 0.685386, Accuracy: 0.750579\n",
            "[353/ 352] loss: 0.685162, Accuracy: 0.750666\n",
            "[354/ 353] loss: 0.684891, Accuracy: 0.750752\n",
            "[355/ 354] loss: 0.684256, Accuracy: 0.751015\n",
            "[356/ 355] loss: 0.684412, Accuracy: 0.750924\n",
            "[357/ 356] loss: 0.684205, Accuracy: 0.750966\n",
            "[358/ 357] loss: 0.684209, Accuracy: 0.750963\n",
            "[359/ 358] loss: 0.684321, Accuracy: 0.750786\n",
            "[360/ 359] loss: 0.684774, Accuracy: 0.750653\n",
            "[361/ 360] loss: 0.685661, Accuracy: 0.750347\n",
            "[362/ 361] loss: 0.685580, Accuracy: 0.750346\n",
            "[363/ 362] loss: 0.685602, Accuracy: 0.750388\n",
            "[364/ 363] loss: 0.685610, Accuracy: 0.750430\n",
            "[365/ 364] loss: 0.684897, Accuracy: 0.750644\n",
            "[366/ 365] loss: 0.684752, Accuracy: 0.750685\n",
            "[367/ 366] loss: 0.685034, Accuracy: 0.750598\n",
            "[368/ 367] loss: 0.684957, Accuracy: 0.750553\n",
            "[369/ 368] loss: 0.685047, Accuracy: 0.750467\n",
            "[370/ 369] loss: 0.684553, Accuracy: 0.750678\n",
            "[371/ 370] loss: 0.684202, Accuracy: 0.750887\n",
            "[372/ 371] loss: 0.683515, Accuracy: 0.751137\n",
            "[373/ 372] loss: 0.683614, Accuracy: 0.751134\n",
            "[374/ 373] loss: 0.683878, Accuracy: 0.751005\n",
            "[375/ 374] loss: 0.683906, Accuracy: 0.750919\n",
            "[376/ 375] loss: 0.684687, Accuracy: 0.750625\n",
            "[377/ 376] loss: 0.684472, Accuracy: 0.750748\n",
            "[378/ 377] loss: 0.684314, Accuracy: 0.750912\n",
            "[379/ 378] loss: 0.683722, Accuracy: 0.751116\n",
            "[380/ 379] loss: 0.683709, Accuracy: 0.751072\n",
            "[381/ 380] loss: 0.683286, Accuracy: 0.751234\n",
            "[382/ 381] loss: 0.683389, Accuracy: 0.751230\n",
            "[383/ 382] loss: 0.683409, Accuracy: 0.751186\n",
            "[384/ 383] loss: 0.683408, Accuracy: 0.751183\n",
            "[385/ 384] loss: 0.683035, Accuracy: 0.751302\n",
            "[386/ 385] loss: 0.682929, Accuracy: 0.751420\n",
            "[387/ 386] loss: 0.682779, Accuracy: 0.751457\n",
            "[388/ 387] loss: 0.682540, Accuracy: 0.751575\n",
            "[389/ 388] loss: 0.682439, Accuracy: 0.751611\n",
            "[390/ 389] loss: 0.681757, Accuracy: 0.751767\n",
            "[391/ 390] loss: 0.681930, Accuracy: 0.751763\n",
            "[392/ 391] loss: 0.682267, Accuracy: 0.751598\n",
            "[393/ 392] loss: 0.682119, Accuracy: 0.751634\n",
            "[394/ 393] loss: 0.681970, Accuracy: 0.751749\n",
            "[395/ 394] loss: 0.682066, Accuracy: 0.751705\n",
            "[396/ 395] loss: 0.681845, Accuracy: 0.751780\n",
            "[397/ 396] loss: 0.682505, Accuracy: 0.751697\n",
            "[398/ 397] loss: 0.682582, Accuracy: 0.751653\n",
            "[399/ 398] loss: 0.682704, Accuracy: 0.751649\n",
            "[400/ 399] loss: 0.683431, Accuracy: 0.751488\n",
            "[401/ 400] loss: 0.683307, Accuracy: 0.751484\n",
            "[402/ 401] loss: 0.683299, Accuracy: 0.751559\n",
            "[403/ 402] loss: 0.682849, Accuracy: 0.751788\n",
            "[404/ 403] loss: 0.682099, Accuracy: 0.752016\n",
            "[405/ 404] loss: 0.681891, Accuracy: 0.752127\n",
            "[406/ 405] loss: 0.681524, Accuracy: 0.752276\n",
            "[407/ 406] loss: 0.681152, Accuracy: 0.752425\n",
            "[408/ 407] loss: 0.681166, Accuracy: 0.752419\n",
            "[409/ 408] loss: 0.681076, Accuracy: 0.752528\n",
            "[410/ 409] loss: 0.681974, Accuracy: 0.752216\n",
            "[411/ 410] loss: 0.681835, Accuracy: 0.752287\n",
            "[412/ 411] loss: 0.682441, Accuracy: 0.752129\n",
            "[413/ 412] loss: 0.682109, Accuracy: 0.752275\n",
            "[414/ 413] loss: 0.681726, Accuracy: 0.752421\n",
            "[415/ 414] loss: 0.681957, Accuracy: 0.752378\n",
            "[416/ 415] loss: 0.682507, Accuracy: 0.752184\n",
            "[417/ 416] loss: 0.683045, Accuracy: 0.751953\n",
            "[418/ 417] loss: 0.682832, Accuracy: 0.751948\n",
            "[419/ 418] loss: 0.682893, Accuracy: 0.751906\n",
            "[420/ 419] loss: 0.682829, Accuracy: 0.751939\n",
            "[421/ 420] loss: 0.682287, Accuracy: 0.752158\n",
            "[422/ 421] loss: 0.682040, Accuracy: 0.752264\n",
            "[423/ 422] loss: 0.681644, Accuracy: 0.752444\n",
            "[424/ 423] loss: 0.681451, Accuracy: 0.752475\n",
            "[425/ 424] loss: 0.681284, Accuracy: 0.752543\n",
            "[426/ 425] loss: 0.681441, Accuracy: 0.752426\n",
            "[427/ 426] loss: 0.682411, Accuracy: 0.752054\n",
            "[428/ 427] loss: 0.682634, Accuracy: 0.752013\n",
            "[429/ 428] loss: 0.682210, Accuracy: 0.752190\n",
            "[430/ 429] loss: 0.682729, Accuracy: 0.752076\n",
            "[431/ 430] loss: 0.682407, Accuracy: 0.752253\n",
            "[432/ 431] loss: 0.681768, Accuracy: 0.752501\n",
            "[433/ 432] loss: 0.682488, Accuracy: 0.752315\n",
            "[434/ 433] loss: 0.682505, Accuracy: 0.752346\n",
            "[435/ 434] loss: 0.682522, Accuracy: 0.752304\n",
            "[436/ 435] loss: 0.682971, Accuracy: 0.752155\n",
            "[437/ 436] loss: 0.683267, Accuracy: 0.752114\n",
            "[438/ 437] loss: 0.682911, Accuracy: 0.752253\n",
            "[439/ 438] loss: 0.682496, Accuracy: 0.752390\n",
            "[440/ 439] loss: 0.682464, Accuracy: 0.752385\n",
            "[441/ 440] loss: 0.682460, Accuracy: 0.752344\n",
            "[442/ 441] loss: 0.682563, Accuracy: 0.752303\n",
            "[443/ 442] loss: 0.682554, Accuracy: 0.752262\n",
            "[444/ 443] loss: 0.683183, Accuracy: 0.752081\n",
            "[445/ 444] loss: 0.683175, Accuracy: 0.752041\n",
            "[446/ 445] loss: 0.683318, Accuracy: 0.752001\n",
            "[447/ 446] loss: 0.683309, Accuracy: 0.751927\n",
            "[448/ 447] loss: 0.683561, Accuracy: 0.751888\n",
            "[449/ 448] loss: 0.683568, Accuracy: 0.751849\n",
            "[450/ 449] loss: 0.683796, Accuracy: 0.751705\n",
            "[451/ 450] loss: 0.683862, Accuracy: 0.751667\n",
            "[452/ 451] loss: 0.683704, Accuracy: 0.751767\n",
            "[453/ 452] loss: 0.683536, Accuracy: 0.751798\n",
            "[454/ 453] loss: 0.683763, Accuracy: 0.751794\n",
            "[455/ 454] loss: 0.683876, Accuracy: 0.751790\n",
            "[456/ 455] loss: 0.683599, Accuracy: 0.751923\n",
            "[457/ 456] loss: 0.683730, Accuracy: 0.751885\n",
            "[458/ 457] loss: 0.683510, Accuracy: 0.752017\n",
            "[459/ 458] loss: 0.683852, Accuracy: 0.751842\n",
            "[460/ 459] loss: 0.683826, Accuracy: 0.751838\n",
            "[461/ 460] loss: 0.684026, Accuracy: 0.751766\n",
            "[462/ 461] loss: 0.683541, Accuracy: 0.751932\n",
            "[463/ 462] loss: 0.683694, Accuracy: 0.751894\n",
            "[464/ 463] loss: 0.684495, Accuracy: 0.751654\n",
            "[465/ 464] loss: 0.684253, Accuracy: 0.751717\n",
            "[466/ 465] loss: 0.684805, Accuracy: 0.751445\n",
            "[467/ 466] loss: 0.684436, Accuracy: 0.751542\n",
            "[468/ 467] loss: 0.684528, Accuracy: 0.751573\n",
            "[469/ 468] loss: 0.684396, Accuracy: 0.751703\n",
            "[470/ 469] loss: 0.684119, Accuracy: 0.751866\n",
            "[471/ 470] loss: 0.683681, Accuracy: 0.752028\n",
            "[472/ 471] loss: 0.683743, Accuracy: 0.751990\n",
            "[473/ 472] loss: 0.683397, Accuracy: 0.752119\n",
            "[474/ 473] loss: 0.683588, Accuracy: 0.752015\n",
            "[475/ 474] loss: 0.682940, Accuracy: 0.752242\n",
            "[476/ 475] loss: 0.682662, Accuracy: 0.752434\n",
            "[477/ 476] loss: 0.682126, Accuracy: 0.752626\n",
            "[478/ 477] loss: 0.682535, Accuracy: 0.752490\n",
            "[479/ 478] loss: 0.682515, Accuracy: 0.752452\n",
            "[480/ 479] loss: 0.682706, Accuracy: 0.752316\n",
            "[481/ 480] loss: 0.682878, Accuracy: 0.752279\n",
            "[482/ 481] loss: 0.682534, Accuracy: 0.752404\n",
            "[483/ 482] loss: 0.682368, Accuracy: 0.752464\n",
            "[484/ 483] loss: 0.681946, Accuracy: 0.752620\n",
            "[485/ 484] loss: 0.681883, Accuracy: 0.752615\n",
            "[486/ 485] loss: 0.681646, Accuracy: 0.752674\n",
            "[487/ 486] loss: 0.681226, Accuracy: 0.752861\n",
            "[488/ 487] loss: 0.681444, Accuracy: 0.752759\n",
            "[489/ 488] loss: 0.681062, Accuracy: 0.752850\n",
            "[490/ 489] loss: 0.680839, Accuracy: 0.752908\n",
            "[491/ 490] loss: 0.680722, Accuracy: 0.752934\n",
            "[492/ 491] loss: 0.680505, Accuracy: 0.752960\n",
            "[493/ 492] loss: 0.680602, Accuracy: 0.752890\n",
            "[494/ 493] loss: 0.680930, Accuracy: 0.752789\n",
            "[495/ 494] loss: 0.681177, Accuracy: 0.752625\n",
            "[496/ 495] loss: 0.681439, Accuracy: 0.752525\n",
            "[497/ 496] loss: 0.681167, Accuracy: 0.752615\n",
            "[498/ 497] loss: 0.681526, Accuracy: 0.752452\n",
            "[499/ 498] loss: 0.681354, Accuracy: 0.752541\n",
            "[500/ 499] loss: 0.681218, Accuracy: 0.752568\n",
            "[501/ 500] loss: 0.681025, Accuracy: 0.752594\n",
            "[502/ 501] loss: 0.681151, Accuracy: 0.752526\n",
            "[503/ 502] loss: 0.681183, Accuracy: 0.752583\n",
            "[504/ 503] loss: 0.681256, Accuracy: 0.752516\n",
            "[505/ 504] loss: 0.681096, Accuracy: 0.752573\n",
            "[506/ 505] loss: 0.681207, Accuracy: 0.752537\n",
            "[507/ 506] loss: 0.680846, Accuracy: 0.752779\n",
            "[508/ 507] loss: 0.680837, Accuracy: 0.752774\n",
            "[509/ 508] loss: 0.680730, Accuracy: 0.752799\n",
            "[510/ 509] loss: 0.680825, Accuracy: 0.752824\n",
            "[511/ 510] loss: 0.680562, Accuracy: 0.752972\n",
            "[512/ 511] loss: 0.680521, Accuracy: 0.752997\n",
            "[513/ 512] loss: 0.681045, Accuracy: 0.752747\n",
            "[514/ 513] loss: 0.681312, Accuracy: 0.752650\n",
            "[515/ 514] loss: 0.681697, Accuracy: 0.752493\n",
            "[516/ 515] loss: 0.682125, Accuracy: 0.752306\n",
            "[517/ 516] loss: 0.681550, Accuracy: 0.752513\n",
            "[518/ 517] loss: 0.681320, Accuracy: 0.752599\n",
            "[519/ 518] loss: 0.681255, Accuracy: 0.752624\n",
            "[520/ 519] loss: 0.681516, Accuracy: 0.752499\n",
            "[521/ 520] loss: 0.681371, Accuracy: 0.752554\n",
            "[522/ 521] loss: 0.681281, Accuracy: 0.752549\n",
            "[523/ 522] loss: 0.681053, Accuracy: 0.752694\n",
            "[524/ 523] loss: 0.680954, Accuracy: 0.752808\n",
            "[525/ 524] loss: 0.680912, Accuracy: 0.752922\n",
            "[526/ 525] loss: 0.681055, Accuracy: 0.752857\n",
            "[527/ 526] loss: 0.681111, Accuracy: 0.752852\n",
            "[528/ 527] loss: 0.680644, Accuracy: 0.753024\n",
            "[529/ 528] loss: 0.681154, Accuracy: 0.753048\n",
            "[530/ 529] loss: 0.680962, Accuracy: 0.753131\n",
            "[531/ 530] loss: 0.680635, Accuracy: 0.753154\n",
            "[532/ 531] loss: 0.680289, Accuracy: 0.753296\n",
            "[533/ 532] loss: 0.680517, Accuracy: 0.753201\n",
            "[534/ 533] loss: 0.680754, Accuracy: 0.753078\n",
            "[535/ 534] loss: 0.680744, Accuracy: 0.752985\n",
            "[536/ 535] loss: 0.680711, Accuracy: 0.752921\n",
            "[537/ 536] loss: 0.680915, Accuracy: 0.752857\n",
            "[538/ 537] loss: 0.680633, Accuracy: 0.752968\n",
            "[539/ 538] loss: 0.680485, Accuracy: 0.752962\n",
            "[540/ 539] loss: 0.680190, Accuracy: 0.753073\n",
            "[541/ 540] loss: 0.680261, Accuracy: 0.752980\n",
            "[542/ 541] loss: 0.680469, Accuracy: 0.752830\n",
            "[543/ 542] loss: 0.680688, Accuracy: 0.752768\n",
            "[544/ 543] loss: 0.680528, Accuracy: 0.752878\n",
            "[545/ 544] loss: 0.680518, Accuracy: 0.752901\n",
            "[546/ 545] loss: 0.681051, Accuracy: 0.752724\n",
            "[547/ 546] loss: 0.681021, Accuracy: 0.752776\n",
            "[548/ 547] loss: 0.680709, Accuracy: 0.752885\n",
            "[549/ 548] loss: 0.680579, Accuracy: 0.752937\n",
            "[550/ 549] loss: 0.680515, Accuracy: 0.752988\n",
            "[551/ 550] loss: 0.680405, Accuracy: 0.753011\n",
            "[552/ 551] loss: 0.680248, Accuracy: 0.753063\n",
            "[553/ 552] loss: 0.680228, Accuracy: 0.753085\n",
            "[554/ 553] loss: 0.680389, Accuracy: 0.753052\n",
            "[555/ 554] loss: 0.680269, Accuracy: 0.753187\n",
            "[556/ 555] loss: 0.680145, Accuracy: 0.753209\n",
            "[557/ 556] loss: 0.679922, Accuracy: 0.753288\n",
            "[558/ 557] loss: 0.679754, Accuracy: 0.753366\n",
            "[559/ 558] loss: 0.679641, Accuracy: 0.753388\n",
            "[560/ 559] loss: 0.679776, Accuracy: 0.753298\n",
            "[561/ 560] loss: 0.679450, Accuracy: 0.753376\n",
            "[562/ 561] loss: 0.679095, Accuracy: 0.753509\n",
            "[563/ 562] loss: 0.679045, Accuracy: 0.753475\n",
            "[564/ 563] loss: 0.679273, Accuracy: 0.753330\n",
            "[565/ 564] loss: 0.679357, Accuracy: 0.753324\n",
            "[566/ 565] loss: 0.679512, Accuracy: 0.753263\n",
            "[567/ 566] loss: 0.679570, Accuracy: 0.753175\n",
            "[568/ 567] loss: 0.679080, Accuracy: 0.753362\n",
            "[569/ 568] loss: 0.679023, Accuracy: 0.753356\n",
            "[570/ 569] loss: 0.678897, Accuracy: 0.753405\n",
            "[571/ 570] loss: 0.679003, Accuracy: 0.753427\n",
            "[572/ 571] loss: 0.678434, Accuracy: 0.753612\n",
            "[573/ 572] loss: 0.678593, Accuracy: 0.753524\n",
            "[574/ 573] loss: 0.678636, Accuracy: 0.753436\n",
            "[575/ 574] loss: 0.679302, Accuracy: 0.753212\n",
            "[576/ 575] loss: 0.679143, Accuracy: 0.753234\n",
            "[577/ 576] loss: 0.679060, Accuracy: 0.753255\n",
            "[578/ 577] loss: 0.678951, Accuracy: 0.753277\n",
            "[579/ 578] loss: 0.678886, Accuracy: 0.753379\n",
            "[580/ 579] loss: 0.679025, Accuracy: 0.753319\n",
            "[581/ 580] loss: 0.679354, Accuracy: 0.753206\n",
            "[582/ 581] loss: 0.679305, Accuracy: 0.753254\n",
            "[583/ 582] loss: 0.679432, Accuracy: 0.753195\n",
            "[584/ 583] loss: 0.679451, Accuracy: 0.753189\n",
            "[585/ 584] loss: 0.679538, Accuracy: 0.753157\n",
            "[586/ 585] loss: 0.679810, Accuracy: 0.753018\n",
            "[587/ 586] loss: 0.680170, Accuracy: 0.752933\n",
            "[588/ 587] loss: 0.680251, Accuracy: 0.752981\n",
            "[589/ 588] loss: 0.680233, Accuracy: 0.753003\n",
            "[590/ 589] loss: 0.680363, Accuracy: 0.752945\n",
            "[591/ 590] loss: 0.680032, Accuracy: 0.753046\n",
            "[592/ 591] loss: 0.680102, Accuracy: 0.753040\n",
            "[593/ 592] loss: 0.680119, Accuracy: 0.753035\n",
            "[594/ 593] loss: 0.679735, Accuracy: 0.753136\n",
            "[595/ 594] loss: 0.679452, Accuracy: 0.753209\n",
            "[596/ 595] loss: 0.679602, Accuracy: 0.753204\n",
            "[597/ 596] loss: 0.679434, Accuracy: 0.753251\n",
            "[598/ 597] loss: 0.680007, Accuracy: 0.753088\n",
            "[599/ 598] loss: 0.679892, Accuracy: 0.753162\n",
            "[600/ 599] loss: 0.679878, Accuracy: 0.753156\n",
            "[601 / 600]  Loss:0.679552, Accuracy:0.753255\n",
            "[601/ 600] loss: 0.679552, Accuracy: 0.753255\n",
            "[602/ 601] loss: 0.679403, Accuracy: 0.753276\n",
            "[603/ 602] loss: 0.679103, Accuracy: 0.753400\n",
            "[604/ 603] loss: 0.679043, Accuracy: 0.753394\n",
            "[605/ 604] loss: 0.679038, Accuracy: 0.753337\n",
            "[606/ 605] loss: 0.678655, Accuracy: 0.753461\n",
            "[607/ 606] loss: 0.678651, Accuracy: 0.753481\n",
            "[608/ 607] loss: 0.678391, Accuracy: 0.753578\n",
            "[609/ 608] loss: 0.678748, Accuracy: 0.753469\n",
            "[610/ 609] loss: 0.678904, Accuracy: 0.753438\n",
            "[611/ 610] loss: 0.678500, Accuracy: 0.753560\n",
            "[612/ 611] loss: 0.677923, Accuracy: 0.753759\n",
            "[613/ 612] loss: 0.678031, Accuracy: 0.753702\n",
            "[614/ 613] loss: 0.677643, Accuracy: 0.753823\n",
            "[615/ 614] loss: 0.677670, Accuracy: 0.753766\n",
            "[616/ 615] loss: 0.677348, Accuracy: 0.753887\n",
            "[617/ 616] loss: 0.677115, Accuracy: 0.753957\n",
            "[618/ 617] loss: 0.676987, Accuracy: 0.754052\n",
            "[619/ 618] loss: 0.677022, Accuracy: 0.754045\n",
            "[620/ 619] loss: 0.677226, Accuracy: 0.753938\n",
            "[621/ 620] loss: 0.676881, Accuracy: 0.754057\n",
            "[622/ 621] loss: 0.676488, Accuracy: 0.754227\n",
            "[623/ 622] loss: 0.677094, Accuracy: 0.754070\n",
            "[624/ 623] loss: 0.676930, Accuracy: 0.754113\n",
            "[625/ 624] loss: 0.677023, Accuracy: 0.754107\n",
            "[626/ 625] loss: 0.677131, Accuracy: 0.754100\n",
            "[627/ 626] loss: 0.676742, Accuracy: 0.754218\n",
            "[628/ 627] loss: 0.677505, Accuracy: 0.753987\n",
            "[629/ 628] loss: 0.677428, Accuracy: 0.754006\n",
            "[630/ 629] loss: 0.677473, Accuracy: 0.754024\n",
            "[631/ 630] loss: 0.677601, Accuracy: 0.753968\n",
            "[632/ 631] loss: 0.677455, Accuracy: 0.754012\n",
            "[633/ 632] loss: 0.677307, Accuracy: 0.754129\n",
            "[634/ 633] loss: 0.677268, Accuracy: 0.754122\n",
            "[635/ 634] loss: 0.677515, Accuracy: 0.754042\n",
            "[636/ 635] loss: 0.677461, Accuracy: 0.753962\n",
            "[637/ 636] loss: 0.677445, Accuracy: 0.753980\n",
            "[638/ 637] loss: 0.677237, Accuracy: 0.754047\n",
            "[639/ 638] loss: 0.676738, Accuracy: 0.754212\n",
            "[640/ 639] loss: 0.676368, Accuracy: 0.754401\n",
            "[641/ 640] loss: 0.676710, Accuracy: 0.754297\n",
            "[642/ 641] loss: 0.676611, Accuracy: 0.754339\n",
            "[643/ 642] loss: 0.676468, Accuracy: 0.754357\n",
            "[644/ 643] loss: 0.676437, Accuracy: 0.754398\n",
            "[645/ 644] loss: 0.676256, Accuracy: 0.754416\n",
            "[646/ 645] loss: 0.676240, Accuracy: 0.754409\n",
            "[647/ 646] loss: 0.676664, Accuracy: 0.754281\n",
            "[648/ 647] loss: 0.676513, Accuracy: 0.754323\n",
            "[649/ 648] loss: 0.676718, Accuracy: 0.754292\n",
            "[650/ 649] loss: 0.677073, Accuracy: 0.754165\n",
            "[651/ 650] loss: 0.676641, Accuracy: 0.754303\n",
            "[652/ 651] loss: 0.676659, Accuracy: 0.754296\n",
            "[653/ 652] loss: 0.676372, Accuracy: 0.754409\n",
            "[654/ 653] loss: 0.676203, Accuracy: 0.754427\n",
            "[655/ 654] loss: 0.676315, Accuracy: 0.754372\n",
            "[656/ 655] loss: 0.676089, Accuracy: 0.754437\n",
            "[657/ 656] loss: 0.676177, Accuracy: 0.754359\n",
            "[658/ 657] loss: 0.676199, Accuracy: 0.754328\n",
            "[659/ 658] loss: 0.676404, Accuracy: 0.754251\n",
            "[660/ 659] loss: 0.676884, Accuracy: 0.754102\n",
            "[661/ 660] loss: 0.677239, Accuracy: 0.753930\n",
            "[662/ 661] loss: 0.677015, Accuracy: 0.753971\n",
            "[663/ 662] loss: 0.676976, Accuracy: 0.753989\n",
            "[664/ 663] loss: 0.676819, Accuracy: 0.754077\n",
            "[665/ 664] loss: 0.676809, Accuracy: 0.754047\n",
            "[666/ 665] loss: 0.676892, Accuracy: 0.754018\n",
            "[667/ 666] loss: 0.676731, Accuracy: 0.754059\n",
            "[668/ 667] loss: 0.676728, Accuracy: 0.754076\n",
            "[669/ 668] loss: 0.676667, Accuracy: 0.754070\n",
            "[670/ 669] loss: 0.676751, Accuracy: 0.754041\n",
            "[671/ 670] loss: 0.676445, Accuracy: 0.754151\n",
            "[672/ 671] loss: 0.676737, Accuracy: 0.754028\n",
            "[673/ 672] loss: 0.676895, Accuracy: 0.753999\n",
            "[674/ 673] loss: 0.676927, Accuracy: 0.753993\n",
            "[675/ 674] loss: 0.676857, Accuracy: 0.754034\n",
            "[676/ 675] loss: 0.676598, Accuracy: 0.754097\n",
            "[677/ 676] loss: 0.676574, Accuracy: 0.754045\n",
            "[678/ 677] loss: 0.676424, Accuracy: 0.754085\n",
            "[679/ 678] loss: 0.676372, Accuracy: 0.754079\n",
            "[680/ 679] loss: 0.676428, Accuracy: 0.754073\n",
            "[681/ 680] loss: 0.676386, Accuracy: 0.754067\n",
            "[682/ 681] loss: 0.676114, Accuracy: 0.754107\n",
            "[683/ 682] loss: 0.676231, Accuracy: 0.754078\n",
            "[684/ 683] loss: 0.676059, Accuracy: 0.754118\n",
            "[685/ 684] loss: 0.676066, Accuracy: 0.754135\n",
            "[686/ 685] loss: 0.676071, Accuracy: 0.754129\n",
            "[687/ 686] loss: 0.675893, Accuracy: 0.754168\n",
            "[688/ 687] loss: 0.675911, Accuracy: 0.754139\n",
            "[689/ 688] loss: 0.675585, Accuracy: 0.754292\n",
            "[690/ 689] loss: 0.675723, Accuracy: 0.754241\n",
            "[691/ 690] loss: 0.675768, Accuracy: 0.754257\n",
            "[692/ 691] loss: 0.675815, Accuracy: 0.754228\n",
            "[693/ 692] loss: 0.675704, Accuracy: 0.754290\n",
            "[694/ 693] loss: 0.675674, Accuracy: 0.754284\n",
            "[695/ 694] loss: 0.675724, Accuracy: 0.754278\n",
            "[696/ 695] loss: 0.675856, Accuracy: 0.754182\n",
            "[697/ 696] loss: 0.675702, Accuracy: 0.754221\n",
            "[698/ 697] loss: 0.675864, Accuracy: 0.754147\n",
            "[699/ 698] loss: 0.675823, Accuracy: 0.754164\n",
            "[700/ 699] loss: 0.675707, Accuracy: 0.754180\n",
            "[701/ 700] loss: 0.675422, Accuracy: 0.754308\n",
            "[702/ 701] loss: 0.675470, Accuracy: 0.754302\n",
            "[703/ 702] loss: 0.675351, Accuracy: 0.754363\n",
            "[704/ 703] loss: 0.675123, Accuracy: 0.754401\n",
            "[705/ 704] loss: 0.675101, Accuracy: 0.754395\n",
            "[706/ 705] loss: 0.675533, Accuracy: 0.754189\n",
            "[707/ 706] loss: 0.675618, Accuracy: 0.754139\n",
            "[708/ 707] loss: 0.675648, Accuracy: 0.754066\n",
            "[709/ 708] loss: 0.675624, Accuracy: 0.754039\n",
            "[710/ 709] loss: 0.675364, Accuracy: 0.754143\n",
            "[711/ 710] loss: 0.675794, Accuracy: 0.753961\n",
            "[712/ 711] loss: 0.675529, Accuracy: 0.754066\n",
            "[713/ 712] loss: 0.675513, Accuracy: 0.754104\n",
            "[714/ 713] loss: 0.675351, Accuracy: 0.754164\n",
            "[715/ 714] loss: 0.675694, Accuracy: 0.754027\n",
            "[716/ 715] loss: 0.675648, Accuracy: 0.754021\n",
            "[717/ 716] loss: 0.675835, Accuracy: 0.753994\n",
            "[718/ 717] loss: 0.675661, Accuracy: 0.754097\n",
            "[719/ 718] loss: 0.675252, Accuracy: 0.754265\n",
            "[720/ 719] loss: 0.674971, Accuracy: 0.754390\n",
            "[721/ 720] loss: 0.675184, Accuracy: 0.754275\n",
            "[722/ 721] loss: 0.674979, Accuracy: 0.754334\n",
            "[723/ 722] loss: 0.675101, Accuracy: 0.754307\n",
            "[724/ 723] loss: 0.675417, Accuracy: 0.754279\n",
            "[725/ 724] loss: 0.675261, Accuracy: 0.754316\n",
            "[726/ 725] loss: 0.675390, Accuracy: 0.754267\n",
            "[727/ 726] loss: 0.675359, Accuracy: 0.754283\n",
            "[728/ 727] loss: 0.675170, Accuracy: 0.754363\n",
            "[729/ 728] loss: 0.675187, Accuracy: 0.754357\n",
            "[730/ 729] loss: 0.675369, Accuracy: 0.754265\n",
            "[731/ 730] loss: 0.675190, Accuracy: 0.754345\n",
            "[732/ 731] loss: 0.675086, Accuracy: 0.754446\n",
            "[733/ 732] loss: 0.675055, Accuracy: 0.754483\n",
            "[734/ 733] loss: 0.675000, Accuracy: 0.754476\n",
            "[735/ 734] loss: 0.674981, Accuracy: 0.754492\n",
            "[736/ 735] loss: 0.675378, Accuracy: 0.754294\n",
            "[737/ 736] loss: 0.675718, Accuracy: 0.754161\n",
            "[738/ 737] loss: 0.675969, Accuracy: 0.754071\n",
            "[739/ 738] loss: 0.676004, Accuracy: 0.754086\n",
            "[740/ 739] loss: 0.675926, Accuracy: 0.754144\n",
            "[741/ 740] loss: 0.676110, Accuracy: 0.754139\n",
            "[742/ 741] loss: 0.676444, Accuracy: 0.754006\n",
            "[743/ 742] loss: 0.676467, Accuracy: 0.753980\n",
            "[744/ 743] loss: 0.676406, Accuracy: 0.754017\n",
            "[745/ 744] loss: 0.676238, Accuracy: 0.754053\n",
            "[746/ 745] loss: 0.676066, Accuracy: 0.754132\n",
            "[747/ 746] loss: 0.676148, Accuracy: 0.754084\n",
            "[748/ 747] loss: 0.676616, Accuracy: 0.753932\n",
            "[749/ 748] loss: 0.676593, Accuracy: 0.753927\n",
            "[750/ 749] loss: 0.676516, Accuracy: 0.753964\n",
            "[751/ 750] loss: 0.676612, Accuracy: 0.753917\n",
            "[752/ 751] loss: 0.676613, Accuracy: 0.753932\n",
            "[753/ 752] loss: 0.676763, Accuracy: 0.753865\n",
            "[754/ 753] loss: 0.676853, Accuracy: 0.753880\n",
            "[755/ 754] loss: 0.676597, Accuracy: 0.754000\n",
            "[756/ 755] loss: 0.676707, Accuracy: 0.753932\n",
            "[757/ 756] loss: 0.676616, Accuracy: 0.753948\n",
            "[758/ 757] loss: 0.676717, Accuracy: 0.753901\n",
            "[759/ 758] loss: 0.676824, Accuracy: 0.753834\n",
            "[760/ 759] loss: 0.676601, Accuracy: 0.753911\n",
            "[761/ 760] loss: 0.676502, Accuracy: 0.753947\n",
            "[762/ 761] loss: 0.676193, Accuracy: 0.754086\n",
            "[763/ 762] loss: 0.676141, Accuracy: 0.754142\n",
            "[764/ 763] loss: 0.676208, Accuracy: 0.754116\n",
            "[765/ 764] loss: 0.676342, Accuracy: 0.754090\n",
            "[766/ 765] loss: 0.676146, Accuracy: 0.754167\n",
            "[767/ 766] loss: 0.675903, Accuracy: 0.754263\n",
            "[768/ 767] loss: 0.676100, Accuracy: 0.754156\n",
            "[769/ 768] loss: 0.675843, Accuracy: 0.754232\n",
            "[770/ 769] loss: 0.675777, Accuracy: 0.754267\n",
            "[771/ 770] loss: 0.675680, Accuracy: 0.754302\n",
            "[772/ 771] loss: 0.675361, Accuracy: 0.754418\n",
            "[773/ 772] loss: 0.675406, Accuracy: 0.754432\n",
            "[774/ 773] loss: 0.675340, Accuracy: 0.754467\n",
            "[775/ 774] loss: 0.675402, Accuracy: 0.754482\n",
            "[776/ 775] loss: 0.675228, Accuracy: 0.754536\n",
            "[777/ 776] loss: 0.675028, Accuracy: 0.754631\n",
            "[778/ 777] loss: 0.675014, Accuracy: 0.754645\n",
            "[779/ 778] loss: 0.674817, Accuracy: 0.754740\n",
            "[780/ 779] loss: 0.674917, Accuracy: 0.754734\n",
            "[781/ 780] loss: 0.674869, Accuracy: 0.754748\n",
            "[782/ 781] loss: 0.675071, Accuracy: 0.754701\n",
            "[783/ 782] loss: 0.675080, Accuracy: 0.754695\n",
            "[784/ 783] loss: 0.675108, Accuracy: 0.754709\n",
            "[785/ 784] loss: 0.674897, Accuracy: 0.754763\n",
            "[786/ 785] loss: 0.674720, Accuracy: 0.754877\n",
            "[787/ 786] loss: 0.674817, Accuracy: 0.754791\n",
            "[788/ 787] loss: 0.674766, Accuracy: 0.754765\n",
            "[789/ 788] loss: 0.674900, Accuracy: 0.754719\n",
            "[790/ 789] loss: 0.674802, Accuracy: 0.754773\n",
            "[791/ 790] loss: 0.674812, Accuracy: 0.754747\n",
            "[792/ 791] loss: 0.674595, Accuracy: 0.754800\n",
            "[793/ 792] loss: 0.675056, Accuracy: 0.754656\n",
            "[794/ 793] loss: 0.674891, Accuracy: 0.754729\n",
            "[795/ 794] loss: 0.674684, Accuracy: 0.754841\n",
            "[796/ 795] loss: 0.674341, Accuracy: 0.754992\n",
            "[797/ 796] loss: 0.674296, Accuracy: 0.755025\n",
            "[798/ 797] loss: 0.674060, Accuracy: 0.755136\n",
            "[799/ 798] loss: 0.673958, Accuracy: 0.755169\n",
            "[800/ 799] loss: 0.673922, Accuracy: 0.755163\n",
            "[801/ 800] loss: 0.673770, Accuracy: 0.755215\n",
            "[802/ 801] loss: 0.673806, Accuracy: 0.755169\n",
            "[803/ 802] loss: 0.674046, Accuracy: 0.755065\n",
            "[804/ 803] loss: 0.673828, Accuracy: 0.755156\n",
            "[805/ 804] loss: 0.673857, Accuracy: 0.755111\n",
            "[806/ 805] loss: 0.674081, Accuracy: 0.755047\n",
            "[807/ 806] loss: 0.673932, Accuracy: 0.755098\n",
            "[808/ 807] loss: 0.673952, Accuracy: 0.755170\n",
            "[809/ 808] loss: 0.673979, Accuracy: 0.755202\n",
            "[810/ 809] loss: 0.673820, Accuracy: 0.755292\n",
            "[811/ 810] loss: 0.673743, Accuracy: 0.755324\n",
            "[812/ 811] loss: 0.673780, Accuracy: 0.755318\n",
            "[813/ 812] loss: 0.673862, Accuracy: 0.755292\n",
            "[814/ 813] loss: 0.673887, Accuracy: 0.755304\n",
            "[815/ 814] loss: 0.674024, Accuracy: 0.755279\n",
            "[816/ 815] loss: 0.673898, Accuracy: 0.755349\n",
            "[817/ 816] loss: 0.673917, Accuracy: 0.755361\n",
            "[818/ 817] loss: 0.674004, Accuracy: 0.755317\n",
            "[819/ 818] loss: 0.674393, Accuracy: 0.755138\n",
            "[820/ 819] loss: 0.674089, Accuracy: 0.755227\n",
            "[821/ 820] loss: 0.674101, Accuracy: 0.755240\n",
            "[822/ 821] loss: 0.674172, Accuracy: 0.755253\n",
            "[823/ 822] loss: 0.674331, Accuracy: 0.755246\n",
            "[824/ 823] loss: 0.674356, Accuracy: 0.755240\n",
            "[825/ 824] loss: 0.674274, Accuracy: 0.755234\n",
            "[826/ 825] loss: 0.674257, Accuracy: 0.755265\n",
            "[827/ 826] loss: 0.674454, Accuracy: 0.755164\n",
            "[828/ 827] loss: 0.674397, Accuracy: 0.755177\n",
            "[829/ 828] loss: 0.674159, Accuracy: 0.755246\n",
            "[830/ 829] loss: 0.674186, Accuracy: 0.755240\n",
            "[831/ 830] loss: 0.674314, Accuracy: 0.755215\n",
            "[832/ 831] loss: 0.674305, Accuracy: 0.755190\n",
            "[833/ 832] loss: 0.674356, Accuracy: 0.755146\n",
            "[834/ 833] loss: 0.674306, Accuracy: 0.755177\n",
            "[835/ 834] loss: 0.674097, Accuracy: 0.755246\n",
            "[836/ 835] loss: 0.673970, Accuracy: 0.755240\n",
            "[837/ 836] loss: 0.673873, Accuracy: 0.755233\n",
            "[838/ 837] loss: 0.673602, Accuracy: 0.755339\n",
            "[839/ 838] loss: 0.673958, Accuracy: 0.755221\n",
            "[840/ 839] loss: 0.673962, Accuracy: 0.755196\n",
            "[841/ 840] loss: 0.674036, Accuracy: 0.755171\n",
            "[842/ 841] loss: 0.673807, Accuracy: 0.755239\n",
            "[843/ 842] loss: 0.674010, Accuracy: 0.755177\n",
            "[844/ 843] loss: 0.673971, Accuracy: 0.755171\n",
            "[845/ 844] loss: 0.673925, Accuracy: 0.755184\n",
            "[846/ 845] loss: 0.673988, Accuracy: 0.755104\n",
            "[847/ 846] loss: 0.673954, Accuracy: 0.755116\n",
            "[848/ 847] loss: 0.673828, Accuracy: 0.755147\n",
            "[849/ 848] loss: 0.673852, Accuracy: 0.755141\n",
            "[850/ 849] loss: 0.674020, Accuracy: 0.755061\n",
            "[851/ 850] loss: 0.674135, Accuracy: 0.755000\n",
            "[852/ 851] loss: 0.674312, Accuracy: 0.754939\n",
            "[853/ 852] loss: 0.674453, Accuracy: 0.754860\n",
            "[854/ 853] loss: 0.674413, Accuracy: 0.754891\n",
            "[855/ 854] loss: 0.674421, Accuracy: 0.754903\n",
            "[856/ 855] loss: 0.674406, Accuracy: 0.754879\n",
            "[857/ 856] loss: 0.674642, Accuracy: 0.754819\n",
            "[858/ 857] loss: 0.674665, Accuracy: 0.754795\n",
            "[859/ 858] loss: 0.674569, Accuracy: 0.754862\n",
            "[860/ 859] loss: 0.674957, Accuracy: 0.754711\n",
            "[861/ 860] loss: 0.674954, Accuracy: 0.754742\n",
            "[862/ 861] loss: 0.674964, Accuracy: 0.754755\n",
            "[863/ 862] loss: 0.675644, Accuracy: 0.754550\n",
            "[864/ 863] loss: 0.675884, Accuracy: 0.754472\n",
            "[865/ 864] loss: 0.675804, Accuracy: 0.754539\n",
            "[866/ 865] loss: 0.675765, Accuracy: 0.754570\n",
            "[867/ 866] loss: 0.675844, Accuracy: 0.754529\n",
            "[868/ 867] loss: 0.675884, Accuracy: 0.754451\n",
            "[869/ 868] loss: 0.676007, Accuracy: 0.754410\n",
            "[870/ 869] loss: 0.676080, Accuracy: 0.754405\n",
            "[871/ 870] loss: 0.676376, Accuracy: 0.754256\n",
            "[872/ 871] loss: 0.676206, Accuracy: 0.754305\n",
            "[873/ 872] loss: 0.676196, Accuracy: 0.754336\n",
            "[874/ 873] loss: 0.676207, Accuracy: 0.754349\n",
            "[875/ 874] loss: 0.676196, Accuracy: 0.754362\n",
            "[876/ 875] loss: 0.676069, Accuracy: 0.754464\n",
            "[877/ 876] loss: 0.676089, Accuracy: 0.754459\n",
            "[878/ 877] loss: 0.676114, Accuracy: 0.754436\n",
            "[879/ 878] loss: 0.675959, Accuracy: 0.754502\n",
            "[880/ 879] loss: 0.676022, Accuracy: 0.754426\n",
            "[881/ 880] loss: 0.676065, Accuracy: 0.754403\n",
            "[882/ 881] loss: 0.675892, Accuracy: 0.754434\n",
            "[883/ 882] loss: 0.675739, Accuracy: 0.754482\n",
            "[884/ 883] loss: 0.675574, Accuracy: 0.754530\n",
            "[885/ 884] loss: 0.675317, Accuracy: 0.754613\n",
            "[886/ 885] loss: 0.675287, Accuracy: 0.754626\n",
            "[887/ 886] loss: 0.675166, Accuracy: 0.754656\n",
            "[888/ 887] loss: 0.675226, Accuracy: 0.754651\n",
            "[889/ 888] loss: 0.675375, Accuracy: 0.754610\n",
            "[890/ 889] loss: 0.675258, Accuracy: 0.754693\n",
            "[891/ 890] loss: 0.675048, Accuracy: 0.754740\n",
            "[892/ 891] loss: 0.674812, Accuracy: 0.754823\n",
            "[893/ 892] loss: 0.674797, Accuracy: 0.754817\n",
            "[894/ 893] loss: 0.674923, Accuracy: 0.754759\n",
            "[895/ 894] loss: 0.674717, Accuracy: 0.754824\n",
            "[896/ 895] loss: 0.674609, Accuracy: 0.754853\n",
            "[897/ 896] loss: 0.674495, Accuracy: 0.754883\n",
            "[898/ 897] loss: 0.674371, Accuracy: 0.754895\n",
            "[899/ 898] loss: 0.674355, Accuracy: 0.754907\n",
            "[900/ 899] loss: 0.674371, Accuracy: 0.754884\n",
            "[901 / 900]  Loss:0.674572, Accuracy:0.754792\n",
            "[901/ 900] loss: 0.674572, Accuracy: 0.754792\n",
            "[902/ 901] loss: 0.674446, Accuracy: 0.754838\n",
            "[903/ 902] loss: 0.674325, Accuracy: 0.754902\n",
            "[904/ 903] loss: 0.674433, Accuracy: 0.754845\n",
            "[905/ 904] loss: 0.674642, Accuracy: 0.754770\n",
            "[906/ 905] loss: 0.674607, Accuracy: 0.754748\n",
            "[907/ 906] loss: 0.674404, Accuracy: 0.754794\n",
            "[908/ 907] loss: 0.674390, Accuracy: 0.754806\n",
            "[909/ 908] loss: 0.674274, Accuracy: 0.754870\n",
            "[910/ 909] loss: 0.674218, Accuracy: 0.754916\n",
            "[911/ 910] loss: 0.674062, Accuracy: 0.754979\n",
            "[912/ 911] loss: 0.674082, Accuracy: 0.754957\n",
            "[913/ 912] loss: 0.674315, Accuracy: 0.754866\n",
            "[914/ 913] loss: 0.674207, Accuracy: 0.754912\n",
            "[915/ 914] loss: 0.674198, Accuracy: 0.754923\n",
            "[916/ 915] loss: 0.674150, Accuracy: 0.754935\n",
            "[917/ 916] loss: 0.674118, Accuracy: 0.754913\n",
            "[918/ 917] loss: 0.674081, Accuracy: 0.754975\n",
            "[919/ 918] loss: 0.673793, Accuracy: 0.755072\n",
            "[920/ 919] loss: 0.674012, Accuracy: 0.754999\n",
            "[921/ 920] loss: 0.673981, Accuracy: 0.754976\n",
            "[922/ 921] loss: 0.674151, Accuracy: 0.754886\n",
            "[923/ 922] loss: 0.674253, Accuracy: 0.754847\n",
            "[924/ 923] loss: 0.674453, Accuracy: 0.754791\n",
            "[925/ 924] loss: 0.674451, Accuracy: 0.754786\n",
            "[926/ 925] loss: 0.674371, Accuracy: 0.754797\n",
            "[927/ 926] loss: 0.674365, Accuracy: 0.754792\n",
            "[928/ 927] loss: 0.674233, Accuracy: 0.754854\n",
            "[929/ 928] loss: 0.674132, Accuracy: 0.754900\n",
            "[930/ 929] loss: 0.673951, Accuracy: 0.754962\n",
            "[931/ 930] loss: 0.673740, Accuracy: 0.755007\n",
            "[932/ 931] loss: 0.674215, Accuracy: 0.754817\n",
            "[933/ 932] loss: 0.674281, Accuracy: 0.754778\n",
            "[934/ 933] loss: 0.674472, Accuracy: 0.754639\n",
            "[935/ 934] loss: 0.674600, Accuracy: 0.754617\n",
            "[936/ 935] loss: 0.674600, Accuracy: 0.754629\n",
            "[937/ 936] loss: 0.674653, Accuracy: 0.754624\n",
            "[938/ 937] loss: 0.674596, Accuracy: 0.754669\n",
            "[939/ 938] loss: 0.674370, Accuracy: 0.754764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#validation code\n",
        "model.eval()\n",
        "val_acc = 0.0\n",
        "val_loss = 0.0\n",
        "for i,data in enumerate(test_loader,1):\n",
        "  images,labels = data\n",
        "  images = images.view(images.shape[0],-1)\n",
        "  images = images.to(device)\n",
        "  labels = labels.to(device)\n",
        "  with torch.no_grad():\n",
        "    res = model(images)\n",
        "    loss = loss_fn(res,labels)\n",
        "  val_loss += loss.item()\n",
        "  _,prediction = torch.max(res,1)\n",
        "  val_acc +=(prediction==labels).float().mean()\n",
        "  print(f'[{i+1}/ {i}] loss: {val_loss/i:6f}, Accuracy: {val_acc/i:6f}')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNkBjTtBweaZ",
        "outputId": "ff9ea9ef-ff7b-4502-8c59-4794bc7da078"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2/ 1] loss: 0.678225, Accuracy: 0.765625\n",
            "[3/ 2] loss: 0.807588, Accuracy: 0.703125\n",
            "[4/ 3] loss: 0.719800, Accuracy: 0.750000\n",
            "[5/ 4] loss: 0.734822, Accuracy: 0.742188\n",
            "[6/ 5] loss: 0.722156, Accuracy: 0.753125\n",
            "[7/ 6] loss: 0.680321, Accuracy: 0.763021\n",
            "[8/ 7] loss: 0.667634, Accuracy: 0.770089\n",
            "[9/ 8] loss: 0.651654, Accuracy: 0.779297\n",
            "[10/ 9] loss: 0.667824, Accuracy: 0.769097\n",
            "[11/ 10] loss: 0.674861, Accuracy: 0.764063\n",
            "[12/ 11] loss: 0.697092, Accuracy: 0.758523\n",
            "[13/ 12] loss: 0.699240, Accuracy: 0.757812\n",
            "[14/ 13] loss: 0.696338, Accuracy: 0.756010\n",
            "[15/ 14] loss: 0.700474, Accuracy: 0.753348\n",
            "[16/ 15] loss: 0.707012, Accuracy: 0.755208\n",
            "[17/ 16] loss: 0.722629, Accuracy: 0.750000\n",
            "[18/ 17] loss: 0.736392, Accuracy: 0.741728\n",
            "[19/ 18] loss: 0.742083, Accuracy: 0.739583\n",
            "[20/ 19] loss: 0.735934, Accuracy: 0.740132\n",
            "[21/ 20] loss: 0.750682, Accuracy: 0.734375\n",
            "[22/ 21] loss: 0.750065, Accuracy: 0.735119\n",
            "[23/ 22] loss: 0.745195, Accuracy: 0.736506\n",
            "[24/ 23] loss: 0.728154, Accuracy: 0.741848\n",
            "[25/ 24] loss: 0.730683, Accuracy: 0.741536\n",
            "[26/ 25] loss: 0.733463, Accuracy: 0.741875\n",
            "[27/ 26] loss: 0.717680, Accuracy: 0.748197\n",
            "[28/ 27] loss: 0.701522, Accuracy: 0.754630\n",
            "[29/ 28] loss: 0.702661, Accuracy: 0.755022\n",
            "[30/ 29] loss: 0.704476, Accuracy: 0.754310\n",
            "[31/ 30] loss: 0.702745, Accuracy: 0.755208\n",
            "[32/ 31] loss: 0.704245, Accuracy: 0.754032\n",
            "[33/ 32] loss: 0.699206, Accuracy: 0.754883\n",
            "[34/ 33] loss: 0.704084, Accuracy: 0.753788\n",
            "[35/ 34] loss: 0.712886, Accuracy: 0.750460\n",
            "[36/ 35] loss: 0.717114, Accuracy: 0.748661\n",
            "[37/ 36] loss: 0.716189, Accuracy: 0.748698\n",
            "[38/ 37] loss: 0.712572, Accuracy: 0.750000\n",
            "[39/ 38] loss: 0.717527, Accuracy: 0.749589\n",
            "[40/ 39] loss: 0.713822, Accuracy: 0.751603\n",
            "[41/ 40] loss: 0.715204, Accuracy: 0.751172\n",
            "[42/ 41] loss: 0.720009, Accuracy: 0.749238\n",
            "[43/ 42] loss: 0.722050, Accuracy: 0.748512\n",
            "[44/ 43] loss: 0.723854, Accuracy: 0.747093\n",
            "[45/ 44] loss: 0.720557, Accuracy: 0.747869\n",
            "[46/ 45] loss: 0.718943, Accuracy: 0.747569\n",
            "[47/ 46] loss: 0.715442, Accuracy: 0.748641\n",
            "[48/ 47] loss: 0.719954, Accuracy: 0.746676\n",
            "[49/ 48] loss: 0.723715, Accuracy: 0.745768\n",
            "[50/ 49] loss: 0.725055, Accuracy: 0.745855\n",
            "[51/ 50] loss: 0.726587, Accuracy: 0.745938\n",
            "[52/ 51] loss: 0.722380, Accuracy: 0.747549\n",
            "[53/ 52] loss: 0.720633, Accuracy: 0.747897\n",
            "[54/ 53] loss: 0.721341, Accuracy: 0.748526\n",
            "[55/ 54] loss: 0.720195, Accuracy: 0.748843\n",
            "[56/ 55] loss: 0.719250, Accuracy: 0.750000\n",
            "[57/ 56] loss: 0.721931, Accuracy: 0.749163\n",
            "[58/ 57] loss: 0.725700, Accuracy: 0.749178\n",
            "[59/ 58] loss: 0.725553, Accuracy: 0.749461\n",
            "[60/ 59] loss: 0.726879, Accuracy: 0.748941\n",
            "[61/ 60] loss: 0.724178, Accuracy: 0.749479\n",
            "[62/ 61] loss: 0.728069, Accuracy: 0.747951\n",
            "[63/ 62] loss: 0.726490, Accuracy: 0.747984\n",
            "[64/ 63] loss: 0.726635, Accuracy: 0.747520\n",
            "[65/ 64] loss: 0.732485, Accuracy: 0.746094\n",
            "[66/ 65] loss: 0.731985, Accuracy: 0.746394\n",
            "[67/ 66] loss: 0.730583, Accuracy: 0.747159\n",
            "[68/ 67] loss: 0.733815, Accuracy: 0.745802\n",
            "[69/ 68] loss: 0.738853, Accuracy: 0.744026\n",
            "[70/ 69] loss: 0.735258, Accuracy: 0.745471\n",
            "[71/ 70] loss: 0.737118, Accuracy: 0.745313\n",
            "[72/ 71] loss: 0.740042, Accuracy: 0.744498\n",
            "[73/ 72] loss: 0.741703, Accuracy: 0.743273\n",
            "[74/ 73] loss: 0.742329, Accuracy: 0.743365\n",
            "[75/ 74] loss: 0.742945, Accuracy: 0.743032\n",
            "[76/ 75] loss: 0.743568, Accuracy: 0.742917\n",
            "[77/ 76] loss: 0.741234, Accuracy: 0.743832\n",
            "[78/ 77] loss: 0.743108, Accuracy: 0.742695\n",
            "[79/ 78] loss: 0.744065, Accuracy: 0.741987\n",
            "[80/ 79] loss: 0.745621, Accuracy: 0.741100\n",
            "[81/ 80] loss: 0.746578, Accuracy: 0.741016\n",
            "[82/ 81] loss: 0.748647, Accuracy: 0.740741\n",
            "[83/ 82] loss: 0.748097, Accuracy: 0.740473\n",
            "[84/ 83] loss: 0.746610, Accuracy: 0.740776\n",
            "[85/ 84] loss: 0.746075, Accuracy: 0.741257\n",
            "[86/ 85] loss: 0.744742, Accuracy: 0.741176\n",
            "[87/ 86] loss: 0.745748, Accuracy: 0.740552\n",
            "[88/ 87] loss: 0.744432, Accuracy: 0.740840\n",
            "[89/ 88] loss: 0.743889, Accuracy: 0.740767\n",
            "[90/ 89] loss: 0.745705, Accuracy: 0.740169\n",
            "[91/ 90] loss: 0.744532, Accuracy: 0.740278\n",
            "[92/ 91] loss: 0.744137, Accuracy: 0.740385\n",
            "[93/ 92] loss: 0.743949, Accuracy: 0.740659\n",
            "[94/ 93] loss: 0.743500, Accuracy: 0.740759\n",
            "[95/ 94] loss: 0.742911, Accuracy: 0.741024\n",
            "[96/ 95] loss: 0.745207, Accuracy: 0.740132\n",
            "[97/ 96] loss: 0.748021, Accuracy: 0.739258\n",
            "[98/ 97] loss: 0.748244, Accuracy: 0.739207\n",
            "[99/ 98] loss: 0.749093, Accuracy: 0.739477\n",
            "[100/ 99] loss: 0.745211, Accuracy: 0.741004\n",
            "[101/ 100] loss: 0.744424, Accuracy: 0.741094\n",
            "[102/ 101] loss: 0.745655, Accuracy: 0.740408\n",
            "[103/ 102] loss: 0.750587, Accuracy: 0.739583\n",
            "[104/ 103] loss: 0.748692, Accuracy: 0.740140\n",
            "[105/ 104] loss: 0.747420, Accuracy: 0.740385\n",
            "[106/ 105] loss: 0.747291, Accuracy: 0.740476\n",
            "[107/ 106] loss: 0.748846, Accuracy: 0.739829\n",
            "[108/ 107] loss: 0.748216, Accuracy: 0.739632\n",
            "[109/ 108] loss: 0.747581, Accuracy: 0.739873\n",
            "[110/ 109] loss: 0.749702, Accuracy: 0.739249\n",
            "[111/ 110] loss: 0.751037, Accuracy: 0.738778\n",
            "[112/ 111] loss: 0.751329, Accuracy: 0.738598\n",
            "[113/ 112] loss: 0.751207, Accuracy: 0.738700\n",
            "[114/ 113] loss: 0.750557, Accuracy: 0.738800\n",
            "[115/ 114] loss: 0.749468, Accuracy: 0.739720\n",
            "[116/ 115] loss: 0.748976, Accuracy: 0.739946\n",
            "[117/ 116] loss: 0.748483, Accuracy: 0.739898\n",
            "[118/ 117] loss: 0.745611, Accuracy: 0.740652\n",
            "[119/ 118] loss: 0.746106, Accuracy: 0.740201\n",
            "[120/ 119] loss: 0.746382, Accuracy: 0.740284\n",
            "[121/ 120] loss: 0.744741, Accuracy: 0.740885\n",
            "[122/ 121] loss: 0.743742, Accuracy: 0.741219\n",
            "[123/ 122] loss: 0.744376, Accuracy: 0.741163\n",
            "[124/ 123] loss: 0.742116, Accuracy: 0.741997\n",
            "[125/ 124] loss: 0.742934, Accuracy: 0.741935\n",
            "[126/ 125] loss: 0.742273, Accuracy: 0.742125\n",
            "[127/ 126] loss: 0.741321, Accuracy: 0.742560\n",
            "[128/ 127] loss: 0.740325, Accuracy: 0.743233\n",
            "[129/ 128] loss: 0.741178, Accuracy: 0.743286\n",
            "[130/ 129] loss: 0.740276, Accuracy: 0.743338\n",
            "[131/ 130] loss: 0.740690, Accuracy: 0.743149\n",
            "[132/ 131] loss: 0.739118, Accuracy: 0.743559\n",
            "[133/ 132] loss: 0.738110, Accuracy: 0.743726\n",
            "[134/ 133] loss: 0.739512, Accuracy: 0.743774\n",
            "[135/ 134] loss: 0.739239, Accuracy: 0.743587\n",
            "[136/ 135] loss: 0.740169, Accuracy: 0.743403\n",
            "[137/ 136] loss: 0.738733, Accuracy: 0.743911\n",
            "[138/ 137] loss: 0.738453, Accuracy: 0.743841\n",
            "[139/ 138] loss: 0.739077, Accuracy: 0.743773\n",
            "[140/ 139] loss: 0.737706, Accuracy: 0.744492\n",
            "[141/ 140] loss: 0.736781, Accuracy: 0.744754\n",
            "[142/ 141] loss: 0.736923, Accuracy: 0.744570\n",
            "[143/ 142] loss: 0.736436, Accuracy: 0.744608\n",
            "[144/ 143] loss: 0.736240, Accuracy: 0.744865\n",
            "[145/ 144] loss: 0.736678, Accuracy: 0.745009\n",
            "[146/ 145] loss: 0.736451, Accuracy: 0.745151\n",
            "[147/ 146] loss: 0.738194, Accuracy: 0.744221\n",
            "[148/ 147] loss: 0.738072, Accuracy: 0.744048\n",
            "[149/ 148] loss: 0.739649, Accuracy: 0.743877\n",
            "[150/ 149] loss: 0.741242, Accuracy: 0.743289\n",
            "[151/ 150] loss: 0.743386, Accuracy: 0.742396\n",
            "[152/ 151] loss: 0.744271, Accuracy: 0.742653\n",
            "[153/ 152] loss: 0.744819, Accuracy: 0.742496\n",
            "[154/ 153] loss: 0.745010, Accuracy: 0.742443\n",
            "[155/ 154] loss: 0.743966, Accuracy: 0.742796\n",
            "[156/ 155] loss: 0.743651, Accuracy: 0.742843\n",
            "[157/ 156] loss: 0.744628, Accuracy: 0.742688\n",
            "[158/ 157] loss: 0.745716, Accuracy: 0.741939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images,labels = next(iter(test_loader))\n",
        "images = images.reshape(-1,28*28).to(device)\n",
        "labels = labels.to(device)\n",
        "outputs = model(images)\n",
        "_,predicted = torch.max(outputs,1)\n",
        "\n",
        "fig ,ax = plt.subplots(1,5,figsize=(15,3))\n",
        "for i in range(5):\n",
        "    ax[i].imshow(images[i].cpu().detach().numpy().reshape(28,28))\n",
        "    ax[i].set_title(f\"True:{labels[i].item()}, Pred: {predicted[i].item()}\")\n",
        "    ax[i].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "DOR6tuHL0U0V",
        "outputId": "4cb34d12-0083-40a7-b318-f3c2c2ebf70a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VE7Y8X0b0KMd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}